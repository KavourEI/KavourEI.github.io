<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/research/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-08-19T12:39:28+03:00</updated>
  <id>http://localhost:4000/tag/research/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Apple Intelligence Foundation Language Models</title>
      <link href="http://localhost:4000/AppleIntelligenceFoundationLanguageModels" rel="alternate" type="text/html" title="Apple Intelligence Foundation Language Models" />
      <published>2024-07-29T00:00:00+03:00</published>
      <updated>2024-07-29T00:00:00+03:00</updated>
      <id>http://localhost:4000/AppleIntelligenceFoundationLanguageModels</id>
      <content type="html" xml:base="http://localhost:4000/AppleIntelligenceFoundationLanguageModels">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.21075&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">KAN or MLP - A Fairer Comparison</title>
      <link href="http://localhost:4000/KANorMLP" rel="alternate" type="text/html" title="KAN or MLP - A Fairer Comparison" />
      <published>2024-07-23T00:00:00+03:00</published>
      <updated>2024-07-23T00:00:00+03:00</updated>
      <id>http://localhost:4000/KANorMLP</id>
      <content type="html" xml:base="http://localhost:4000/KANorMLP">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, we control the number of parameters and FLOPs to compare the performance of KAN and MLP. Our main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. We also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, we find that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. We hope these results provide insights for future research on KAN and other MLP alternatives.&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://github.com/yu-rp/KANbeFair&quot;&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.16674&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Stretching Each Dollar- Diffusion Training from Scratch on a Micro-Budget</title>
      <link href="http://localhost:4000/DiffusionTrainingfromScratch" rel="alternate" type="text/html" title="Stretching Each Dollar- Diffusion Training from Scratch on a Micro-Budget" />
      <published>2024-07-22T00:00:00+03:00</published>
      <updated>2024-07-22T00:00:00+03:00</updated>
      <id>http://localhost:4000/DiffusionTrainingfromScratch</id>
      <content type="html" xml:base="http://localhost:4000/DiffusionTrainingfromScratch">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to address this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose to randomly mask up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only $1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive FID and high-quality generations while incurring 118× lower cost than stable diffusion models and 14× lower cost than the current state-of-the-art approach that costs $28,400. We aim to release our end-to-end training pipeline to further democratize the training of large-scale diffusion models on micro-budgets.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.15811&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Shape of Motion - 4D Reconstruction from a Single Video</title>
      <link href="http://localhost:4000/ShapeOfMotion" rel="alternate" type="text/html" title="Shape of Motion - 4D Reconstruction from a Single Video" />
      <published>2024-07-18T00:00:00+03:00</published>
      <updated>2024-07-18T00:00:00+03:00</updated>
      <id>http://localhost:4000/ShapeOfMotion</id>
      <content type="html" xml:base="http://localhost:4000/ShapeOfMotion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches are limited in that they either depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. In this work, we introduce a method capable of reconstructing generic dynamic scenes, featuring explicit, full-sequence-long 3D motion, from casually captured monocular videos. We tackle the under-constrained nature of the problem with two key insights: First, we exploit the low-dimensional structure of 3D motion by representing scene motion with a compact set of SE3 motion bases. Each point's motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we utilize a comprehensive set of data-driven priors, including monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes.&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://shape-of-motion.github.io/&quot;&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.13764&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">SpreadsheetLLM - Encoding Spreadsheets for Large Language Models</title>
      <link href="http://localhost:4000/MicrosoftSpreadsheetLLM" rel="alternate" type="text/html" title="SpreadsheetLLM - Encoding Spreadsheets for Large Language Models" />
      <published>2024-07-12T00:00:00+03:00</published>
      <updated>2024-07-12T00:00:00+03:00</updated>
      <id>http://localhost:4000/MicrosoftSpreadsheetLLM</id>
      <content type="html" xml:base="http://localhost:4000/MicrosoftSpreadsheetLLM">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Spreadsheets, with their extensive two-dimensional grids, various layouts, and diverse formatting options, present notable challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.09025&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Mixture of A Million Experts</title>
      <link href="http://localhost:4000/MixtureofAMillionExperts" rel="alternate" type="text/html" title="Mixture of A Million Experts" />
      <published>2024-07-04T00:00:00+03:00</published>
      <updated>2024-07-04T00:00:00+03:00</updated>
      <id>http://localhost:4000/MixtureofAMillionExperts</id>
      <content type="html" xml:base="http://localhost:4000/MixtureofAMillionExperts">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.04153&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">RouteLLM-Learning to Route LLMs with Preference Data</title>
      <link href="http://localhost:4000/RouteLLM" rel="alternate" type="text/html" title="RouteLLM-Learning to Route LLMs with Preference Data" />
      <published>2024-07-01T00:00:00+03:00</published>
      <updated>2024-07-01T00:00:00+03:00</updated>
      <id>http://localhost:4000/RouteLLM</id>
      <content type="html" xml:base="http://localhost:4000/RouteLLM">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.18665&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AI Agents That Matter</title>
      <link href="http://localhost:4000/AIAgentsThatMatter" rel="alternate" type="text/html" title="AI Agents That Matter" />
      <published>2024-07-01T00:00:00+03:00</published>
      <updated>2024-07-01T00:00:00+03:00</updated>
      <id>http://localhost:4000/AIAgentsThatMatter</id>
      <content type="html" xml:base="http://localhost:4000/AIAgentsThatMatter">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;AI agents are an exciting new research direction, and agent development is driven by benchmarks. Our analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. Our focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. We design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. We prescribe a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. We hope that the steps we introduce for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.01502&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Data curation via joint example selection further accelerates multimodal learning</title>
      <link href="http://localhost:4000/DataCuration" rel="alternate" type="text/html" title="Data curation via joint example selection further accelerates multimodal learning" />
      <published>2024-06-25T00:00:00+03:00</published>
      <updated>2024-06-25T00:00:00+03:00</updated>
      <id>http://localhost:4000/DataCuration</id>
      <content type="html" xml:base="http://localhost:4000/DataCuration">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly selecting batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from larger super-batches, we also leverage recent advances in model approximation to reduce the associated computational overhead. As a result, our approach--multimodal contrastive learning with joint example selection (JEST)--surpasses state-of-the-art models with up to 13× fewer iterations and 10× less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing the level of data curation as a new dimension for neural scaling laws.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.17711&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Connecting the Dots - LLMs can Infer and Verbalize Latent Structure from Disparate Training Data</title>
      <link href="http://localhost:4000/ConnectDots" rel="alternate" type="text/html" title="Connecting the Dots - LLMs can Infer and Verbalize Latent Structure from Disparate Training Data" />
      <published>2024-06-20T00:00:00+03:00</published>
      <updated>2024-06-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/ConnectDots</id>
      <content type="html" xml:base="http://localhost:4000/ConnectDots">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. Further experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs (x,f(x)) can articulate a definition of f and compute inverses. While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures. Overall, the ability of LLMs to &quot;connect the dots&quot; without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.14546&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
</feed>
