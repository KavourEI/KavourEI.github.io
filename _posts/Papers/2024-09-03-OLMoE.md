---
layout: post
current: post
cover:  assets/images/arxiv.jpeg
navigation: True
title: OLMoE-Open Mixture of Experts Language Models
date: 2024-09-03
tags: [research]
class: post-template
subclass: 'post'
author: Kavour
---

<h2> Abstract </h2>

<p> We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.</p>

<h2> Authors </h2>

<p> <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Muennighoff,+N">Niklas Muennighoff</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Soldaini,+L">Luca Soldaini</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Groeneveld,+D">Dirk Groeneveld</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lo,+K">Kyle Lo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Morrison,+J">Jacob Morrison</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Min,+S">Sewon Min</a>,  <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+W">Weijia Shi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Walsh,+P">Pete Walsh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tafjord,+O">Oyvind Tafjord</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lambert,+N">Nathan Lambert</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+Y">Yuling Gu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Arora,+S">Shane Arora</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bhagia,+A">Akshita Bhagia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schwenk,+D">Dustin Schwenk</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wadden,+D">David Wadden</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wettig,+A">Alexander Wettig</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hui,+B">Binyuan Hui</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dettmers,+T">Tim Dettmers</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kiela,+D">Douwe Kiela</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Farhadi,+A">Ali Farhadi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Smith,+N+A">Noah A. Smith</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Koh,+P+W">Pang Wei Koh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Singh,+A">Amanpreet Singh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hajishirzi,+H">Hannaneh Hajishirzi</a></p>

<p>For more information go <a href='https://arxiv.org/abs/2409.02060'>here</a></p>