---
layout: post
current: post
cover:  assets/images/arxiv.jpeg
navigation: True
title: MoMa - Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts
date: 2024-08-12
tags: [research]
class: post-template
subclass: 'post'
author: Kavour
---

<h2> Abstract </h2>

<p> We introduce MoMa, a novel modality-aware mixture-of-experts (MoE) architecture designed for pre-training mixed-modal, early-fusion language models. MoMa processes images and text in arbitrary sequences by dividing expert modules into modality-specific groups. These groups exclusively process designated tokens while employing learned routing within each group to maintain semantically informed adaptivity. Our empirical results reveal substantial pre-training efficiency gains through this modality-specific parameter allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model, featuring 4 text experts and 4 image experts, achieves impressive FLOPs savings: 3.7x overall, with 2.6x for text and 5.2x for image processing compared to a compute-equivalent dense baseline, measured by pre-training loss. This outperforms the standard expert-choice MoE with 8 mixed-modal experts, which achieves 3x overall FLOPs savings (3x for text, 2.8x for image). Combining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs savings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination hurts performance in causal inference due to increased sensitivity to router accuracy. These results demonstrate MoMa's potential to significantly advance the efficiency of mixed-modal, early-fusion language model pre-training, paving the way for more resource-efficient and capable multimodal AI systems.</p>

<h2> Authors </h2>

<p><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+X+V">Xi Victoria Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shrivastava,+A">Akshat Shrivastava</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+L">Liang Luo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Iyer,+S">Srinivasan Iyer</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lewis,+M">Mike Lewis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ghosh,+G">Gargi Ghosh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zettlemoyer,+L">Luke Zettlemoyer</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Aghajanyan,+A">Armen Aghajanyan</a></p>

<p>For more information go <a href='https://arxiv.org/abs/2407.21770'>here</a></p>