---
layout: post
current: post
cover: assets/images/news_images/Cerebras.webp
navigation: True
title: Achieving 3x Faster Performance
date: 2024-10-24
tags: [news]
class: post-template
subclass: 'post'
author: Kavour
---

<p>Cerebras Technologies has announced a significant update to its inference capabilities, achieving a remarkable 3x performance boost for its Llama 3.1-70B model. This advancement positions Cerebras Inference as a leader in speed and efficiency, enabling transformative applications across various industries.</p>

<p>Cerebras Technologies has recently unveiled the most substantial update to Cerebras Inference since its launch, achieving an impressive throughput of 2,100 tokens per second for the Llama 3.1-70B model. This performance enhancement represents a threefold increase over previous versions and establishes Cerebras Inference as a game-changing solution in the AI landscape.</p>

<p>The speed of Cerebras Inference is striking when compared to traditional GPU solutions:</p>
<ul>
    <li><strong>16x faster</strong> than the fastest GPU solution available.</li>
    <li><strong>8x faster</strong> than GPUs running the smaller Llama 3.1-3B model.</li>
    <li>This performance leap is akin to a new generation of GPU upgrades (H100/A100) achieved through a single software release.</li>
</ul>

<p>Fast inference is crucial for developing next-generation AI applications across various domains, including voice recognition, video generation, and advanced reasoning tasks. Leading companies like Tavus and GSK are already leveraging Cerebras Inference to enhance their workflows and push the boundaries of AI capabilities.</p>

<p>Cerebras Inference has undergone rigorous testing by Artificial Analysis, a third-party benchmarking organization. The results highlight its superiority:</p>
<ul>
    <li>Cerebras Inference is <strong>16x faster</strong> than the most optimized GPU solutions.</li>
    <li>It outperforms hyperscale cloud services by a factor of <strong>68x</strong>.</li>
    <li>For multi-step workflows, it completes requests in just 0.4 seconds compared to 1.1 to 4.2 seconds on GPU-based solutions.</li>
</ul>

<p>The latest release builds on previous advancements by optimizing critical kernels such as MatMul and element-wise operations. The Wafer Scale Engine has been enhanced to utilize peak bandwidth and compute capabilities more effectively. Additionally, speculative decoding has been implemented, allowing for faster answer generation while maintaining output accuracy.</p>

<p>The speed enhancements provided by Cerebras Inference are already transforming how organizations approach AI application development:</p>
<ul>
    <li><strong>Pharmaceutical Research:</strong> GSK is utilizing the speed of Cerebras Inference to develop intelligent research agents that significantly enhance productivity in drug discovery processes.</li>
    <li><strong>Voice AI Development:</strong> LiveKit's CEO notes that with Cerebras Inference, voice AI can now operate at human-level speed and accuracy, revolutionizing real-time applications.</li>
    <li><strong>Reasoning Tasks:</strong> The platform enables models to perform extensive reasoning without incurring typical latency penalties, making it ideal for complex coding and research tasks.</li>
</ul>

<p>The substantial performance improvements showcased in this update demonstrate the potential of the Wafer Scale Engine for inference tasks. As Cerebras continues to optimize both software and hardware capabilities, users can expect further enhancements in model selection, context lengths, and API features in the near future.</p>

<p>The recent advancements in Cerebras Inference highlight the transformative power of fast inference in AI applications. With a throughput of 2,100 tokens per second for Llama 3.1-70B, Cerebras has set a new standard for performance that will enable developers to create more responsive and intelligent applications across various sectors.</p>

<p> You can read official Cerebras <a href='https://cerebras.ai/blog/cerebras-inference-3x-faster'>blog post</a> where there are more details about the results included.</p>