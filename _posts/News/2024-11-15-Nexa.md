---
layout: post
current: post
cover: assets/images/news_images/nexa.png
navigation: True
title: A Breakthrough in Multimodal AI for Edge Devices by Nexa.AI's OmniVision
date: 2024-11-15
tags: [news]
class: post-template
subclass: 'post'
author: Kavour
---


<p>Nexa AI's OmniVision is a compact multimodal model designed for processing both visual and text inputs, optimized for edge devices, showcasing significant advancements in art analysis, scene comprehension, and more.</p>

<p> Nexa AI's OmniVision represents a significant leap in this domain, offering a sub-billion parameter multimodal model that effectively processes visual and textual inputs. With its recent upgrade to the <a href='https://huggingface.co/NexaAIDev/omnivision-968M'>OmniVision-968M</a> version, the model has improved capabilities in art analysis, scene comprehension, style recognition, color perception, and world knowledge. This article delves into the architecture, training methodology, and performance benchmarks of OmniVision, highlighting its potential applications in edge computing environments.</p>

<p>OmniVision is designed to operate efficiently on edge devices, making it suitable for various applications where computational resources are limited. The model operates with an FP16 version requiring only 988 MB of RAM and 948 MB of storage space. Its architecture consists of three key components:</p>
<ul>
    <li><strong>Vision Encoder:</strong> This component transforms input images into embeddings that can be processed further.</li>
    <li><strong>Projection Layer:</strong> It aligns the image embeddings with the token space of the Qwen2.5-0.5B-Instruct model, enabling effective visual-language understanding.</li>
    <li><strong>Response Generation:</strong> The model generates responses based on both visual and textual inputs, enhancing its contextual understanding.</li>
</ul>

<p>The development of OmniVision involved a three-stage training pipeline:</p>
<ul>
    <li><strong>Stage One:</strong> Establishing basic visual-linguistic alignments using image-caption pairs while only unfreezing the projection layer parameters.</li>
    <li><strong>Stage Two:</strong> Enhancing contextual understanding through image-based question-answering datasets, allowing the model to generate contextually appropriate responses.</li>
    <li><strong>Stage Three:</strong> Implementing Direct Preference Optimization (DPO) by generating responses to images and refining them through a teacher model that produces minimally edited corrections.</li>
</ul>

<p>A notable challenge in deploying multimodal models on edge devices is the computational overhead associated with processing image tokens. Traditional architectures like LLaVA generate numerous tokens per image, leading to high latency and costs. To address this, OmniVision employs a reshaping mechanism during the projection stage that compresses image embeddings from `[batch_size, 729, hidden_size]` to `[batch_size, 81, hidden_size*9]`. This reduction in token count improves performance significantly while maintaining output quality.</p>

<p>The DPO approach used in OmniVision focuses on generating minimal-edit pairs for training. By ensuring that the teacher model makes small adjustments to the base model's outputs while preserving their structure, this technique enhances output quality without disrupting core capabilities. This method allows for precise improvements in accuracy-critical elements of the model's responses.</p>

<p>The performance of OmniVision has been evaluated against various benchmark datasets including MM-VET, ChartQA, MMMU, ScienceQA, and POPE. The results demonstrate that OmniVision consistently outperforms previous models such as nanoLLAVA:</p>

<table>
    <tr>
        <th>Benchmark</th>
        <th>OmniVision</th>
        <th>NanoLLAVA</th>
        <th>Qwen2-VL-2B</th>
    </tr>
    <tr>
        <td>MM-VET</td>
        <td>27.5</td>
        <td>23.9</td>
        <td>49.5</td>
    </tr>
    <tr>
        <td>ChartQA (Test)</td>
        <td>59.2</td>
        <td>N/A</td>
        <td>73.5</td>
    </tr>
    <tr>
        <td>MMMU (Test)</td>
        <td>41.8</td>
        <td>28.6</td>
        <td>41.1</td>
    </tr>
    <tr>
        <td>ScienceQA (Eval)</td>
        <td>62.2</td>
        <td>59.0</td>
        <td>N/A</td>
    </tr>
    <tr>
        <td>POPE</td>
        <td>89.4</td>
        <td>84.1</td>
        <td>N/A</td>
    </tr>
</table>

<p>Nexa AI is committed to further developing OmniVision into a fully optimized solution for edge AI multimodal applications. While the current version demonstrates impressive capabilities, ongoing improvements aim to address existing limitations and enhance overall performance.</p>

<p>The launch of OmniVision marks a significant advancement in multimodal AI technology tailored for edge devices. With its efficient architecture and innovative training methodologies, OmniVision stands out as a powerful tool for processing visual and textual data seamlessly. As Nexa AI continues to refine this model, it holds great promise for applications across various sectors where efficient data processing is crucial. Find more details and full report <a href='https://nexa.ai/blogs/omni-vision'>here</a>.</p>