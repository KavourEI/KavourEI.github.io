---
layout: post
current: post
cover: assets/images/news_images/Cerebras.webp
navigation: True
title: Introducing Cerebras Inference - AI at Instant Speed
tags: [news]
class: post-template
subclass: 'post'
author: Kavour
---

<p> Cerebras has unveiled its new AI inference solution, claiming it to be the fastest in the world, outpacing NVIDIA GPU-based clouds by 20 times and delivering industry-leading cost efficiency.</p> 

<img src = 'https://cerebras.ai/wp-content/uploads/2024/08/Screenshot-2024-08-26-at-11.39.02%E2%80%AFPM.png' />

<p> Powered by the third-generation Wafer Scale Engine (WSE-3), this solution can process 1,800 tokens per second for Llama3.1 8B models and 450 tokens per second for Llama3.1 70B models, all while maintaining high accuracy with native 16-bit weights. The system's exceptional memory bandwidth and unique chip design eliminate traditional bottlenecks, enabling real-time AI responses. With open API access and competitive pricing, Cerebras Inference aims to revolutionize the development and deployment of large language models (LLMs) across various industries.</p>

<p> This breakthrough allows for more sophisticated AI workflows, such as enhanced real-time intelligence and complex tasks like code generation, which previously required extensive processing power and time. As Cerebras expands support to even larger models, its platform is set to open new possibilities in AI innovation.</p>

<p> To read more and benefit from those changes you can find out more <a href='https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed'>here</a>.</p>