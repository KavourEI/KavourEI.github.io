---
layout: post
current: post
cover:  assets/images/news_images/google_ai.jpg
navigation: True
title: Google Announcements
date: 2024-05-14
tags: [news]
class: post-template
subclass: 'post'
author: Kavour
---


<p>Just a day after OpenAI wowed us with GPT-4o, Google decided it’s their turn to dazzle! Let's dive into the goodies unveiled at the Google IO conference. </p>

<p>Among the exciting announcements, we have:
<ul>
<li> <strong><a href="https://deepmind.google/technologies/veo/">Veo</a></strong>: Google’s star video generation model.</li>
<li> <strong><a href="https://deepmind.google/technologies/gemini/project-astra/">Project Astra</a></strong>: The futuristic AI assistant in the making.</li>
<li> <strong><a href="https://deepmind.google/technologies/gemini/">Gemini 1.5 Pro Updates</a></strong>: Introducing two new versions of their flagship model – one sleeker, the other with a whopping 2M token context length.</li>
</ul></p>

Now, let's break these down one by one:

<h2> Veo </h2>
<p>Meet Veo, Google DeepMind's newest and shiniest video generation model. Here's what it can do:
<ul>
<li> Produce high-quality videos in 1080p </li>
<li>> Extend beyond a minute of runtime </li>
<li> Deliver a spectrum of cinematic and visual styles </li>
</p>

<p>Veo lets you input an image or video paired with a textual prompt. It can animate the image or edit the video as needed. Plus, it supports masked editing, so you can tweak specific areas by adding a mask and text prompt.
On the technical side, Google enhanced video captions in Veo’s training data and uses high-quality, compressed video representations (latents) to boost performance, speed, and efficiency.</p>

<h2> Project Astra </h2>
<p>Enter Astra, Google’s new project aimed at crafting the AI assistant of tomorrow, hot on the heels of OpenAI's GPT-4o demo.
Powered by Gemini, Astra supports real-time audio, text, video, and image interactions. Although it's still a prototype, Astra’s demo came through pre-recorded videos since it’s not yet widely available.
Early testers noted a bit of lag, less emotional intelligence, and tone compared to GPT-4o, but praised its text-to-speech prowess and potentially superior long-context video support.</p>

<h2> Gemini 1.5 Pro </h2>

<p>Google rolled out two new iterations of their flagship model, Gemini 1.5 Pro:
<ul>
<li> <strong>Gemini 1.5 Pro Flash</strong>: A nimble, fast, and cost-efficient version, still multimodal with a 1M token context length. It boasts an MMLU of 78.9% vs. 81.9% for the original Gemini 1.5 Pro.</li>
<li> <strong>Gemini 1.5 Pro</strong>: This powerhouse’s context length is doubled to 2M tokens and is currently accessible via a waitlist for select API developers.</li>
</ul></p>

<h2> Other Announcements </h2>
<p>Google also revealed:
<ul>
<li> <strong>Imagen 3</strong>: Their most advanced image generation model, available in versions tailored for tasks ranging from quick sketches to high-res images.</li>
<li> <strong>Gemma 2 and PaliGemma</strong> New open-source models. PaliGemma is Google’s inaugural vision-language model and is available now. Gemma 2, with 27B parameters, outperforms its predecessor and will be available in June. </li>
</ul>
The jam-packed 2-hour session also featured updates across Google’s ecosystem, including Search, Workspace, Photos, Android, and more.
</p>

<h2> Access </h2>
<p>
The Gemini API and Google AI Studio are now live in over 200 countries. Gemini 1.5 Flash is priced at $0.35 per 1M tokens, with context caching launching next month.
While Veo, Astra, and the 2M context Gemini 1.5 Pro aren’t available yet, you can join the waitlist for access. But no worries, Gemini 1.5 Pro Flash is ready for you through the API, and PaliGemma is freely available on Kaggle.
Time to explore these futuristic tools and see where they can take us!</p>