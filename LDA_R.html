<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Uncovering Topics in BBC News with Latent Dirichlet Allocation in R</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Data Science and AI News" />
    <link rel="shortcut icon" href="http://localhost:4000/assets/images/favicon_DSW_black.png" type="image/png" />
    <link rel="canonical" href="http://localhost:4000/LDA_R" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Kavour" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Uncovering Topics in BBC News with Latent Dirichlet Allocation in R" />
    <meta property="og:description" content="Welcome everyone! Keeping in mind the post about Latent Dirichlet Allocation (in case you have not read it yet and you are interested, you can read it here), I am going explore the computational part with you on a dataset containing BBC News (you can find the corresponding data here)." />
    <meta property="og:url" content="http://localhost:4000/LDA_R" />
    <meta property="og:image" content="http://localhost:4000/assets/images/LDA_BBC/topics_bbc_0.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2023-10-17T00:00:00+03:00" />
    <meta property="article:modified_time" content="2023-10-17T00:00:00+03:00" />
    <meta property="article:tag" content="Project" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Uncovering Topics in BBC News with Latent Dirichlet Allocation in R" />
    <meta name="twitter:description" content="Welcome everyone! Keeping in mind the post about Latent Dirichlet Allocation (in case you have not read it yet and you are interested, you can read it here), I am going explore the computational part with you on a dataset containing BBC News (you can find the corresponding data here)." />
    <meta name="twitter:url" content="http://localhost:4000/" />
    <meta name="twitter:image" content="http://localhost:4000/assets/images/LDA_BBC/topics_bbc_0.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Kavour" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Project" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Kavour",
        "logo": "http://localhost:4000/assets/images/favicon.png"
    },
    "url": "http://localhost:4000/LDA_R",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/assets/images/LDA_BBC/topics_bbc_0.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/LDA_R"
    },
    "description": "Welcome everyone! Keeping in mind the post about Latent Dirichlet Allocation (in case you have not read it yet and you are interested, you can read it here), I am going explore the computational part with you on a dataset containing BBC News (you can find the corresponding data here)."
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Uncovering Topics in BBC News with Latent Dirichlet Allocation in R" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="http://localhost:4000/"><img src="/assets/images/favicon.png" alt="Kavour" /></a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-project" role="menuitem"><a href="/tag/project/">Post</a></li>
    <li class="nav-news" role="menuitem"><a href="/tag/news/">News</a></li>
    <li class="nav-news" role="menuitem"><a href="/tag/research/">Research</a></li>
    <li class="nav-about" role="menuitem"><a href="/about">About</a></li>
    <!--
    <li class="nav-getting-started" role="menuitem"><a href="/tag/getting-started/">Getting Started</a></li>
    <li class="nav-try-ghost" role="menuitem"><a href="https://ghost.org">Try Ghost</a></li>
    -->
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
            
                <a class="social-link social-link-li" href="https://linkedin.com/in/themis-kavour" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32">
    <path d="M29.63 0H2.37A2.368 2.368 0 0 0 0 2.37v27.26A2.368 2.368 0 0 0 2.37 32h27.26A2.368 2.368 0 0 0 32 29.63V2.37A2.368 2.368 0 0 0 29.63 0zM9.507 26.907H4.898V12.384h4.61v14.523zM7.202 10.295a2.669 2.669 0 1 1 .001-5.338 2.669 2.669 0 0 1 0 5.338zM27.102 26.907h-4.609v-7.187c0-1.713-.03-3.918-2.387-3.918-2.388 0-2.753 1.867-2.753 3.796v7.309h-4.611V12.384h4.428v1.987h.063c.617-1.167 2.123-2.397 4.369-2.397 4.674 0 5.533 3.076 5.533 7.073v7.86h-.001z"/>
</svg>
</a>
            
            
                <a class="social-link social-link-gh" href="https://github.com/KavourEI" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="24" height="24">
  <path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.302 3.438 9.8 8.205 11.387.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61-.546-1.387-1.333-1.757-1.333-1.757-1.089-.744.083-.729.083-.729 1.205.085 1.84 1.236 1.84 1.236 1.07 1.835 2.807 1.305 3.492.998.108-.775.418-1.305.76-1.605-2.665-.305-5.466-1.335-5.466-5.93 0-1.31.467-2.382 1.235-3.22-.123-.303-.535-1.527.118-3.176 0 0 1.008-.322 3.3 1.23a11.49 11.49 0 0 1 3-.404c1.02.005 2.045.138 3 .404 2.29-1.552 3.297-1.23 3.297-1.23.653 1.649.241 2.873.118 3.176.77.838 1.233 1.91 1.233 3.22 0 4.61-2.807 5.625-5.48 5.92.43.37.823 1.1.823 2.22 0 1.605-.015 2.896-.015 3.286 0 .322.217.694.825.576C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/>
</svg>
</a>
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Subscribe</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  tag-project tag-machine-learning tag-nlp tag-bayesian-statistics post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="17 October 2023">17 October 2023</time>
                    
                        <span class="date-divider">/</span>
                        
                            
                               <a href='/tag/project/'>PROJECT</a>,
                            
                        
                            
                               <a href='/tag/machine-learning/'>MACHINE LEARNING</a>,
                            
                        
                            
                               <a href='/tag/nlp/'>NLP</a>,
                            
                        
                            
                               <a href='/tag/bayesian-statistics/'>BAYESIAN STATISTICS</a>
                            
                        
                    
                </section>
                <h1 class="post-full-title">Uncovering Topics in BBC News with Latent Dirichlet Allocation in R</h1>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/images/LDA_BBC/topics_bbc_0.jpg)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p> Welcome everyone! Keeping in mind the post about Latent Dirichlet Allocation (in case you have not read it yet and you are interested, you can read it <a href="/the-editor/">here</a>), I am going explore the computational part with you on a dataset containing BBC News (you can find the corresponding data <a href="/the-editor/">here</a>). With the assistance of R-programming language, we are going to find an optimal number of topics, in which we can cluster BBC News and tidy the archived collection of BBC News we have in our dataset. </p>

<p> The agenda is as follows. Initially we are going to load the required libraries in order to make this a successful journey. Later on, we are going to go through some preprocessing steps in order to make the algorithms job a little bit easier. Our third stop is going to be the model itself. We are going to go through some arguments in order to establish a common knowledge on how each of them works. The final step is going to be some exploration and commenting on our model's result. </p>

<h2 id="specialformatting">Tools are built to be used! 
The libraries that we are going to use.</h2>

<p> Just in case you are new with R-programming language, in order to install the libraries that will be presented below, you can run the following command: </p>

<pre><code>
  install.packages("name_of_the_library")
</code></pre>

<p> After having installed the libraries you can load them (in order to use them) as follows:  </p>

<pre><code>
  library(tidyverse)
  library(tm)
  library(quarteda)
  library(ldatuning)
  library(LDAvis)
  library(tidytext)
</code></pre>

<p> As you can see we are going to use:
<ul>
<li> <strong>tidyverse</strong> library: in order to manipulate data. This library will load all the excited and Wonderfull constructed parts of this universe a.k.a. ggplot, dplyr, tidyr, purrr, stringr, etc. You can accomplish great manipulation goals by using those libraries!</li>
<li> <strong>tm</strong> library: we will need this library in order to accomplish several preprocessing steps of text data. With the help of this library one can tokenize data, convert text data to a corpus and many more exciting nlp assignments. (fan fact : tm = text mining)</li>
<li> <strong>quarteda</strong> library: Even though this is another library for preprocessing steps, in combination with tm library, when used correctly, your fingers can be converted to multiple nlp ü¶∏ superheroes!</li>
<li> <strong>ldatuning</strong> library: this library will help us decide the number of topics we should look as optimal.</li>
<li> <strong>LDAvis</strong> library: this library (I won't lie to you) the existence of which I found out recently on the process of topic modelling studying, provides great ways to visualise the results with the help of JSON. I advise you to go along this whole process and end up using this library alongside in order to see the possibilities that you gain by including it in your topic modelling analysis!</li>
<li> <strong>tidytext</strong> library: the go-to tool that takes messy text data and turn it into a neat and usable format for analysis. </li>
</ul></p>

<h2 id="specialformatting">Preprocessing is always a must!</h2>

<p>We will now move forward to the step which in my opinion is the most important in a  process of model building. This step if neglected or completed recklessly, can result in a model that would not perform as expected and most certainly, would not give us back the results as expected. Anyways, in order to be able to complete this step, we need to have data to preprocess. We load the data downloaded as follows :</p>

<pre><code>
  bbc &lt;- read.csv('/Users/...your_path.../bbc_news.csv', stringsAsFactors = FALSE)
</code></pre>

<p>The data as you may have already have seen, contain the following variables:
<ul>
<li> title : The title of the BBC News </li>
<li> pubDate : Publication date </li>
<li> guid : BBC News link </li>
<li> link : BBC News link </li>
<li> description : The description of the BBC News </li>
</ul>
Initially, I would like to apologise of the explanations on guid and link variables. Unfortunately, there are no explanation in the dataset source and as I result I have written the descriptions based on what I came across in the dataset. Apart from that, we are going to work only with one variable and this is description. We also are going to keep the title as well, just for reference. As a result, we going to drop the rest. This can be easily done as follows :
</p>

<pre><code>
  bbc2 &lt;- bbc %&gt;% 
  select(title, description)
</code></pre>

<p>As a first toward preprocessing we are going to convert this dataset into a corpus. Corpus is a collection of data (text or audio) which are organised as dataset.</p>

<pre><code>
  bbc.corpus &lt;- corpus(bbc2, text_field = 'description')
</code></pre>

<p> Next in line are the changes that needed to be done in the given dataset. Namely, we need to tokenize that dataset. Apart from that we need to remove punctuation as the special characters do not contain and relevant information for topic indications. For the same reasons, we need to remove the numbers and of course the stopwords<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>  of English language.  </p>

<p>Following those changes we are going to stem<sup class="footnote-ref"><a href="#fn2" id="fnref2">[21]</a></sup> the corpus.</p>

<p>Finally, we are going to convert the whole corpus to lower characters. This is done so that the model will not judge same words like Legislation and legislation as two different words only for the first letter. Those steps are completed with the following commands:</p>

<pre><code>
  bbc.corpus.tokens &lt;- tokens(bbc.corpus, remove_punct = TRUE, remove_numbers = TRUE)
  bbc.corpus.tokens &lt;- tokens_remove(bbc.corpus.tokens, stopwords("english"))
  bbc.corpus.tokens &lt;- tokens_wordstem(bbc.corpus.tokens, language = 'english')
  bbc.corpus.tokens &lt;- tokens_tolower(bbc.corpus.tokens)
</code></pre>

<p>At this point your object bbc.corpus.tokens should look like this:</p>

<p><img src="assets/images/LDA_BBC/topic_bbc_1.png" alt="bbc.corpus.tokens" /></p>

<p>Finally, we are going to create a sparse <strong>document-feature matrix</strong> with the <em>dfm</em> function. This is needed in order for LDA function to work.</p>

<pre><code>
  dfm_bbc &lt;- dfm(bbc.corpus.tokens)
</code></pre>

<p>That is all for the preprocessing step. Before moving to the next step which the model building, I feel the need to tell you the following. NLP preprocessing depends on the context of data that you have and the goal you want to achieve. You really need to think this through. In our case, you will see some words later on like the word <mark>say</mark> or <mark>seem</mark>, that you may think it is better to remove after the first results due to the fact that those words appear in many topic groups. This may result in a more thorough view of your groups. If this is your goal, this is fine way to go. Therefore, If your goal is to make this model let's say more general and try to predict other text data with it I advise you to think it twice, which words should be removed, which should stay and what is the reason and result of such action.</p>

<blockquote>
<p>LDA modeling</p>
<p>LDA thinking</p>
<p>LDA building</p>
</blockquote>

<p>It is time for us to move forward to our model building. This will be completed with the help of LDA function. However LDA function has some arguments that needed to be filled. We will need x an object of class DocumentTermMatrix (a.k.a. our preprocessed data), <strong>k</strong> which is our "guess" in the number of topics that exist in the dataset containing multiple text data, <strong>method</strong> which is our selection on the method that the algorithm will use in order to compute the results and finally <strong>control</strong> where we will include a list of parameter settings to help the model work. Even though we have the help, in R-Studio IDE, I would like to elaborate the arguments a little bit more. </p>
<p>
<ul>
<li> k is our "guess". The reason I used quotes earlier is that, this may not be really a guess in certain cases. In one hand, we may have an idea of the bunch of documents that we would like to cluster. In the other hand we may use a tuning tool to help us choose. Either way, this is not what the term guess means. Well, of course you could really, try your luck and guess k, but after the first initiation I am sure you will see a pattern and correct your choice! üòâ </li>
<li> method is actually a choice between two different ways of working. In order to fill this argument we have to choose between "VEM" or "Gibbs". What are those? you may say, and which one to choose? üòµ‚Äçüí´ Well frankly I have the answer! VEM or else Variational Expectation Maximization is an optimisation-based approach where Gibbs on the other hand is a sampling Markov Chain Monte Carlo (MCMC) approach. Both have their strength and weaknesses. For example, VEM is a deterministic method (given we provide the same data, the same output we get) whereas Gibbs is a stochastic method which means that is samples from probability distributions and can provide different results from the same data, but will eventually converge to the true distribution, given some iterations occured. I will let you search around and figure out some other facts about those two as analysing how those work, as well as all their pros and cons can be really long job and is out of this scope. But a little hint is that they converge in different speeds, as you can guess, and of course they can handle large dataset in different ways.  </li>
<li> control is a list of subargumets, like alpha, verbose, seed and many more where you can really tune the way LDA will work. </li>
</ul>
</p>

<p>Of course apart from those, there are other arguments that you may want/need to tune but the least required so that we proceed are the aforementioned ones. So before running our model, I am going to search the number of topics with the help of ldatuning::FindTopicNumber() function as I indicated earlier. Initially we need to convert our dfm to an object of class DocumentTermMatrix as described earlier.</p>

<pre><code>
  new.dtm &lt;- convert(dfm_bbc, to = 'topicmodels')

  result &lt;- FindTopicsNumber(
    new.dtm,
    topics = seq(from = 2, to = 15, by = 1),
    metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
    method = "Gibbs",
    control = list(seed = 1234),
    #mc.cores = 2L,
    #verbose = TRUE
  )
</code></pre>

<p>Due to the seed, this should result to the following metrics matrix:</p>

<p><img src="assets/images/LDA_BBC/topic_bbc_2.png" alt="results" /></p>

<p>The package <em>ldatuning</em> provide to us a way to visually check the results with the usage of the function <em>FindTopicsNumber_plot</em>:</p>

<pre><code>
  FindTopicsNumber_plot(result)
</code></pre>

<p><img src="assets/images/LDA_BBC/topic_bbc_3.png" alt="find_topics_number" /></p>

<p>Our goal here, as you can see is to choose K-candidate (number of topics) that either maximise the bottom or minimise the top. I am not going to go through the metrics and how they are calculated but I will leave a reference at the end if you are interested. From the results produces (mainly the plot ü´•) the best candidate is K=3. This is obtained by the CaoJuan2009 metric. I have excluded the usage of the other three due to the fact that they do not converge. Here I am aiming for results on the level of field like politics, sports etc. If you would like more detailed clustering, for example foreign policy, domestic policy, soccer, basketball, etc then I advise you to search for 20 or even higher number of topics. </p>

<p>Next in line is the setting of the model's hyperparameters, a.k.a. alpha and beta. Please recall the intuition behind those two hyperparameters in order to fully understand the meaning of the results. In order to find an optimal setting, one could work with 3 or more ways.</p>
<p>
<ol>
  <li> Use the perplexity of topic modelling LDA model. </li>
  <li> Use the coherence measure of topic modelling LDA model. </li>
  <li> Use intuition about the documents. </li>
</ol>
</p>
<p>The last of the options is when the researcher has a brief idea about the documents, the level of vocabulary details, and the variance of words per document and topics. Even in this case, the researcher would have to try a few of the settings in order to find the optimum. Here I am going to work with the first two options. Before getting there though, I would like to give a rough definition for both notions of perplexity and topic coherence.</p>

<p>
<ul>
  <li><strong>Perplexity</strong> : In LDA topic modeling context, perplexity measures of how well an LDA model works in other words how generalised is the computed process. This is measured by the notion of how well a model can predict a new set of documents' topics based on what it learnerd from a training set. Lower perplexity means the model is better at making predictions.</li>
  <li> <strong>Coherence</strong> : Topic coherence is a measure that helps assess how easy it is to understand the topics created by the model computed. This measures if the words in a topic are semantically related to each other. If the coherence score is high, it means the topics are more understandable and therefore our LDA model is a fine tuned model.</li>
</ul>
</p>

<p>Perplexity and Coherence have their pros and cons. Here we are going to use perplexity to continue. Coherence is not hard to be computed though (hint use topic_coherence of package topicdoc). Here for the perplexity, I have decided to create two vectors with some of  possible values of alpha and beta and then compute all the perplexity values for a set of test data. The computations and results are the following</p>

<pre><code>
  alpha_values &lt;- seq(from =0.1,to = 1, by = 0.1)
  alpha_values
  beta_values &lt;- seq(from =0.1,to = 1, by = 0.1)
  beta_values
  
  n &lt;- length(alpha_values)
  result_matrix &lt;- matrix(0, n * n, 3)
  
  row_num &lt;- 1
  for (i in 1:n) {
    for (j in 1:n) {
      result_matrix[row_num, 1] &lt;- alpha_values[i]
      result_matrix[row_num, 2] &lt;- beta_values[j]
      
      m &lt;- LDA(train_dtm, k=3, method = 'Gibbs', control = list(alpha = alpha_values[i], seed = 1234), beta = beta_values[j])
      perp.value &lt;- perplexity(m, test_dtm)
      
      result_matrix[row_num, 3] &lt;- perp.value
      row_num &lt;- row_num + 1
    }
  }
</code></pre>

<p>In order to get the rows with the minimum perplexity value we have to run the following:

<pre><code>
  result_matrix[result_matrix[,3] == min(result_matrix[,3]),]
</code></pre>

and the result till this point should be :</p>

<p><img src="assets/images/LDA_BBC/topic_bbc_4.png" alt="ResList" /></p>

<p>We see here the setting for hyperparameter alpha is 0.2 whereas for beta according to perplexity measure it makes no difference. Recall the meaning of them and try to understand what 0.2 means.</p>
<p>As a result, we are ready to build our model according to those settings and proceed to the final act of this article where we will review the results of it.</p>

<h2 id="specialformatting">Model Building</h2>

<p>After having gathered all the information needed and went through all required preprocessing step it is time to build the model. This is done with the help of <em>LDA</em> function of package <em>topicmodels</em>. </p>

<pre><code>
  model.lda &lt;- LDA(new.dtm, k = 3, method = "Gibbs", control = list(alpha = 0.2, seed = 1234))
</code></pre>

<p>From this point and on, you can check many very interesting statistics like the per-topic-per-word probability </p>

<pre><code>
  news.topics &lt;- tidy(m_final, matrix = 'beta')
</code></pre>

<p>where you can see what is the probability of each word belonging to a topic:</p>

<p><img src="assets/images/LDA_BBC/topc_bbc_5.png" alt="topic_term_beta" /></p>

<p>or the top terms per topic in order to draw conclusions on the actual humanly communicated context of topic</p>

<pre><code>
  top.terms &lt;- news.topics %&gt;% 
    group_by(topic) %&gt;% 
    top_n(5) %&gt;% 
    ungroup() %&gt;% 
    arrange(topic, -beta)

  top.terms %&gt;% 
    mutate(term = reorder(term, beta)) %&gt;% 
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = TRUE) +
    facet_wrap(~ topic, scales = 'free') +
    coord_flip()
</code></pre>

<p><img src="assets/images/LDA_BBC/topic_bbc_6.png" alt="term_beta" /></p>

<p>You can see here the word say that I mentioned in the beginning... üòâ Here we can get an idea about each topic that the model clustered our BBC news data. I would say, a topic about sports, a topic about news related to Ukraine, and a topic of domestic (UK) news. Finally I would like you to copy paste this final part of chunk, and check out the result!</p>

<pre><code>
  new.dtm.2 &lt;- new.dtm[slam::row_sums(new.dtm) &gt; 0, ]
  phi &lt;- as.matrix(posterior(m_final)$terms)
  theta &lt;- as.matrix(posterior(m_final)$topics)
  vocab &lt;- colnames(phi)
  doc.length &lt;- slam::row_sums(new.dtm)
  term.freq &lt;- slam::col_sums(new.dtm)[match(vocab, colnames(new.dtm))]
  
  json &lt;- createJSON(phi = phi, theta = theta, vocab = vocab, doc.length = doc.length, term.frequency = term.freq)
  serVis(json)
</code></pre>

<p><img src="assets/images/LDA_BBC/topic_bbc_7.png" alt="LDAvis_res" /></p>

<p>This chunk will create an interactive app as the following where you can explore the capabilities available there. You can check out all the words that are included in every topic.</p>

<p>Overall, I believe that you have a good idea and all the tools to start exploring LDA yourself. I hope you've enjoyed the implementation presented here. Feel free to copy paste any part of the code, insert your data and explore nlp world from a different perspective.</p>

<hr />

<p>Be safe, code safer!</p>


                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            
                <section class="subscribe-form">
                    <h3 class="subscribe-form-title">Subscribe to Kavour</h3>
                    <p>Get the latest posts delivered right to your inbox</p>
                    <form method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  /><input class="location" type="hidden" name="location"  /><input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" type="email" name="email"  placeholder="youremail@example.com" />
    </div>
    <button class="" type="submit" disabled><span>Subscribe</span></button>
    <script type="text/javascript">(function(g,h,o,s,t){h[o]('.location')[s]=h[o]('.location')[s] || g.location.href;h[o]('.referrer')[s]=h[o]('.referrer')[s] || h.referrer;})(window,document,'querySelector','value');</script>
</form>

                </section>
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/images/blog-cover.jpg)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Kavour &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/project/">Project</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/jingle_or_no_jingle">Jingle or No Jingle. A Hilariously Serious Dive into PyTorch Image Classification for Santa Claus Detection. üéÑ</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/lda_fundamentals">Topic Modelling - Latent Dirichlet Allocation</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/neural_network">Neural Network ~ Predicting a Numerical Value</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/project/">
                                
                                    See all 4 posts  ‚Üí
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/jingle_or_no_jingle">
                <div class="post-card-image" style="background-image: url(/assets/images/jingle_no_jingle/Post_header.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/jingle_or_no_jingle">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Project</span>
                            
                        
                            
                               <span class="post-card-tags">Deep learning</span>
                            
                        
                            
                                <span class="post-card-tags">Pytorch vision</span>
                            
                        
                    

                    <h2 class="post-card-title">Jingle or No Jingle. A Hilariously Serious Dive into PyTorch Image Classification for Santa Claus Detection. üéÑ</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Season's Greetings, data scientists and tech enthusiasts! In the spirit of ho-ho-hilarity and cutting-edge Christmas cheer, I present to you a Christmas-themed trip into the world of PyTorch image classification. Armed with the</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                <span class="reading-time">
                    
                    
                      6 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/lda_fundamentals">
                <div class="post-card-image" style="background-image: url(/assets/images/lda_fund/lda_basic_0.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/lda_fundamentals">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Project</span>
                            
                        
                            
                               <span class="post-card-tags">Machine learning</span>
                            
                        
                            
                               <span class="post-card-tags">Nlp</span>
                            
                        
                            
                                <span class="post-card-tags">Bayesian statistics</span>
                            
                        
                    

                    <h2 class="post-card-title">Topic Modelling - Latent Dirichlet Allocation</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p>Hello everyone! In this post I am going to go through an NLP subject. As you may have already read in this post's title, Topic Modelling is what I aim to explain to</p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                <span class="reading-time">
                    
                    
                      9 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="http://localhost:4000/">
            
                <img src="/assets/images/favicon_DSW_black.png" alt="Kavour icon" />
            
            <span>Kavour</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">Uncovering Topics in BBC News with Latent Dirichlet Allocation in R</div>
        <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        
        <a class="floating-header-share-lin" href="https://www.linkedin.com/sharing/share-offsite/?url=https://kavourei.github.io/LDA_R"
           onclick="window.open(this.href, 'share-linkedin', 'width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32">
    <path d="M29.63 0H2.37A2.368 2.368 0 0 0 0 2.37v27.26A2.368 2.368 0 0 0 2.37 32h27.26A2.368 2.368 0 0 0 32 29.63V2.37A2.368 2.368 0 0 0 29.63 0zM9.507 26.907H4.898V12.384h4.61v14.523zM7.202 10.295a2.669 2.669 0 1 1 .001-5.338 2.669 2.669 0 0 1 0 5.338zM27.102 26.907h-4.609v-7.187c0-1.713-.03-3.918-2.387-3.918-2.388 0-2.753 1.867-2.753 3.796v7.309h-4.611V12.384h4.428v1.987h.063c.617-1.167 2.123-2.397 4.369-2.397 4.674 0 5.533 3.076 5.533 7.073v7.86h-.001z"/>
</svg>

        </a>

        <a class="floating-header-share-x" href="https://x.com/share?url=https://kavourei.github.io/LDA_R"
           onclick="window.open(this.href, 'share-x', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg"  viewBox="0 0 30 30" width="480px" height="480px"><path d="M26.37,26l-8.795-12.822l0.015,0.012L25.52,4h-2.65l-6.46,7.48L11.28,4H4.33l8.211,11.971L12.54,15.97L3.88,26h2.65 l7.182-8.322L19.42,26H26.37z M10.23,6l12.34,18h-2.1L8.12,6H10.23z"/></svg>
        </a>
    </div>
    
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="http://localhost:4000/">Kavour</a> &copy; 2024</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyllt/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                    <img class="subscribe-overlay-logo" src="/assets/images/favicon.png" alt="Kavour" />
                
                <h1 class="subscribe-overlay-title">Subscribe to Kavour</h1>
                <p class="subscribe-overlay-description">Stay up to date! Get all the latest &amp; greatest posts delivered straight to your inbox</p>
                <form method="post" action="/subscribe/" class="">
    <input class="confirm" type="hidden" name="confirm"  /><input class="location" type="hidden" name="location"  /><input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" type="email" name="email"  placeholder="youremail@example.com" />
    </div>
    <button class="" type="submit" disabled><span>Subscribe</span></button>
    <script type="text/javascript">(function(g,h,o,s,t){h[o]('.location')[s]=h[o]('.location')[s] || g.location.href;h[o]('.referrer')[s]=h[o]('.referrer')[s] || h.referrer;})(window,document,'querySelector','value');</script>
</form>

            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JSSW2W2D3P"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JSSW2W2D3P');
</script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
