<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/news/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-06-29T22:16:01+03:00</updated>
  <id>http://localhost:4000/tag/news/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Meta releases New AI Research Models to Accelerate Innovation at Scale</title>
      <link href="http://localhost:4000/MetaNewAI" rel="alternate" type="text/html" title="Meta releases New AI Research Models to Accelerate Innovation at Scale" />
      <published>2024-06-18T00:00:00+03:00</published>
      <updated>2024-06-18T00:00:00+03:00</updated>
      <id>http://localhost:4000/MetaNewAI</id>
      <content type="html" xml:base="http://localhost:4000/MetaNewAI">&lt;p&gt; For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever. &lt;/p&gt;

&lt;p&gt; Today, we shared some of our latest FAIR research models with the world. These publicly released models include image-to-text and text-to-music generation models, a multi-token prediction model, and a technique for detecting AI-generated speech. By making this research publicly available, Meta aims to inspire further iterations and ultimately promote responsible advancements in AI. &lt;/p&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://www.youtube.com/embed/p9oM5dWmFZ0&quot; title=&quot;Meet Meta Chameleon&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For more information you can see the official meta &lt;a href=&quot;https://about.fb.com/news/2024/06/releasing-new-ai-research-models-to-accelerate-innovation-at-scale/&quot;&gt;announcement&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">NVIDIA announced Nemotron 340B</title>
      <link href="http://localhost:4000/NVIDIARelease" rel="alternate" type="text/html" title="NVIDIA announced Nemotron 340B" />
      <published>2024-06-14T00:00:00+03:00</published>
      <updated>2024-06-14T00:00:00+03:00</updated>
      <id>http://localhost:4000/NVIDIARelease</id>
      <content type="html" xml:base="http://localhost:4000/NVIDIARelease">&lt;p&gt;Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.&lt;/p&gt;

&lt;p&gt;NVIDIA on 14th of June, announced Nemotron-4 340B, a family of open models that developers can use to generate synthetic data for training large language models (LLMs) for commercial applications across healthcare, finance, manufacturing, retail and every other industry. &lt;/p&gt;

&lt;p&gt;High-quality training data plays a critical role in the performance, accuracy and quality of responses from a custom LLM — but robust datasets can be prohibitively expensive and difficult to access. &lt;/p&gt;

&lt;p&gt;Through a uniquely permissive open model license, Nemotron-4 340B gives developers a free, scalable way to generate synthetic data that can help build powerful LLMs.&lt;/p&gt;

&lt;p&gt;The Nemotron-4 340B family includes base, instruct and reward models that form a pipeline to generate synthetic data used for training and refining LLMs. The models are optimized to work with NVIDIA NeMo, an open-source framework for end-to-end model training, including data curation, customization and evaluation. They’re also optimized for inference with the open-source NVIDIA TensorRT-LLM library. &lt;/p&gt;

&lt;p&gt;Nemotron-4 340B can be downloaded now from the NVIDIA NGC catalog and Hugging Face. Developers will soon be able to access the models at ai.nvidia.com, where they’ll be packaged as an NVIDIA NIM microservice with a standard application programming interface that can be deployed anywhere.&lt;/p&gt;

&lt;p&gt;If you want to find out more go to the &lt;a href=&quot;https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/&quot;&gt;official blog of NVIDIA&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Google open Project</title>
      <link href="http://localhost:4000/GoogleProjectIDX" rel="alternate" type="text/html" title="Google open Project" />
      <published>2024-06-14T00:00:00+03:00</published>
      <updated>2024-06-14T00:00:00+03:00</updated>
      <id>http://localhost:4000/GoogleProjectIDX</id>
      <content type="html" xml:base="http://localhost:4000/GoogleProjectIDX">&lt;p&gt;During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.&lt;/p&gt;

&lt;p&gt; &lt;i&gt;&quot;As AI becomes more prevalent, the complexities that come with deploying all of that really becomes harder, becomes greater, and we wanted to help solve that challenge,&quot;&lt;/i&gt; said Jeanine Banks, Google’s VP and general manager for Developer X and the company’s head of developer relations.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;&quot;That’s why we built project IDX, a multi-platform development experience that makes building applications fast and easy. Project IDX makes it really frictionless to get going with your preferred framework or language with easy-to-use templates like Next.js, Astro, Flutter, Dart, Angular, Go and more.&quot;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt; With this update, Google is integrating the Google Maps Platform into Project IDX, enabling the addition of geolocation features to apps. The update also includes integrations with Chrome Dev Tools and Lighthouse to assist in debugging applications. Soon, developers will be able to deploy apps to Cloud Run, Google Cloud's serverless platform for front- and back-end services.&lt;/p&gt;

&lt;p&gt; The development environment will also integrate with Checks, Google's AI-powered compliance platform, which is transitioning from beta to general availability on Tuesday.&lt;/p&gt;

&lt;p&gt; Project IDX isn't just about building AI-enabled applications; it's also about using AI to enhance the coding process. To facilitate this, IDX includes standard features like code completion and a chat assistant sidebar, as well as innovative tools like the ability to highlight a snippet of code and use Google's Gemini model to modify it, similar to generative fill in Photoshop.&lt;/p&gt;

&lt;p&gt; Whenever Gemini suggests code, it provides links back to the original source and its associated license.&lt;/p&gt;

&lt;p&gt; Built on the open-source Visual Studio Code, Project IDX also integrates seamlessly with GitHub, simplifying integration with existing workflows. In one of the latest releases, Google added built-in iOS and Android emulators for mobile developers directly into the IDE.&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/-wlZY4tfGMY&quot; title=&quot;Project IDX: Full-stack application development with generative AI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt; Feeling like you have to give it a go?. Check ProjectIDX &lt;a href=&quot;https://idx.dev/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Apple announced partnership with ChatGPT</title>
      <link href="http://localhost:4000/AppleGPT" rel="alternate" type="text/html" title="Apple announced partnership with ChatGPT" />
      <published>2024-06-10T00:00:00+03:00</published>
      <updated>2024-06-10T00:00:00+03:00</updated>
      <id>http://localhost:4000/AppleGPT</id>
      <content type="html" xml:base="http://localhost:4000/AppleGPT">&lt;p&gt;Apple is partnering with OpenAI to put ChatGPT into Siri, the company announced at its WWDC 2024 keynote on 10th of June 2024.&lt;/p&gt;

&lt;iframe width=&quot;600&quot; height=&quot;338&quot; src=&quot;https://www.youtube.com/embed/p2dhZ3AoDDs&quot; title=&quot;Biggest AI announcements from Apple&amp;#39;s WWDC 2024&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;During the keynote of its annual Worldwide Developers Conference (WWDC) on Monday, Apple unveiled a new generative artificial intelligence (AI) offering called Apple Intelligence. The company also announced a highly anticipated partnership with OpenAI.&lt;/p&gt;

&lt;p&gt;Apple Intelligence is designed as a personal intelligence system for iPhone, iPad, and Mac, combining generative models with personal context to enhance relevance. This offering aims to simplify everyday tasks and actions across various apps. Alongside Apple Intelligence, Apple introduced a privacy-focused solution called Private Cloud Compute. As both, OpenAI and Apple, sides of this agreement stated &lt;i&gt;&quot;Apple users are asked before any questions are sent to ChatGPT, along with any documents or photos, and Siri then presents the answer directly.&quot;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Additionally, Apple is integrating OpenAI’s ChatGPT into iOS 18, iPadOS 18, and macOS Sequoia. This integration, which includes features like the new Writing Tools and Siri, will be powered by OpenAI’s GPT-4o model and will be available later this year.&lt;/p&gt;

&lt;p&gt;For more info on the topic:
&lt;ul&gt;
&lt;li&gt; &lt;a href=&quot;https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/&quot;&gt;Apple&lt;/a&gt;'s press release about latest advancements on the topic&lt;/li&gt;
&lt;li&gt; &lt;a href=&quot;https://openai.com/index/openai-and-apple-announce-partnership/&quot;&gt;OpenAI&lt;/a&gt;'s announcement about the partnership&lt;/li&gt;&amp;lt;/li&amp;gt;
&amp;lt;/p&amp;gt;
&lt;/ul&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Apple is partnering with OpenAI to put ChatGPT into Siri, the company announced at its WWDC 2024 keynote on 10th of June 2024.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Anthropic’s now lets you create bots to work for you and interact with external APIs and tools</title>
      <link href="http://localhost:4000/AnthropicBots" rel="alternate" type="text/html" title="Anthropic's now lets you create bots to work for you and interact with external APIs and tools" />
      <published>2024-05-30T00:00:00+03:00</published>
      <updated>2024-05-30T00:00:00+03:00</updated>
      <id>http://localhost:4000/AnthropicBots</id>
      <content type="html" xml:base="http://localhost:4000/AnthropicBots">&lt;p&gt; &lt;a href=&quot;https://www.anthropic.com/news/tool-use-ga&quot;&gt;Tool use&lt;/a&gt;, which enables Claude to interact with external tools and APIs, is now generally available across the entire Claude 3 model family on the Anthropic Messages API, Amazon Bedrock, and Google Cloud's Vertex AI. With tool use, Claude can perform tasks, manipulate data, and provide more dynamic—and accurate—responses.&lt;/p&gt;

&lt;h2&gt; Tool use &lt;/h2&gt;

&lt;p&gt; Define a toolset for Claude and specify your request in natural language. Claude will then select the appropriate tool to fulfill the task and, when appropriate, execute the corresponding action: 

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Extract structured data from unstructured text&lt;/strong&gt;: Pull names, dates, and amounts from invoices to reduce manual data entry.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Convert natural language requests into structured API calls&lt;/strong&gt;: Enable teams to self-serve common actions (e.g., &quot;cancel subscription&quot;) with simple commands.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Answer questions by searching databases or using web APIs&lt;/strong&gt;: Provide instant, accurate responses to customer inquiries in support chatbots. &lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Automate simple tasks through software APIs&lt;/strong&gt;: Save time and minimize errors in data entry or file management. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orchestrate multiple fast Claude subagents for granular tasks&lt;/strong&gt;: Automatically find the optimal meeting time based on attendee availability. &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/b77htH1eX-s&quot; title=&quot;Claude 3 tool use: customer support&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Felling like you want to find out more?
&lt;ul&gt;
&lt;li&gt; Read the &lt;a href=&quot;https://docs.anthropic.com/en/docs/tool-use&quot;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Check the &lt;a href=&quot;https://github.com/anthropics/courses/tree/master/ToolUse&quot;&gt;tool use tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Explore the &lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/tool_use&quot;&gt;Anthropic Cookbooks on tool use&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Tool use, which enables Claude to interact with external tools and APIs, is now generally available across the entire Claude 3 model family on the Anthropic Messages API, Amazon Bedrock, and Google Cloud's Vertex AI. With tool use, Claude can perform tasks, manipulate data, and provide more dynamic—and accurate—responses.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Microsoft unveils Copilot + PCs, new Phi-3 models + Vision</title>
      <link href="http://localhost:4000/MicrosoftAnnouncementsCopilot" rel="alternate" type="text/html" title="Microsoft unveils Copilot + PCs, new Phi-3 models + Vision" />
      <published>2024-05-23T00:00:00+03:00</published>
      <updated>2024-05-23T00:00:00+03:00</updated>
      <id>http://localhost:4000/MicrosoftAnnouncementsCopilot</id>
      <content type="html" xml:base="http://localhost:4000/MicrosoftAnnouncementsCopilot">&lt;p&gt;As stated &lt;a href=&quot;https://ai.azure.com/explore/models/Phi-3-vision-128k-instruct/version/2/registry/azureml?tid=c2beb3f8-6ade-48ce-8e7d-6be943f1fbf7#overview&quot;&gt;here&lt;/a&gt;:

Phi-3 Vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.&lt;/p&gt;

&lt;p&gt;Resources and Technical Documentation: 
&lt;ul&gt;
&lt;li&gt; &lt;a href=&quot;https://aka.ms/phi3blog-april&quot;&gt;Phi-3 Microsoft Blog&lt;/a&gt; &lt;/li&gt;
&lt;li&gt; &lt;a href=&quot;https://aka.ms/phi3-tech-report&quot;&gt;Phi-3 Technocal Report&lt;/a&gt; &lt;li&gt;
&amp;lt;/ul&amp;gt;&amp;lt;/p&amp;gt;

&lt;p&gt;Moving forward, Microsoft has unveiled an ambitious new direction for its laptops, focusing on cutting-edge AI features that require a Neural Processing Unit (NPU). The new Copilot+ PCs badge designates approved AI-ready laptops, currently limited to models with Qualcomm Snapdragon X processors. While these laptops are not yet available, many are expected to hit the market soon.&lt;/p&gt;

&lt;p&gt;Microsoft emphasizes efficiency, value, and a new keyboard button as key elements of this new computing wave. This initiative aims to make AI PCs a significant and practical reality, bringing attention to Windows on Arm. As previously speculated, Microsoft is integrating its Copilot AI directly on local computers. Here’s a comprehensive overview of what we know from Microsoft’s Copilot+ PC press conference and prior information. 

Let's take a moment and watch the promotion video created by Microsoft.&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/5JmkWJNng2I&quot; title=&quot;Introducing Copilot+ PCs&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;So, what do you think about it? Amazed or feeling neutral according to the advancements and given the fact that Apple, Microsoft's rival, used NPU for first time at 2017 and at Microsoft the first usaged is recorded on 2021? Overall, my opinion is that we can get a lot out of this by using it properly. The future is exciting and I can't wait to see what may come!&lt;/p&gt;

&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">As stated here:</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Paragon changes RAG model for your customers</title>
      <link href="http://localhost:4000/paragon" rel="alternate" type="text/html" title="Paragon changes RAG model for your customers" />
      <published>2024-05-22T00:00:00+03:00</published>
      <updated>2024-05-22T00:00:00+03:00</updated>
      <id>http://localhost:4000/paragon</id>
      <content type="html" xml:base="http://localhost:4000/paragon">&lt;p&gt; &lt;i&gt;Integrate your multi-tenant AI SaaS with 100+ 3rd party apps with 70% less engineering.&lt;/i&gt;

&lt;p&gt; Today, third-party integrations are one of the most important parts of building a SaaS application. The average enterprise uses over 1,000 different cloud apps, and customers expect to buy software that integrates seamlessly with their other tools. However, achieving a robust set of high-quality product integrations typically requires anywhere from months to years of engineering work to build and maintain across multiple providers.&lt;/p&gt;

&lt;p&gt; Paragon is an embedded solution for integrating your product with third-party SaaS apps, providing your customers with a seamless, unified integration experience. This allows teams to avoid the cost, time, and risk that come with building and maintaining their own integrations solution. A single installation of Paragon takes just a couple of hours and enables your application to support integrations with the most popular SaaS apps.&lt;/p&gt;

&lt;p&gt; You implement Paragon by adding the Connect SDK to your application, which allows you to display the Connect Portal - a component that your users interact with in order to connect their third-party app accounts. Paragon provides fully managed authentication for each third-party app provider we support, and allows you to access your users' app accounts using Workflows or via the Paragon API.&lt;/p&gt;

&lt;p&gt; If you are interested in finding more about paragon, you can click &lt;a href=&quot;https://www.useparagon.com/paragon-for-ai?utm_source=alphasignal&amp;amp;utm_medium=newsletter&amp;amp;utm_content=ai_pitch&quot;&gt;here&lt;/a&gt; and explore it on you own pace! &lt;/p&gt; 
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Integrate your multi-tenant AI SaaS with 100+ 3rd party apps with 70% less engineering.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Grok comes to Europe</title>
      <link href="http://localhost:4000/GrokToEurope" rel="alternate" type="text/html" title="Grok comes to Europe" />
      <published>2024-05-16T00:00:00+03:00</published>
      <updated>2024-05-16T00:00:00+03:00</updated>
      <id>http://localhost:4000/GrokToEurope</id>
      <content type="html" xml:base="http://localhost:4000/GrokToEurope">&lt;p&gt; It has been announced that Grok AI model has expanded to Europe. &lt;/p&gt;

&lt;p&gt; For me Grok was not a well known AI chatbot. Was it because I live in Europe, was it because I didn't catch the announcements?!? I don't know. I will assume the same for you. As a result, let us first asnwer to the question what is Grok.&lt;/p&gt;

&lt;p&gt; As it is stated &lt;a href=&quot;https://x.ai/blog/grok&quot;&gt;here&lt;/a&gt; &lt;i&gt;&quot;Grok is an AI modeled after the Hitchhiker’s Guide to the Galaxy. It is intended to answer almost anything and, far harder, even suggest what questions to ask!&lt;/i&gt;&quot;. So Grok is an AI chatbot developed by Elon Musk's company xAI. By searching around, you can find out that Grok can access real-time information through social media platform X, which is kinda expected if you think who the owner is, and is said to answer &quot;spicy&quot; questions typically regected by most other AI systems. It can be accessed through a premium + X subscription. &lt;/p&gt;

&lt;p&gt; xAI says that Grok is able to respond to questions that most other chatbots would refuse, no matter the size of the risk. We can take a look at the shared &lt;a href=&quot;https://twitter.com/elonmusk/status/1720643054065873124?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1720643054065873124%7Ctwgr%5E7dcd667d00c687e5ff639f666b649cd9da8e7619%7Ctwcon%5Es1_&amp;amp;ref_url=https%3A%2F%2Fd-1606052890471219633.ampproject.net%2F2401262004000%2Fframe.html&quot;&gt;screenshot&lt;/a&gt; of Grok that is willing to provide a step-by-step guide to making cocaine for so-said &lt;i&gt;&quot;developmental purposes&quot;&lt;/i&gt;. In a previous post, Elon Musk posts, &lt;i&gt;xAI's Grok system is designed to have a little humor in its responses&lt;/i&gt; and we can see in the &quot;developemental post&quot; that is stated &lt;i&gt;&quot;start cooking and hope you don't blow yourself up or get arrested.&quot;&lt;/i&gt;. There are plently of such examples and cases, if you are willing to search around. Getting deeper into topic is not in the scope of this post.&lt;/p&gt;

&lt;h3&gt; How to sign and access Grok AI &lt;/h3&gt;

&lt;p&gt; To access Grok AI, you need to subscribe to the X Premium+ service, which is currently the gateway to Grok AI’s rich features. This subscription is available to X Premium+ subscribers and is priced at $16 a month. Once you have subscribed, you can visit the official Grok AI portal and authenticate yourself using your X credentials. After this, you will have access to the Grok AI platform and its unique features. &lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">It has been announced that Grok AI model has expanded to Europe.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Chatgpt new features announcements</title>
      <link href="http://localhost:4000/ChatTablesCharts" rel="alternate" type="text/html" title="Chatgpt new features announcements" />
      <published>2024-05-16T00:00:00+03:00</published>
      <updated>2024-05-16T00:00:00+03:00</updated>
      <id>http://localhost:4000/ChatTablesCharts</id>
      <content type="html" xml:base="http://localhost:4000/ChatTablesCharts">&lt;p&gt;OpenAI is taking AI capabilities to another level with its new ChatGPT feature. This new update enhances the user experience by allowing ChatGPT to interact with tables, charts, and add files directly from Google Drive and Microsoft OneDrive.&lt;/p&gt;

&lt;p&gt;The table and chart interaction feature enables ChatGPT to understand and interpret data in tables and charts effectively. This means it can now provide insights, draw conclusions, and answer queries related to data presented in these formats. Whether you're dealing with complex data sets or need a quick analysis, ChatGPT is equipped to assist you.&lt;/p&gt;

&lt;p&gt;Moreover, the update also adds a feature that allows for direct file addition from Google Drive and Microsoft OneDrive. This integration removes the hassle of downloading and re-uploading files. You can now easily fetch data from files stored in your cloud storage, making it more convenient and time-efficient.&lt;/p&gt;

&lt;p&gt;These new features reinforce OpenAI’s commitment to make ChatGPT more useful and versatile for all users. Whether you're a student, professional, or anyone in need of data interpretation or easy file access, these updates are designed to make your tasks easier.&lt;/p&gt;

&lt;p&gt;Don't hesitate to look for details &lt;a href=&quot;https://openai.com/index/improvements-to-data-analysis-in-chatgpt/&quot;&gt;here&lt;/a&gt; and how to use those new tools to get the most in your day-to-day tasks.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">OpenAI is taking AI capabilities to another level with its new ChatGPT feature. This new update enhances the user experience by allowing ChatGPT to interact with tables, charts, and add files directly from Google Drive and Microsoft OneDrive.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Google Announcements</title>
      <link href="http://localhost:4000/VeoAstraGemini" rel="alternate" type="text/html" title="Google Announcements" />
      <published>2024-05-14T00:00:00+03:00</published>
      <updated>2024-05-14T00:00:00+03:00</updated>
      <id>http://localhost:4000/VeoAstraGemini</id>
      <content type="html" xml:base="http://localhost:4000/VeoAstraGemini">&lt;p&gt;Just a day after OpenAI wowed us with GPT-4o, Google decided it’s their turn to dazzle! Let's dive into the goodies unveiled at the Google IO conference. &lt;/p&gt;

&lt;p&gt;Among the exciting announcements, we have:
&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;&lt;a href=&quot;https://deepmind.google/technologies/veo/&quot;&gt;Veo&lt;/a&gt;&lt;/strong&gt;: Google’s star video generation model.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;&lt;a href=&quot;https://deepmind.google/technologies/gemini/project-astra/&quot;&gt;Project Astra&lt;/a&gt;&lt;/strong&gt;: The futuristic AI assistant in the making.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;&lt;a href=&quot;https://deepmind.google/technologies/gemini/&quot;&gt;Gemini 1.5 Pro Updates&lt;/a&gt;&lt;/strong&gt;: Introducing two new versions of their flagship model – one sleeker, the other with a whopping 2M token context length.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt;Now, let’s break these down one by one:&lt;/p&gt;

&lt;h2&gt; Veo &lt;/h2&gt;
&lt;p&gt;Meet Veo, Google DeepMind's newest and shiniest video generation model. Here's what it can do:
&lt;ul&gt;
&lt;li&gt; Produce high-quality videos in 1080p &lt;/li&gt;
&lt;li&gt;&amp;gt; Extend beyond a minute of runtime &lt;/li&gt;
&lt;li&gt; Deliver a spectrum of cinematic and visual styles &lt;/li&gt;
&amp;lt;/p&amp;gt;

&lt;p&gt;Veo lets you input an image or video paired with a textual prompt. It can animate the image or edit the video as needed. Plus, it supports masked editing, so you can tweak specific areas by adding a mask and text prompt.
On the technical side, Google enhanced video captions in Veo’s training data and uses high-quality, compressed video representations (latents) to boost performance, speed, and efficiency.&lt;/p&gt;

&lt;h2&gt; Project Astra &lt;/h2&gt;
&lt;p&gt;Enter Astra, Google’s new project aimed at crafting the AI assistant of tomorrow, hot on the heels of OpenAI's GPT-4o demo.
Powered by Gemini, Astra supports real-time audio, text, video, and image interactions. Although it's still a prototype, Astra’s demo came through pre-recorded videos since it’s not yet widely available.
Early testers noted a bit of lag, less emotional intelligence, and tone compared to GPT-4o, but praised its text-to-speech prowess and potentially superior long-context video support.&lt;/p&gt;

&lt;h2&gt; Gemini 1.5 Pro &lt;/h2&gt;

&lt;p&gt;Google rolled out two new iterations of their flagship model, Gemini 1.5 Pro:
&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Gemini 1.5 Pro Flash&lt;/strong&gt;: A nimble, fast, and cost-efficient version, still multimodal with a 1M token context length. It boasts an MMLU of 78.9% vs. 81.9% for the original Gemini 1.5 Pro.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Gemini 1.5 Pro&lt;/strong&gt;: This powerhouse’s context length is doubled to 2M tokens and is currently accessible via a waitlist for select API developers.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;h2&gt; Other Announcements &lt;/h2&gt;
&lt;p&gt;Google also revealed:
&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Imagen 3&lt;/strong&gt;: Their most advanced image generation model, available in versions tailored for tasks ranging from quick sketches to high-res images.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Gemma 2 and PaliGemma&lt;/strong&gt; New open-source models. PaliGemma is Google’s inaugural vision-language model and is available now. Gemma 2, with 27B parameters, outperforms its predecessor and will be available in June. &lt;/li&gt;
&lt;/ul&gt;
The jam-packed 2-hour session also featured updates across Google’s ecosystem, including Search, Workspace, Photos, Android, and more.
&lt;/p&gt;

&lt;h2&gt; Access &lt;/h2&gt;
&lt;p&gt;
The Gemini API and Google AI Studio are now live in over 200 countries. Gemini 1.5 Flash is priced at $0.35 per 1M tokens, with context caching launching next month.
While Veo, Astra, and the 2M context Gemini 1.5 Pro aren’t available yet, you can join the waitlist for access. But no worries, Gemini 1.5 Pro Flash is ready for you through the API, and PaliGemma is freely available on Kaggle.
Time to explore these futuristic tools and see where they can take us!&lt;/p&gt;
&lt;/ul&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Just a day after OpenAI wowed us with GPT-4o, Google decided it’s their turn to dazzle! Let's dive into the goodies unveiled at the Google IO conference.</summary>
      

      
      
    </entry>
  
</feed>
