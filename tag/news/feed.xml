<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/news/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-07-18T15:48:40+03:00</updated>
  <id>http://localhost:4000/tag/news/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">MInference</title>
      <link href="http://localhost:4000/MInference" rel="alternate" type="text/html" title="MInference" />
      <published>2024-07-07T00:00:00+03:00</published>
      <updated>2024-07-07T00:00:00+03:00</updated>
      <id>http://localhost:4000/MInference</id>
      <content type="html" xml:base="http://localhost:4000/MInference">&lt;p&gt;&lt;q&gt; Now, you can process 1M context 10x faster in a single A100 using Long-context LLMs like LLaMA-3-8B-1M, GLM-4-1M, with even better accuracy, try MInference 1.0 right now!&lt;/q&gt; as stated in the announcement.&lt;/p&gt;

&lt;p&gt;Microsoft has recently unveiled MInference, an open-source library designed to streamline and enhance the process of machine learning inference. This repository, hosted on &lt;a href=&quot;https://github.com/microsoft/MInference?tab=readme-ov-file&quot;&gt;GitHub&lt;/a&gt;, offers a comprehensive suite of tools and resources aimed at developers and data scientists looking to integrate efficient inference capabilities into their applications.&lt;/p&gt;

&lt;p&gt; Some of the key features are 

&lt;ol&gt; 
    &lt;li&gt; High Performance: MInference is optimized for speed and efficiency. This will make it certain for you to have rapid inference times even with large and complex models. This makes it ideal for real-time applications where quick response times are critical for you and/or your business.&lt;/li&gt;
    &lt;li&gt; Versatility: The library supports a wide range of machine learning models and frameworks. Whether you are working with PyTorch of TensorFlow MInference provides seamless integration, allowing for flexibility and ease of use across different platforms and environments. Take a look at the &lt;a href=&quot;https://github.com/microsoft/MInference/tree/main/examples&quot;&gt;examples&lt;/a&gt; to figure out more about how to use it.&lt;/li&gt;
    &lt;li&gt; Scalability: Designed with scalability in mind, MInference can handle inference tasks from small-scale projects to large, enterprise-level applications. This scalability ensures that as your data and model complexity grow, MInference can accommodate and maintain performance. Keep in mind though that currently the supported models are:&lt;/li&gt;
    &lt;ul&gt;
        &lt;li&gt; LLaMA-3: &lt;a href=&quot;https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k&quot;&gt;gradientai/Llama-3-8B-Instruct-262k&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k&quot;&gt;gradientai/Llama-3-8B-Instruct-Gradient-1048k&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-4194k&quot;&gt;gradientai/Llama-3-8B-Instruct-Gradient-4194k&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt; GLM-4: &lt;a href=&quot;https://huggingface.co/THUDM/glm-4-9b-chat-1m&quot;&gt;THUDM/glm-4-9b-chat-1m&lt;/a&gt; &lt;/li&gt;
        &lt;li&gt; Yi: &lt;a href=&quot;https://huggingface.co/01-ai/Yi-9B-200K&quot;&gt;01-ai/Yi-9B-200K&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt; Phi-3: &lt;a href=&quot;https://huggingface.co/microsoft/Phi-3-mini-128k-instruct&quot;&gt;microsoft/Phi-3-mini-128k-instruct&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt; Qwen2: &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-7B-Instruct&quot;&gt;Qwen/Qwen2-7B-Instruct&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;li&gt; User-Friendly API: MInference boasts a user-friendly API, making it accessible for both beginners and experienced practitioners. The well-documented functions and examples facilitate a smooth learning curve and quick implementation.&lt;/li&gt;
    &lt;li&gt; Community and Support: As an open-source project, MInference benefits from the contributions and support of the developer community. You can take advantage of this open-source project to contibute and/or get any support resolving any issues that may come up.&lt;/li&gt;

&lt;p&gt;So to get statred with MInference, in case you feel like it, you check the detailed README file that has the instructions to install anything needed acompanied with examples and documentations. Whether you are a developer of a data scientist I think you will find it tempting to include and impletement efficient machine learning inference in your project so my advice, take a look at is and see if it fits your likes.&lt;/p&gt;

&lt;p&gt; If you are interested about reading more, there is a paper, currently in review that you can find &lt;a href=&quot;https://arxiv.org/abs/2407.02490&quot;&gt;here&lt;/a&gt;&lt;/p&gt;. 
&lt;/ol&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Now, you can process 1M context 10x faster in a single A100 using Long-context LLMs like LLaMA-3-8B-1M, GLM-4-1M, with even better accuracy, try MInference 1.0 right now! as stated in the announcement.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Gen-3 Alpha opened by Runway</title>
      <link href="http://localhost:4000/RunwayTTV" rel="alternate" type="text/html" title="Gen-3 Alpha opened by Runway" />
      <published>2024-07-06T00:00:00+03:00</published>
      <updated>2024-07-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/RunwayTTV</id>
      <content type="html" xml:base="http://localhost:4000/RunwayTTV">&lt;p&gt;As it is stated &lt;a href=&quot;https://runwayml.com/blog/introducing-gen-3-alpha/&quot;&gt;here&lt;/a&gt; &lt;q&gt;Gen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;Based on the results we see, we should be excited about upcoming Text to Video, Image to Video and Text to Image tools that will be available for us to use. You can take a deeper view on their products &lt;a href=&quot;https://runwayml.com/&quot;&gt;here&lt;/a&gt; and try runway for free &lt;a href=&quot;https://app.runwayml.com/signup&quot;&gt;here.&lt;/a&gt; I don't know about you but I can't wait to check the upcoming models they have to offer!&lt;/p&gt;

&lt;p&gt;For those of you that have not yet got the chance to know Runway, it is founded in 2018 by Cristóbal Valenzuela, Anastasis Germanidis and Alejandro Matamala-Ortiz. As Paul Drews and Emily Zhao writting at &lt;a href=&quot;https://salesforceventures.com/perspectives/welcome-runway/&quot;&gt;Salesforce ventures.&lt;/a&gt; The initial idea came out of Cris’ thesis project at the Interactive Telecommunications Program at NYU, where he met his co-founders while researching applications of machine learning models for image and video use in the creative domains.&lt;/p&gt;

&lt;p&gt;Informed by their own experiences as artists, the Runway co-founders set out to answer the question of how a well-built digital tool could simplify the approachability of complex ML models and circumvent the need for deep technical background to give better access to state of the art machine intelligence to artists and designers. The mission of Runway was to democratize access to machine learning so more people can start thinking of new and creative ways to use those models.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">As it is stated here Gen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Gemini 1.5 Pro 2M context window</title>
      <link href="http://localhost:4000/GoogleAIStudio" rel="alternate" type="text/html" title="Gemini 1.5 Pro 2M context window" />
      <published>2024-07-06T00:00:00+03:00</published>
      <updated>2024-07-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/GoogleAIStudio</id>
      <content type="html" xml:base="http://localhost:4000/GoogleAIStudio">&lt;p&gt;&lt;q&gt;Gemini 1.5 Pro 2M context window, code execution capabilities, and Gemma 2 are available today&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;As it is stated in the official blog post of &lt;a href=&quot;https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/&quot;&gt;Google for developers&lt;/a&gt; it is decided to give developers access to the 2 million context window for Gemini 1.5 Pro, code execution capabilities in the Gemini API, and adding Gemma 2 in Google AI Studio.So Google Cloud is making two variations of its flagship AI model—Gemini 1.5 Flash and Pro—publicly accessible.&lt;/p&gt;

&lt;p&gt;A small multimodal model with a 1 million context window that tackles narrow high-frequency tasks and the most powerful version of Google’s LLM, upgraded to contain a 2 million context window, respectively are open to all developers to try and experiment with.&lt;/p&gt;

&lt;p&gt;Through this action Gemini variations aims to showcase how Google AI's work will assist businesses to develop, monitor annd/or expand challenging AI agents to find delicate solutions to their problems. During a press announcement, Google Cloud Chief Executive Thomas Kurian boasts the company sees “incredible momentum” with its generative AI recent developments, with organizations such as Accenture, Airbus, Anthropic, Box, Broadcom, Cognizant, Confluent, Databricks, Deloitte, Equifax, Estée Lauder Companies, Ford, GitLab, GM, the Golden State Warriors, Goldman Sachs, Hugging Face, IHG Hotels and Resorts, Lufthansa Group, Moody’s, Samsung, and others building on its platform. He attributes this adoption growth to the combination of what Google’s models are capable of and the company’s Vertex platform. It’ll &lt;q&gt;continue to introduce new capability in both those layers at a rapid pace.&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;You can find out more about recent advancements &lt;a href=&quot;https://developers.google.com/&quot;&gt;here&lt;/a&gt; under the category, Trending news.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Gemini 1.5 Pro 2M context window, code execution capabilities, and Gemma 2 are available today</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Google releases Gemma 2</title>
      <link href="http://localhost:4000/Gemma2" rel="alternate" type="text/html" title="Google releases Gemma 2" />
      <published>2024-06-27T00:00:00+03:00</published>
      <updated>2024-06-27T00:00:00+03:00</updated>
      <id>http://localhost:4000/Gemma2</id>
      <content type="html" xml:base="http://localhost:4000/Gemma2">&lt;p&gt;Google has introduced Gemma 2, its latest generation of open AI models, aimed at enhancing research and development in artificial intelligence. With 9B and 27B parameter versions, Gemma 2 boasts significant improvements in performance and efficiency, making it cost-effective to deploy on common hardware like NVIDIA GPUs. The model is designed for seamless integration with existing AI tools and frameworks, ensuring swift and efficient inference processes.&lt;/p&gt;

&lt;p&gt;Gemma 2 stands out for its focus on responsible AI development. It incorporates advanced safety features and provides open-sourced evaluation tools to facilitate ethical AI research. This ensures that developers can work with the model while adhering to best practices in AI safety and responsibility. The availability of these tools underscores Google's commitment to fostering a trustworthy AI ecosystem.&lt;/p&gt;

&lt;p&gt;Researchers and developers can access Gemma 2 through various platforms, including Google AI Studio, Kaggle, and Hugging Face. This accessibility is intended to encourage widespread experimentation and innovation within the AI community. By providing such a powerful and open resource, Google aims to accelerate advancements in AI and support the development of new, groundbreaking applications.&lt;/p&gt;

&lt;p&gt;For more details, visit the official &lt;a href=&quot;https://blog.google/technology/developers/google-gemma-2/&quot;&gt;Google blog post&lt;/a&gt;.
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Google has introduced Gemma 2, its latest generation of open AI models, aimed at enhancing research and development in artificial intelligence. With 9B and 27B parameter versions, Gemma 2 boasts significant improvements in performance and efficiency, making it cost-effective to deploy on common hardware like NVIDIA GPUs. The model is designed for seamless integration with existing AI tools and frameworks, ensuring swift and efficient inference processes.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Meta releases New AI Research Models to Accelerate Innovation at Scale</title>
      <link href="http://localhost:4000/MetaNewAI" rel="alternate" type="text/html" title="Meta releases New AI Research Models to Accelerate Innovation at Scale" />
      <published>2024-06-18T00:00:00+03:00</published>
      <updated>2024-06-18T00:00:00+03:00</updated>
      <id>http://localhost:4000/MetaNewAI</id>
      <content type="html" xml:base="http://localhost:4000/MetaNewAI">&lt;p&gt; For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever. &lt;/p&gt;

&lt;p&gt; Today, we shared some of our latest FAIR research models with the world. These publicly released models include image-to-text and text-to-music generation models, a multi-token prediction model, and a technique for detecting AI-generated speech. By making this research publicly available, Meta aims to inspire further iterations and ultimately promote responsible advancements in AI. &lt;/p&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://www.youtube.com/embed/p9oM5dWmFZ0&quot; title=&quot;Meet Meta Chameleon&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For more information you can see the official meta &lt;a href=&quot;https://about.fb.com/news/2024/06/releasing-new-ai-research-models-to-accelerate-innovation-at-scale/&quot;&gt;announcement&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">NVIDIA announced Nemotron 340B</title>
      <link href="http://localhost:4000/NVIDIARelease" rel="alternate" type="text/html" title="NVIDIA announced Nemotron 340B" />
      <published>2024-06-14T00:00:00+03:00</published>
      <updated>2024-06-14T00:00:00+03:00</updated>
      <id>http://localhost:4000/NVIDIARelease</id>
      <content type="html" xml:base="http://localhost:4000/NVIDIARelease">&lt;p&gt;Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.&lt;/p&gt;

&lt;p&gt;NVIDIA on 14th of June, announced Nemotron-4 340B, a family of open models that developers can use to generate synthetic data for training large language models (LLMs) for commercial applications across healthcare, finance, manufacturing, retail and every other industry. &lt;/p&gt;

&lt;p&gt;High-quality training data plays a critical role in the performance, accuracy and quality of responses from a custom LLM — but robust datasets can be prohibitively expensive and difficult to access. &lt;/p&gt;

&lt;p&gt;Through a uniquely permissive open model license, Nemotron-4 340B gives developers a free, scalable way to generate synthetic data that can help build powerful LLMs.&lt;/p&gt;

&lt;p&gt;The Nemotron-4 340B family includes base, instruct and reward models that form a pipeline to generate synthetic data used for training and refining LLMs. The models are optimized to work with NVIDIA NeMo, an open-source framework for end-to-end model training, including data curation, customization and evaluation. They’re also optimized for inference with the open-source NVIDIA TensorRT-LLM library. &lt;/p&gt;

&lt;p&gt;Nemotron-4 340B can be downloaded now from the NVIDIA NGC catalog and Hugging Face. Developers will soon be able to access the models at ai.nvidia.com, where they’ll be packaged as an NVIDIA NIM microservice with a standard application programming interface that can be deployed anywhere.&lt;/p&gt;

&lt;p&gt;If you want to find out more go to the &lt;a href=&quot;https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/&quot;&gt;official blog of NVIDIA&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Google open Project</title>
      <link href="http://localhost:4000/GoogleProjectIDX" rel="alternate" type="text/html" title="Google open Project" />
      <published>2024-06-14T00:00:00+03:00</published>
      <updated>2024-06-14T00:00:00+03:00</updated>
      <id>http://localhost:4000/GoogleProjectIDX</id>
      <content type="html" xml:base="http://localhost:4000/GoogleProjectIDX">&lt;p&gt;During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.&lt;/p&gt;

&lt;p&gt; &lt;i&gt;&quot;As AI becomes more prevalent, the complexities that come with deploying all of that really becomes harder, becomes greater, and we wanted to help solve that challenge,&quot;&lt;/i&gt; said Jeanine Banks, Google’s VP and general manager for Developer X and the company’s head of developer relations.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;&quot;That’s why we built project IDX, a multi-platform development experience that makes building applications fast and easy. Project IDX makes it really frictionless to get going with your preferred framework or language with easy-to-use templates like Next.js, Astro, Flutter, Dart, Angular, Go and more.&quot;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt; With this update, Google is integrating the Google Maps Platform into Project IDX, enabling the addition of geolocation features to apps. The update also includes integrations with Chrome Dev Tools and Lighthouse to assist in debugging applications. Soon, developers will be able to deploy apps to Cloud Run, Google Cloud's serverless platform for front- and back-end services.&lt;/p&gt;

&lt;p&gt; The development environment will also integrate with Checks, Google's AI-powered compliance platform, which is transitioning from beta to general availability on Tuesday.&lt;/p&gt;

&lt;p&gt; Project IDX isn't just about building AI-enabled applications; it's also about using AI to enhance the coding process. To facilitate this, IDX includes standard features like code completion and a chat assistant sidebar, as well as innovative tools like the ability to highlight a snippet of code and use Google's Gemini model to modify it, similar to generative fill in Photoshop.&lt;/p&gt;

&lt;p&gt; Whenever Gemini suggests code, it provides links back to the original source and its associated license.&lt;/p&gt;

&lt;p&gt; Built on the open-source Visual Studio Code, Project IDX also integrates seamlessly with GitHub, simplifying integration with existing workflows. In one of the latest releases, Google added built-in iOS and Android emulators for mobile developers directly into the IDE.&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/-wlZY4tfGMY&quot; title=&quot;Project IDX: Full-stack application development with generative AI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt; Feeling like you have to give it a go?. Check ProjectIDX &lt;a href=&quot;https://idx.dev/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Apple announced partnership with ChatGPT</title>
      <link href="http://localhost:4000/AppleGPT" rel="alternate" type="text/html" title="Apple announced partnership with ChatGPT" />
      <published>2024-06-10T00:00:00+03:00</published>
      <updated>2024-06-10T00:00:00+03:00</updated>
      <id>http://localhost:4000/AppleGPT</id>
      <content type="html" xml:base="http://localhost:4000/AppleGPT">&lt;p&gt;Apple is partnering with OpenAI to put ChatGPT into Siri, the company announced at its WWDC 2024 keynote on 10th of June 2024.&lt;/p&gt;

&lt;iframe width=&quot;600&quot; height=&quot;338&quot; src=&quot;https://www.youtube.com/embed/p2dhZ3AoDDs&quot; title=&quot;Biggest AI announcements from Apple&amp;#39;s WWDC 2024&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;During the keynote of its annual Worldwide Developers Conference (WWDC) on Monday, Apple unveiled a new generative artificial intelligence (AI) offering called Apple Intelligence. The company also announced a highly anticipated partnership with OpenAI.&lt;/p&gt;

&lt;p&gt;Apple Intelligence is designed as a personal intelligence system for iPhone, iPad, and Mac, combining generative models with personal context to enhance relevance. This offering aims to simplify everyday tasks and actions across various apps. Alongside Apple Intelligence, Apple introduced a privacy-focused solution called Private Cloud Compute. As both, OpenAI and Apple, sides of this agreement stated &lt;i&gt;&quot;Apple users are asked before any questions are sent to ChatGPT, along with any documents or photos, and Siri then presents the answer directly.&quot;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Additionally, Apple is integrating OpenAI’s ChatGPT into iOS 18, iPadOS 18, and macOS Sequoia. This integration, which includes features like the new Writing Tools and Siri, will be powered by OpenAI’s GPT-4o model and will be available later this year.&lt;/p&gt;

&lt;p&gt;For more info on the topic:
&lt;ul&gt;
&lt;li&gt; &lt;a href=&quot;https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/&quot;&gt;Apple&lt;/a&gt;'s press release about latest advancements on the topic&lt;/li&gt;
&lt;li&gt; &lt;a href=&quot;https://openai.com/index/openai-and-apple-announce-partnership/&quot;&gt;OpenAI&lt;/a&gt;'s announcement about the partnership&lt;/li&gt;&amp;lt;/li&amp;gt;
&amp;lt;/p&amp;gt;
&lt;/ul&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Apple is partnering with OpenAI to put ChatGPT into Siri, the company announced at its WWDC 2024 keynote on 10th of June 2024.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Anthropic’s now lets you create bots to work for you and interact with external APIs and tools</title>
      <link href="http://localhost:4000/AnthropicBots" rel="alternate" type="text/html" title="Anthropic's now lets you create bots to work for you and interact with external APIs and tools" />
      <published>2024-05-30T00:00:00+03:00</published>
      <updated>2024-05-30T00:00:00+03:00</updated>
      <id>http://localhost:4000/AnthropicBots</id>
      <content type="html" xml:base="http://localhost:4000/AnthropicBots">&lt;p&gt; &lt;a href=&quot;https://www.anthropic.com/news/tool-use-ga&quot;&gt;Tool use&lt;/a&gt;, which enables Claude to interact with external tools and APIs, is now generally available across the entire Claude 3 model family on the Anthropic Messages API, Amazon Bedrock, and Google Cloud's Vertex AI. With tool use, Claude can perform tasks, manipulate data, and provide more dynamic—and accurate—responses.&lt;/p&gt;

&lt;h2&gt; Tool use &lt;/h2&gt;

&lt;p&gt; Define a toolset for Claude and specify your request in natural language. Claude will then select the appropriate tool to fulfill the task and, when appropriate, execute the corresponding action: 

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Extract structured data from unstructured text&lt;/strong&gt;: Pull names, dates, and amounts from invoices to reduce manual data entry.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Convert natural language requests into structured API calls&lt;/strong&gt;: Enable teams to self-serve common actions (e.g., &quot;cancel subscription&quot;) with simple commands.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Answer questions by searching databases or using web APIs&lt;/strong&gt;: Provide instant, accurate responses to customer inquiries in support chatbots. &lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Automate simple tasks through software APIs&lt;/strong&gt;: Save time and minimize errors in data entry or file management. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orchestrate multiple fast Claude subagents for granular tasks&lt;/strong&gt;: Automatically find the optimal meeting time based on attendee availability. &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/b77htH1eX-s&quot; title=&quot;Claude 3 tool use: customer support&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Felling like you want to find out more?
&lt;ul&gt;
&lt;li&gt; Read the &lt;a href=&quot;https://docs.anthropic.com/en/docs/tool-use&quot;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Check the &lt;a href=&quot;https://github.com/anthropics/courses/tree/master/ToolUse&quot;&gt;tool use tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Explore the &lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/tool_use&quot;&gt;Anthropic Cookbooks on tool use&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">Tool use, which enables Claude to interact with external tools and APIs, is now generally available across the entire Claude 3 model family on the Anthropic Messages API, Amazon Bedrock, and Google Cloud's Vertex AI. With tool use, Claude can perform tasks, manipulate data, and provide more dynamic—and accurate—responses.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Microsoft unveils Copilot + PCs, new Phi-3 models + Vision</title>
      <link href="http://localhost:4000/MicrosoftAnnouncementsCopilot" rel="alternate" type="text/html" title="Microsoft unveils Copilot + PCs, new Phi-3 models + Vision" />
      <published>2024-05-23T00:00:00+03:00</published>
      <updated>2024-05-23T00:00:00+03:00</updated>
      <id>http://localhost:4000/MicrosoftAnnouncementsCopilot</id>
      <content type="html" xml:base="http://localhost:4000/MicrosoftAnnouncementsCopilot">&lt;p&gt;As stated &lt;a href=&quot;https://ai.azure.com/explore/models/Phi-3-vision-128k-instruct/version/2/registry/azureml?tid=c2beb3f8-6ade-48ce-8e7d-6be943f1fbf7#overview&quot;&gt;here&lt;/a&gt;:

Phi-3 Vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.&lt;/p&gt;

&lt;p&gt;Resources and Technical Documentation: 
&lt;ul&gt;
&lt;li&gt; &lt;a href=&quot;https://aka.ms/phi3blog-april&quot;&gt;Phi-3 Microsoft Blog&lt;/a&gt; &lt;/li&gt;
&lt;li&gt; &lt;a href=&quot;https://aka.ms/phi3-tech-report&quot;&gt;Phi-3 Technocal Report&lt;/a&gt; &lt;li&gt;
&amp;lt;/ul&amp;gt;&amp;lt;/p&amp;gt;

&lt;p&gt;Moving forward, Microsoft has unveiled an ambitious new direction for its laptops, focusing on cutting-edge AI features that require a Neural Processing Unit (NPU). The new Copilot+ PCs badge designates approved AI-ready laptops, currently limited to models with Qualcomm Snapdragon X processors. While these laptops are not yet available, many are expected to hit the market soon.&lt;/p&gt;

&lt;p&gt;Microsoft emphasizes efficiency, value, and a new keyboard button as key elements of this new computing wave. This initiative aims to make AI PCs a significant and practical reality, bringing attention to Windows on Arm. As previously speculated, Microsoft is integrating its Copilot AI directly on local computers. Here’s a comprehensive overview of what we know from Microsoft’s Copilot+ PC press conference and prior information. 

Let's take a moment and watch the promotion video created by Microsoft.&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/5JmkWJNng2I&quot; title=&quot;Introducing Copilot+ PCs&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;So, what do you think about it? Amazed or feeling neutral according to the advancements and given the fact that Apple, Microsoft's rival, used NPU for first time at 2017 and at Microsoft the first usaged is recorded on 2021? Overall, my opinion is that we can get a lot out of this by using it properly. The future is exciting and I can't wait to see what may come!&lt;/p&gt;

&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="news" />
      

      
        <summary type="html">As stated here:</summary>
      

      
      
    </entry>
  
</feed>
