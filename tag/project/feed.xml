<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/project/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2025-03-29T16:25:05+02:00</updated>
  <id>http://localhost:4000/tag/project/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Multiple datasources - Route selections</title>
      <link href="http://localhost:4000/rag3" rel="alternate" type="text/html" title="Multiple datasources - Route selections" />
      <published>2024-09-09T00:00:00+03:00</published>
      <updated>2024-09-09T00:00:00+03:00</updated>
      <id>http://localhost:4000/rag3</id>
      <content type="html" xml:base="http://localhost:4000/rag3">&lt;p&gt; I hope you enjoy every step so far. Until this point of our Langchain/RAG journey, we have managed to build a simple local application and a querry transformation assistant. But what happens when we have multiple data sources? How to define where our application will retrieve the required information from? The definition of this process, finding the correct road, or better let's say finding the correct route, is called Routing.&lt;/p&gt;

&lt;p&gt; There are many ways to give directions in real life, this applies here as well! As we can see in the following image, there are two main categories of routing. 

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Logical Routing&lt;/strong&gt;: Let LLM choose a DB based on the question.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Semantic Routing&lt;/strong&gt;: Embed question and choose prompt based on similarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:1400/1*iBm4xuEwvnp9KFyH9x05cw.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt; Before going though the architecture of such a RAG model for either Logical or Semantic routing, we are going to take a look at the differences accompanied with some simplier examples to understand which is best for your case case to go with.&lt;/p&gt;

&lt;h3&gt; Logical Routing &lt;/h3&gt;

&lt;p&gt; Routes are defined based on pre-determined rules, conditions, or algorithms. These rules are purely technical, and they don't take into account the meaning or context of the data. Logical Routing decisions are made based on factors like IP addresses, routing tables, or specific conditions that match certain criteria. It focuses on efficiently directing traffic or requests based on a logical or structural framework.&lt;/p&gt;

&lt;p&gt;To put logical routing straight, imagine youâ€™re in a city, and there are different routes based on traffic signals or road rules. These routes don't care about where you're coming from or your personal preferences, but simply about making sure traffic moves according to the road signs.

&lt;ul&gt;
&lt;li&gt; Situation: Youâ€™re driving from Point A to Point B.&lt;/li&gt;
&lt;li&gt; Logical Rule: If a road is closed, take the next available road.&lt;/li&gt;
&lt;li&gt; Routing Decision: You don't think about why the road is closed; you just follow the detour and proceed based on traffic rules. It's like having a GPS system giving you orders, while driving, in a city that you have never been.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;h3&gt; Semantic Routing &lt;/h3&gt;

&lt;p&gt;  Routes are chosen based on the meaning or context of the data being transmitted. The logical routing decisions consider the content, purpose, or relationships within the data. It uses metadata or content analysis to decide the best route for a request, often ensuring that the most relevant resources handle the data. It prioritizes meaning and relevance to ensure that the request or data is handled in the best possible way based on its content.&lt;/p&gt;

&lt;p&gt; Imagine you're in a library, and someone asks where to find a book. Instead of directing them based on just shelf numbers (logical), you direct them based on their interest in the content of the book. You ask, &quot;What topic are you looking for?&quot; and route them accordingly to the right section.

&lt;ul&gt;
&lt;li&gt; Situation: You want to ask a question, but there are different people who can answer it.&lt;/li&gt;
&lt;li&gt; Semantic Rule: If your question is about science, youâ€™re directed to a scientist; if it's about cooking, youâ€™re directed to a chef.&lt;/li&gt;
&lt;li&gt; Routing Decision: The decision isnâ€™t based on random criteria but on the meaning of your question and whoâ€™s best equipped to answer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;
Based on Data/Context, Logical Routing ignores the meaning of the data and routes based on fixed rules while semantic routing analyzes the meaning and routes accordingly. Overall and while looking at the purpose, logical routing focuses on efficiency and technical structure and on the other hand, semantic routing focuses on relevance and meaningful processing.
&lt;/p&gt;

&lt;p&gt; Now that we got the main idea about what is the meaning of both, let us take a closer look to the code and how to build a system like that.&lt;/p&gt;

&lt;p&gt; Initially, we need to call the modules that are going to help us accomplish our goal. I am not going to go through &lt;code&gt;.env&lt;/code&gt; file again and won't be mentioning this file again in the future. Hopefully, you got the idea behind it, why it is important and the reasons I insist on using one. If you are not so sure on what file I am referring to, please take a look at the first two notebooks of this series and follow the steps to create one. Remember &lt;code&gt;.env&lt;/code&gt; file will keep your Usernames, Passwords and Endpoint IDs safe without the need to erase them or hide them when sharing any file. So the modules we are going to use within this notebook are the following.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import AzureOpenAIEmbeddings
# from langchain.chat_models import AzureChatOpenAI
from langchain_openai import AzureChatOpenAI
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from typing import Literal
from langchain.utils.math import cosine_similarity
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda

from dotenv import load_dotenv, dotenv_values
load_dotenv()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we are using LLM models we need to define embeddings so that we can communicate with the computers and we &lt;i&gt;&quot;talk&quot;&lt;/i&gt; the same language.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;embeddings = AzureOpenAIEmbeddings(
    deployment = os.getenv('OPENAI_DEPLOYMENT_NAME'),
    model = os.getenv('OPENAI_MODEL_NAME_EMB'),
    azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),
    openai_api_type = os.getenv('OPENAI_API_TYPE'),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Apart from that, &lt;strong&gt;vectorstore&lt;/strong&gt; is essential as we need a place to store the vectors that will result from our embeddings call. Hecne, we are going to define a function call &lt;i&gt;vectorstor_gen&lt;/i&gt; in order to store the information acquired (vectors generatated) from the files one owns and what to search from.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def vectorstore_gen(file, dir):
    loader = PyPDFLoader(file)
    documents = loader.load()

    # Split text into chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)
    text_chunks = text_splitter.split_documents(documents)

    vectorstore = Chroma.from_documents(documents=text_chunks,
                                        embedding=embeddings,
                                        persist_directory=dir)
    vectorstore.persist()
    return vectorstore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now image you have two documents or batter let's say two different files containing documents, and you want to create two vectorstore for both of them so that our llm will be able to choose betwen then and answer to your questions. Remember that we have created a folder data in our project and there we store all the data that we want to use for our RAG projects. In this example I am going to use dummy pdf names (&lt;i&gt;A.pdf&lt;/i&gt; and &lt;i&gt;B.pdf&lt;/i&gt;) and imaging that those two files contain information/documentation about different LLM models. Feel free to replace those with two irrelevant pdf files (when it comes to topic)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# Create a vectorestore to answer questions about topic A
vectorstore_A = vectorstore_gen('data/A.pdf', 'data/vectorstore_A')
# Create a vectorstore to answer questions about topic B
vectorstore_B = vectorstore_gen('data/B.pdf', 'data/vectorstore_B')

retriever_A = vectorstore_A.as_retriever(search_kwargs={'k':5})
retriever_B = vectorstore_B.as_retriever(search_kwargs={'k':5})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now it is time to create the classes we are going to use to our routing models. Initially we are going to start with Logical Routing. Initially, we are going to build a class to route to the most relevant datasource accourding to the User's question. We are going to use an extention of &lt;code&gt;BaseModel&lt;/code&gt;. The &lt;code&gt;BaseModel&lt;/code&gt; class provides automatic validation parsing and serialization of data. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
class QueryRouter(BaseModel):
    &quot;&quot;&quot;Route a user query to the appropriate datasource that will help answer the query accurately&quot;&quot;&quot;
    datasource: Literal['A', 'B', 'general'] = Field(..., description=&quot;Given a user question choose which datasource would be most relevant for answering their question&quot;)
    question: str = Field(..., description=&quot;User question to be routed to the appropriate datasource&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  Next we need to build and and call an LLM model that will help us build our RAG model, within which we are going to include routing stratey.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
llm = AzureChatOpenAI(
    deployment_name = os.getenv('LLM_35_Deployment'),
    model_name = os.getenv('LLM_35_Model'),
    azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),
    temperature = 0
)

structured_llm = llm.with_structured_output(QueryRouter, method=&quot;function_calling&quot;, include_raw=True)
structured_llm.invoke(&quot;Which datasource should be used for a question about general knowledge?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it is time to build the routing prompt that will be included in the RAG pipeline.  &lt;strong&gt;Not that&lt;/strong&gt; if the pdf files that you include are not about a topic of NLP and/or LLMs pleas read the system prompt and change it accordingly&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
router_prompt = ChatPromptTemplate.from_messages(
    [
        (&quot;system&quot;,
         &quot;You are an expert router that can direct user queries to the appropriate datasource. Route the following user question about a topic in NLP and LLMs to the appropriate datasource.\nIf it is a general question not related to the provided datasources, route it to the general datasource.\n&quot;),
        (&quot;user&quot;, &quot;{question}&quot;)
    ]
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Finally we are ready to create the first router pipeline for our RAG model and include everythin we have been creating so far. &lt;strong&gt;Not that&lt;/strong&gt; change the upcoming &lt;code&gt;question&lt;/code&gt; accordingly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
router = (
        {'question': RunnablePassthrough()}
        | router_prompt
        | structured_llm
)
question = &quot;How does the A work?&quot;
result = router.invoke(question)
result
result['parsed'].datasource.lower()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; The final step is to define the a function that will choose which database should be used to answer our question, either A or B. In case you are running alongsite this project, the code chunks, and you have replaced  A and B for example, (A could be vectore similiarity and B could be KAN Model) then you could repalce A and B in the following code chunk as well in order to obtain relevant answer/result.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
qa_prompt = hub.pull('rlm/rag-prompt')

def choose_route(result):

    llm_route = AzureChatOpenAI(
        deployment_name = os.getenv('LLM_35_Deployment'),
        model_name = os.getenv('LLM_35_Model'),
        azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),
        temperature = 0

    )
    if &quot;B&quot; in result['parsed'].datasource.lower():
        print(f&quot;&amp;gt; Asking about B ...\nQuestion: {result['parsed'].question}\nAnswer:&quot;)
        B_chain = (
            {'context': retriever_B, 'question': RunnablePassthrough()}
            | qa_prompt
            | llm_route
            | StrOutputParser()
        )
        return B_chain.invoke(result['parsed'].question)
    elif &quot;A&quot; in result['parsed'].datasource.lower():
        print(f&quot;&amp;gt; Asking about A ...\nQuestion: {result['parsed'].question}\nAnswer:&quot;)
        A_chain = (
            {'context': retriever_lora, 'question': RunnablePassthrough()}
            | qa_prompt
            | llm_route
            | StrOutputParser()
        )
        return A_chain.invoke(result['parsed'].question)
    else:
        print(f&quot;&amp;gt; Asking about a general question ...\nQuestion: {result.question}\nAnswer:&quot;)
        general_chain = llm_route | StrOutputParser()
        return general_chain.invoke(result.question)

full_chain = router | RunnableLambda(choose_route)
full_chain.invoke(&quot;What is A?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; That concludes our example for logical routing. Next we are going to change a bit our previous coding chunks and produce an example about semantic routing. In this example we are going to create two templates about different topics. We are going to assume that the users ask questions about physics or mathematics. Initially we need to create two templates that are going to give a character to our LLM according to the question provided.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
physical_template = &quot;&quot;&quot;
You are a very smart physics professor. \
You are great at answering questions about physics in a concise and easy to understand manner. \
When you don't know the answer to a question you admit that you don't know.

Here is a question:
{question}
&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
math_template = &quot;&quot;&quot;You are a very good mathematician. You are great at answering math questions. \
You are so good because you are able to break down hard problems into their component parts, \
answer the component parts, and then put them together to answer the broader question.

Here is a question:
{question}&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to create a list containing the character templates. We are going to create embeddings about those routes in order to calculate the similarity with the question provided by the user. Remember that semantic routing checks the similarity of the meaning after all. The choice of the database that the RAG will provide the answer from is not defined by rules as the previous case.&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
routes = [physical_template, math_template]
route_embeddings = embeddings.embed_documents(routes)
len(route_embeddings)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can proceed to the definition of a function that according to the topic of the question that was provided, LLM will answer the question with a routing strategy to either physics or mathematics experties.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
def router(input):
    # Generate embeddings for the user query
    query_embedding = embeddings.embed_query(input['question'])
    # Getting similarity scores between the user query and the routes. This contains the similarity scores between the user query and each of the two routes.
    similarity = cosine_similarity([query_embedding], route_embeddings)[0]
    # Find the route that gives the maximum similarity score
    route_id = similarity.argmax()
    if route_id == 0:
        print(f&quot;&amp;gt; Asking a physics question ...\nQuestion: {input['question']}\nAnswer:&quot;)
    else:
        print(f&quot;&amp;gt; Asking a math question ...\nQuestion: {input['question']}\nAnswer:&quot;)

    return PromptTemplate.from_template(routes[route_id])

semantic_router_chain = (
    {'question': RunnablePassthrough()}
    | RunnableLambda(router)
    | llm
    | StrOutputParser
)

semantic_router_chain.invoke(&quot;What is the formula for the area of a circle?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; I hope you enjoyed this session as much as I did and a learned a thing or two. I wish that you kept a code chunk and will use it later on, on your own projects. Stay tuned for the next topic that we are going to take a look at this LangChain series&lt;/p&gt;

&lt;p&gt;&lt;i&gt; Be safe, code safer!&lt;/i&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="project" />
      

      
        <summary type="html">I hope you enjoy every step so far. Until this point of our Langchain/RAG journey, we have managed to build a simple local application and a querry transformation assistant. But what happens when we have multiple data sources? How to define where our application will retrieve the required information from? The definition of this process, finding the correct road, or better let's say finding the correct route, is called Routing.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">RAG - Query Transformation</title>
      <link href="http://localhost:4000/rag2" rel="alternate" type="text/html" title="RAG - Query Transformation" />
      <published>2024-07-29T00:00:00+03:00</published>
      <updated>2024-07-29T00:00:00+03:00</updated>
      <id>http://localhost:4000/rag2</id>
      <content type="html" xml:base="http://localhost:4000/rag2">&lt;p&gt; Welcome back, I hope you enjoyed the &lt;a href=&quot;https://kavourei.github.io/rag1&quot;&gt;first part&lt;/a&gt; of this series where we are going to explore a portion portion of RAG tool. It is higly suggested that you take a look at all the projects of this series step by step and more importantly to code along this project. If you don't code it out you won't get it.&lt;/p&gt;

&lt;p&gt; In this notebook we are going to take a look how to create an assistant that will help us modify our question. This technique is called Query transformation. Imagine Query Transformation as your search request going through a makeover montage in a movie. Your original query walks in a bit plain and straightforward, and then gets spruced up with the latest styles and smarts to become the most efficient, dashing version of itself. By rephrasing, optimizing, and enhancing your search terms, Query Transformation ensures that what youâ€™re asking for is crystal clear and ready to fetch the best possible results. Itâ€™s like sending your query to a high-end stylist who makes sure itâ€™s dressed to impress and ready to get exactly what you need! &lt;/p&gt;

&lt;p&gt; It is taken for granted that you already have created a &lt;code&gt;.env&lt;/code&gt; file as requested in the first part of this series.&lt;/p&gt;

&lt;h2&gt; Modules Required &lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import os
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import AzureChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import AzureOpenAIEmbeddings
from langchain.load import loads, dumps
from typing import List

load_dotenv()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; The &lt;i&gt;&quot;new&quot;&lt;/i&gt; modules we are going to use in this section are &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;langchain.prompts.ChatPromptTemplate&lt;/strong&gt;: &lt;code&gt;ChatPromptTemplate&lt;/code&gt; is used to create templates for chat prompts. Using these templates can be used to standardize and structure the prompts that are fed into the language model, ensuring consistency and improving the quality of the generated responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;langchain.load.loads&lt;/strong&gt;: &lt;code&gt;loads&lt;/code&gt; function is used to load data or configurations from a serialized format (such as JSON or YAML).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;langchain.load.dumps&lt;/strong&gt;: &lt;code&gt;dumps&lt;/code&gt; function in the langchain library is used for serializing data or configurations into a specific format (like JSON or YAML).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;typing.List&lt;/strong&gt;: &lt;code&gt;List&lt;/code&gt; class from this module is used to indicate that a variable is expected to be a list of a certain type.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; The main ingedient in our recipy in this series is always an LLM agent, which will assist us. As a result, the first step that will take us closer to this result, is to call our LLM model&lt;/p&gt;

&lt;h2&gt; LLM Agent and its Prompt &lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;llm_075 = AzureChatOpenAI(deployment_name=os.getenv('LLM_35_Deployment'),
                         model_name=os.getenv('LLM_35_Model'),
                         azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
                         temperature=0.75,
                         )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; In this call we have set the &lt;code&gt;temperature&lt;/code&gt; argument to be 0.75 (mind that it is an argument that takes value from the close interval [0,1]). But what this argument represent, right? Imagine the temperature argument in an LLM call as the spice level in a recipe. When you set the temperature low, itâ€™s like adding just a pinch of spice, making the responses mild, predictable, and focused. Crank up the temperature, and itâ€™s like dumping in hot sauce, making the responses more adventurous, creative, and sometimes a bit unpredictable. So, adjusting the temperature lets you control how bold or conservative the language modelâ€™s answers will be, ensuring your conversational dish is seasoned just to your taste! Since we need it to generate new questions similar, yet better formated to ours we need to use this spiciness.&lt;/p&gt;

&lt;p&gt; Next we are going to define a prompt. During the first part of this series we used a prompt from the self, where we requested it using the hub module. Now we are going to create our own. The reason for that is to give to our agent &quot;personality&quot; or better purpose for its existance.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prompt = ChatPromptTemplate.from_template(
    &quot;&quot;&quot;
    You are a smart assistant. Your task is to create 5 questions, each phrased differently and from various perspectives, based on the given question, to retrieve relevant documents from a vector database. By offering multiple perspectives on the user's question, your aim is to help the user mitigate some of the constraints of distance-based similarity search. List these alternative questions, each on a new line. Original question: {question}
    &quot;&quot;&quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt; Chain Construction &lt;h2&gt;

&lt;p&gt; Finally, we are ready to create our very first chain, in this section. Initially, we need to pass our question in the chain. Then we insert it to the prompt in th &lt;code&gt; {question} &lt;/code&gt; place. After having our prompt ready-to-go, we &quot;send&quot; it to our LLM agent. Lastly, using &lt;code&gt;StrOutputParser()&lt;/code&gt; and &lt;code&gt;(lambda x: &quot;\n&quot;.join(x.split(&quot;\n&quot;)))&lt;/code&gt; we exporet the results in a readable and nice format for us humans! ðŸ¤– In general before setting up a chain it is suggested to think your steps one by one as simple as possible, define your functions/tools (if needed) and then set it up. Do not start from defining everything, as lated on you will miss something or will need to modify them as you need those steps to be &quot;connected&quot; somehow.&lt;/p&gt;

&lt;p&gt; By connected, I mean the &lt;code&gt; {question} &lt;/code&gt; part in the prompt building, or in other cases more information like &lt;code&gt; {context} &lt;/code&gt; etc. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
generate_queries = (
        {&quot;question&quot;: RunnablePassthrough()}
        | prompt
        | llm_075
        | StrOutputParser()
        | (lambda x: &quot;\n&quot;.join(x.split(&quot;\n&quot;)))
)

result = generate_queries.invoke(&quot;What do you know about Query Transforamtion?&quot;)
print(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt; Fusion Scores and uniqueness &lt;/h2&gt;

&lt;p&gt;Since we have generated our &lt;i&gt;better-formatted&lt;/i&gt; question, we need to use them to get better-more relevant answers to our questions. There are many options, that you could experiment with. Here we are going to explore &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking&quot;&gt;RAG-Fusion&lt;/a&gt;. Through this process, relevant information for each question is retrieved. A union of the retrievals is created that keeps only the unique of them. Finally, a rank is measures and depending on our preference an answer is presented.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def rrf(results: List[List], k=60):
    fused_scores = {}
    for docs in results:        
        for rank, doc in enumerate(docs):
            doc_str = dumps(doc)
            if doc_str not in fused_scores:
                fused_scores[doc_str] = 0
            previous_score = fused_scores[doc_str]
            fused_scores[doc_str] += 1 / (rank + k)
        reranked_results = [
            (loads(doc), score)
            for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
        ]        
        return reranked_results
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Initially a dictionary is created to save the fused scores for each unique document retrieved (&lt;code&gt;fused_scores&lt;/code&gt;). After that we iterate through each ranked document, and each document depending on its rank. We then create a key for each document. Either we add the document in the list (if it does not exists) or retrieve its score. Based on the question, we update the score using the provided RRF formula &lt;code&gt;1/(rank + k)&lt;/code&gt;. Finally, we return the list of containing each document and its fused score in the format of tuples.&lt;/p&gt;

&lt;p&gt; After having this tool defined and set up in our toolbox, we can either us the previously defined prompt, or create a new one to continue with. Just for practise we are going to set up a new one where we are going to generate 3 questions, and not 5 as we did in the previously created prompth.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prompt = ChatPromptTemplate.from_template(
    &quot;&quot;&quot;
    You are a smart assistant. Your task is to create 3 questions, each phrased differently and from various perspectives, based on the provided question, to retrieve relevant documents from a vector database. By offering multiple perspectives on the user's question, your aim is to help the user address some of the limitations of distance-based similarity search. List these alternative questions, each on a new line. Original question: {question}
    &quot;&quot;&quot;
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; So we have our LLM agent ready-to-go, its purpose of existance set, a new tool to generate the scores and retrieve the most relevant information. So you may wonder what is left. We now are ready to define a new chain to include everything we did.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;generate_queries = (
        {&quot;question&quot;: RunnablePassthrough()}
        | prompt
        | llm_075
        | StrOutputParser()
        | (lambda x: x.split(&quot;\n&quot;))
)

fusion_retrieval_chain = (
        {'question': RunnablePassthrough()}
        | generate_queries
        | retriever.map()
        | rrf
)

fusion_retrieval_chain.invoke(&quot;What are the benefits of Bert?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt; Food for thought &lt;/h2&gt;

&lt;p&gt; There are multiple other ways that you could modify the purpose of the LLM agent, your assistance, so that it could help you the way you want. For example, imagine that you have a complex question, where you are not sure how to provide/invoke it to your shiny and brand new chain. Or,  when providing it to your chain the results, you are getting back are not satisfying. Giving your agent a new slightly modified purpose, everything will be again bright and shiny. For example, you could use within your chain the following prompt:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;decompostion_prompt = ChatPromptTemplate.from_template(
    &quot;&quot;&quot;
    You are a helpful assistant capable of simplifying complex questions into smaller parts.
    Your goal is to break down the given question into several sub-questions that can each be answered separately, ultimately addressing the main question.
    List these sub-questions, each separated by a newline character.
    Original question: {question}
    Output (3 queries):
    &quot;&quot;&quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Feeding this prompt to a new chain could save you from some time and simplify your question to reach to your goal, step by step.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query_generation_chain = (
        {&quot;question&quot;: RunnablePassthrough()}
        | decompostion_prompt
        | llm_075
        | StrOutputParser()
        | (lambda x: x.split(&quot;\n&quot;))
)

questions = query_generation_chain.invoke(&quot;What are the benefits of LDA?&quot;)
questions&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; ðŸš€ Imagine the possibilities and think about what you could do by defining a proper prompt based on you needs. ðŸš€ I hope you enjoyed this as much as I did and of course, learned something from it! In the next post of this series, we will explore Hypothetical Document Embeddings. We'll create our own documents, allowing their embedding vectors to identify neighborhoods in the corpus embedding space where similar real documents can be retrieved based on vector similarity.&lt;/p&gt; 

&lt;p&gt; &lt;i&gt;Be safe, code safer!&lt;/i&gt;&lt;/p&gt;
&lt;/h2&gt;&lt;/h2&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="project" />
      

      
        <summary type="html">Welcome back, I hope you enjoyed the first part of this series where we are going to explore a portion portion of RAG tool. It is higly suggested that you take a look at all the projects of this series step by step and more importantly to code along this project. If you don't code it out you won't get it.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introduction to RAG models</title>
      <link href="http://localhost:4000/rag1" rel="alternate" type="text/html" title="Introduction to RAG models" />
      <published>2024-07-22T00:00:00+03:00</published>
      <updated>2024-07-22T00:00:00+03:00</updated>
      <id>http://localhost:4000/rag1</id>
      <content type="html" xml:base="http://localhost:4000/rag1">&lt;p&gt; Firstly, in case you don't know what is RAG here is an unofficial explanation. Imagine youâ€™re on a treasure hunt, but instead of a dusty old map, youâ€™ve got a genius guide who knows every hidden corner. Thatâ€™s RAG, short for &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;. Itâ€™s like having a super-smart friend who fetches the most relevant bits of knowledge from a massive library (the retrieval part) and then crafts a perfectly tailored response just for you (the generation part). So, if your brain is a bit like a rusty old filing cabinet, think of RAG as your personal, turbo-charged librarian whoâ€™s always got the answer before you can say &quot;Google it!&quot;&lt;/p&gt;

&lt;p&gt; In this series of posts, we are going to explore LangChain tools and create RAG models for several applications. We are going to go through the little details that may or may not work in our cases and how to fix those teeny tiny configurations that may be neccesary for our purposes. Without further ado..&lt;/p&gt;

&lt;h2&gt; Dotenv File &lt;/h2&gt;

&lt;p&gt;Before moving forward to modules and script, there is a high need for a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file. Initially, you need to create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file in the folder you are going to create this project, so that you save your passwords. This is not neccesary from the functionality point of view but it is from the security point of view. Think of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file as your projectâ€™s secret diary, where it whispers all its deepest, darkest secrets like passwords, API keys, and configuration settings. You need it because you donâ€™t want these secrets plastered all over the code like graffiti. By keeping them in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file, you ensure they stay hidden and safe, only revealing themselves to those in the knowâ€”your code. So, a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file is like having a secret stash of information that keeps your project running smoothly without spilling the beans to the world. No need for functy file name or anything just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file.&lt;/p&gt;

&lt;h2&gt; Modules Required &lt;/h2&gt;

&lt;p&gt; The python modules we are going to use for this introductory project are the following&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain import hub
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import AzureOpenAIEmbeddings
from langchain.chat_models import AzureChatOpenAI
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;os&lt;/strong&gt;: Used for accessing environment variables with os.getenv().&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;dotenv.load_dotenv&lt;/strong&gt;: Intended to load environment variables from the .env file.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_community.document_loaders.PyPDFLoader&lt;/strong&gt;: Used to load PDF documents.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_community.document_loaders.DirectoryLoader&lt;/strong&gt;: Used to load documents from a directory.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain.text_splitter.RecursiveCharacterTextSplitter&lt;/strong&gt;: Used to split documents into chunks.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_community.vectorstores.Chroma&lt;/strong&gt;: Used to create a vectorstore from document embeddings.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain.chat_models.AzureChatOpenAI&lt;/strong&gt;: Used to initialize the Azure OpenAI model for the QA chain.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain.hub&lt;/strong&gt;: Used to pull a prompt from the hub.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_core.output_parsers.StrOutputParser&lt;/strong&gt;: Used to parse the output of the QA chain.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_core.runnables.RunnablePassthrough&lt;/strong&gt;: Used in the QA chain to pass through the question.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; In the file where we will run this python script, we need to create a folder where we are going to save our data, our files. Else we can load them from a different path. For the purposes of this series, I decided to use the first option. We use PyPDFLoader to laod them as in the following script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;loader = DirectoryLoader('data/', glob = '*.pdf', loader_cls=PyPDFLoader)
documents = loader.load()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Here we see that the first argument &lt;code&gt;data/&lt;/code&gt; is the relative path where the documents are placed. WIth the second argument &lt;code&gt;*.pdf&lt;/code&gt; we define that we want to take under consideration all the files that end with the afforementioned character sequence '.pdf' (the word any is represented by '*'). Lastly, we define the loader class to use for the purposes of the files loading process &lt;code&gt;PyPDFLoader&lt;/code&gt;. Mind that there are other classes that can be used for different types of documents like &lt;code&gt;UnstructuredFileLoader&lt;/code&gt;, &lt;code&gt;TextLoader&lt;/code&gt;, &lt;code&gt;BSHTMLLoader&lt;/code&gt;, &lt;code&gt;CSVLoader&lt;/code&gt;. To find more about this check &lt;a href=&quot;https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html&quot;&gt;here&lt;/a&gt;. Let your imagination go wild! There are also other types of loaders (which we are going to explore later on) where you can load information from webpages, youtube and many many more. After that is completed we use this class and &lt;code&gt;load()&lt;/code&gt; everything exists in the file path and save them to the onject documents.&lt;/p&gt;

&lt;h2&gt; Chunks and Overlap &lt;/h2&gt;

&lt;p&gt; Next in line of the things we need to accomplish is split the documents into chunks. Chunk?!?!? Imagine youâ€™re trying to eat a massive pizza all by yourself. Chunk_size is like deciding how many slices you cut it into so you can manage each piece without choking. Chunk overlap, on the other hand, is making sure each slice has a bit of the previous oneâ€™s crust, so you donâ€™t miss any of the delicious toppings in between. To optimize them, you balance the slice size (chunk_size) to be just right for easy munching, and the overlap so you get all the flavors without making it too repetitive. Get it right, and youâ€™ll devour that pizza with maximum efficiency and satisfaction!&lt;/p&gt;

&lt;p&gt; Here are some tips to help you determine the optimal chunk size if common chunking methods, such as fixed chunking, are not suitable for your use case:

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Data Preprocessing&lt;/strong&gt;: Before deciding on the best chunk size, you need to preprocess your data to ensure its quality. For instance, if your data is sourced from the web, you might need to remove HTML tags or other noise elements.&lt;/li&gt;

&lt;li&gt; &lt;strong&gt;Chunk Sizes&lt;/strong&gt;: After preprocessing, choose a range of potential chunk sizes to test. The selection should consider the nature of the content (e.g., short messages vs. lengthy documents), the embedding model youâ€™ll use, and its token limits. Aim to find a balance between preserving context and maintaining accuracy. Start by exploring various chunk sizes, such as smaller chunks (e.g., 128 or 256 tokens) for capturing granular semantic information and larger chunks (e.g., 512 or 1024 tokens) for retaining more context.&lt;/li&gt;

&lt;li&gt; &lt;strong&gt;Evaluation of Chunk Sizes by performance results.&lt;/strong&gt;: To test different chunk sizes, use either multiple indices or a single index with multiple namespaces. Create embeddings for the selected chunk sizes using a representative dataset and save them in your index or indices. Then, run a series of queries to evaluate the quality and compare the performance of the various chunk sizes. This process is likely iterative, requiring you to test different chunk sizes against different queries until you identify the best-performing chunk size for your content and expected queries.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt; For more information regarding chunk size and chunk overlap, you may refer on &lt;a href=&quot;https://www.kaggle.com/discussions/general/503436&quot;&gt;Guide to Chunk Size and Overlap&lt;/a&gt; by Kaggle, &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies/&quot;&gt;Chunk Sizes&lt;/a&gt; by Llama Index  or &lt;a href=&quot;https://zilliz.com/learn/guide-to-chunking-sreategies-for-rag&quot;&gt;A Guide to Chunking Strategies for Retrieval Augmented Generation (RAG)&lt;/a&gt; by Zilliz. We are not goint to go through the process of deciding the optimal chunk size, as this is out of this project's scope.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32)
text_chunks = text_splitter.split_documents(documents)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now that we have collected our data, and divided into some well structured chunks, easily digestible pieces, we need to create an embeddings model. This model will assists us on creatign embeddings out of the collected and stored documents that we are interested on finding further information about. As I have already mentioned above, I have decided to use Azure OpenAI to assists us. It is up to you which model you are going to use. there are many alternatives bnoth free and paid ones for each part of our project to consider. Do not hesitate to ask me if you have any qyestions on how to change the code so that you use a different kind of model!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;embeddings = AzureOpenAIEmbeddings(
    deployment=os.getenv('OPENAI_DEPLOYMENT_NAME_EMB'),
    model=os.getenv('OPENAI_MODEL_NAME_EMB'),
    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
    openai_api_type=os.getenv('OPENAI_API_TYPE'),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Since we have our embeddings, we are now ready to create our vectorstore where we are going to save our embeddings. Imagine youâ€™ve got a magical, super-organized pantry where every ingredient knows exactly where it belongs and can jump right into your hand when you need it. Thatâ€™s a vectorstore! Itâ€™s a special kind of database where information is stored as vectors, or points in a high-dimensional space, making it super easy to find and retrieve. So, a vectorstore is like having a pantry where every spice, snack, and secret ingredient is neatly indexed and ready to leap out at your command, making your cookingâ€”or in this case, data retrievalâ€”fast and efficient! We have decided to use Chroma as our vectorstore service, but youare free to use any one you need. Some alternatives that you could consider are Pinecone, FAISS, Lance where you can find further information &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.sklearn.SKLearnVectorStore.html&quot;&gt;SKLearnVectorStore&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vectorstore = Chroma.from_documents(documents = text_chunks,                                    
    embedding = embeddings,
    persist_directory=&quot;data/vectorstore&quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt; Retriever &lt;/h2&gt;

&lt;p&gt; Now that we have our data vectorestore set up, we are ready to initialize our retriever. Picture the retriever in a RAG process as your ultra-savvy shopping buddy who knows exactly where everything is in the store. When you need something specific, the retriever zips around the aisles, grabbing the most relevant items off the shelves and bringing them back to you in record time. In the RAG (Retrieval-Augmented Generation) process, the retrieverâ€™s job is to fetch the most pertinent pieces of information from a vast database, so the generator can then whip up a perfectly informed response. Itâ€™s like having a shopping wizard who makes sure you always have the right ingredients for the perfect recipe!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
prompt = hub.pull('rlm/rag-prompt')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Argument &lt;code&gt;'k':5&lt;/code&gt; makes sure that our retriever will bring back to the generator the 5 most similar items, not 6, not 4. The line &lt;code&gt;hub.pull('rlm/rag-prompt')&lt;/code&gt; is used to pull a specific prompt template named 'rlm/rag-prompt' from a hub. You could define and use your own prompt for this part (which we shall experiment in a later on post). To find out more on predefined qa-prompts, go to &amp;lt;a href'https://docs.smith.langchain.com/old/category/prompt-hub'&amp;gt;Langchain Hub&amp;lt;/a&amp;gt;.&lt;/p&gt;

&lt;h2&gt; Chain Creation &lt;/h2&gt;

&lt;p&gt; The next step in our RAG project is to define the LLM (Large Language Model) that will provide the answers for us. Imagine an LLM as a super-intelligent, chatty robot thatâ€™s read every book, article, and meme on the internet and somehow remembers them all. It stands for Large Language Model, and itâ€™s like having a best friend whoâ€™s always ready to chat, offer advice, or spin a tale, because itâ€™s been trained on a vast mountain of text data. This robot buddy can understand your questions and whip up responses that sound like they came straight from a well-read, eloquent author. So, if you ever need a conversation partner whoâ€™s a walking encyclopedia with a knack for witty comebacks, the LLMâ€™s got your back!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;llm = AzureChatOpenAI(
    deployment_name=os.getenv('LLM_35_Deployment'),
    model_name=os.getenv('LLM_35_Model'),
    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
    temperature=0,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Moving forward, we need to define the RAG chain. Think of a RAG chain as a magical relay race where information is passed along to make the ultimate answer. Imagine a team of information specialists: the first runner grabs the relevant facts (thatâ€™s the retrieverâ€™s job), the second runner crafts those facts into a coherent, brilliant response (thanks to the generator), and the baton gets passed seamlessly from one to the other. This chain of handoffs ensures you get a well-rounded, perfectly polished answer every time. So, a RAG chain is like a finely-tuned relay team making sure no detail gets left behind and every answer is a winner! We need the RAG chain to clearly define the steps that we need to be included from prompt to final response. Later on we are going to take a closer look on LangGraph a new tool of Langchain which assist us on defining clearly a this process. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rag_chain = (
    {&quot;context&quot;: lambda x: retriever, &quot;question&quot;: RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; You can see clearly what are the steps of our first really simple RAG chain. Initially, the contect and question are passed through associated to the lambda &quot;x&quot; object (in our case the retriever). On the next step the information (context and question) are passed to the prompt so that the instructions are provided to our llm selected model. The model analyzes and constructes an answer to our question. At the end, with the assistance of &lt;code&gt;StrOutputParser()&lt;/code&gt; it is ensures that the answer is presented in the desired human readable format.&lt;/p&gt;

&lt;p&gt;Now you are ready to ask your own questions to the model you just created to find out more about your documents.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;question = &quot;My custom question&quot;
rag_chain.invoke(question)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Well, I hope you enjoyed this as much as I did and learned something from it! Hope to see you again in the next post of this series where we are going to talk about Query transformation and how we can use LLM models so that an LLM can define our question on a different &quot;better&quot; way.&lt;/p&gt;

&lt;p&gt; &lt;i&gt;Be safe, code safer!&lt;/i&gt;&lt;/p&gt;

&lt;p&gt; This whole series is inspired by &amp;lt;a = href='https://www.sakunaharinda.xyz/ragatouille-book/intro.html#'&amp;gt;Ragatoulle&amp;lt;/a&amp;gt;. Throughout this series, as this is the main goal of this blog, I aim to provide as simple as possible explanations about the modules and functions used with addition of my personal touch wherever I find it necessary.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="project" />
      

      
        <summary type="html">Firstly, in case you don't know what is RAG here is an unofficial explanation. Imagine youâ€™re on a treasure hunt, but instead of a dusty old map, youâ€™ve got a genius guide who knows every hidden corner. Thatâ€™s RAG, short for Retrieval-Augmented Generation. Itâ€™s like having a super-smart friend who fetches the most relevant bits of knowledge from a massive library (the retrieval part) and then crafts a perfectly tailored response just for you (the generation part). So, if your brain is a bit like a rusty old filing cabinet, think of RAG as your personal, turbo-charged librarian whoâ€™s always got the answer before you can say &quot;Google it!&quot;</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Jingle or No Jingle. A Hilariously Serious Dive into PyTorch Image Classification for Santa Claus Detection. ðŸŽ„</title>
      <link href="http://localhost:4000/jingle_or_no_jingle" rel="alternate" type="text/html" title="Jingle or No Jingle. A Hilariously Serious Dive into PyTorch Image Classification for Santa Claus Detection. ðŸŽ„" />
      <published>2023-12-26T00:00:00+02:00</published>
      <updated>2023-12-26T00:00:00+02:00</updated>
      <id>http://localhost:4000/jingle_or_no_jingle</id>
      <content type="html" xml:base="http://localhost:4000/jingle_or_no_jingle">&lt;p&gt; Season's Greetings, data scientists and tech enthusiasts! In the spirit of ho-ho-hilarity and cutting-edge Christmas cheer, I present to you a Christmas-themed trip into the world of PyTorch image classification. Armed with the power of pixels and powered by the magic of MacOS, this jolly project endeavors to answer the age-old question: Is Santa Claus photo bombing your holiday snapshots? Join me on this merry adventure and get ready for a sleigh ride through code, Christmas spirit, and a dash of high-tech merriment! &lt;/p&gt;
&lt;p&gt; Before we dive into the jingle of PyTorch and the festive magic of image classification, let's address the elephant in the room â€“ a.k.a. the neural network. If you're already familiar with the ins and outs of neural networks, fantastic! If not, take a brief pause and explore the basics in this &lt;a href=&quot;/&quot;&gt;introductory post&lt;/a&gt;. Fear not! I won't be unwrapping the intricacies of neural network structures here. Instead, we're keeping it as simple as Santa's route to your chimney, because after all, his visit is just around the corner! &lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Introduction and Data Preparation&lt;/h2&gt;

&lt;p&gt; To kick things off, let's import the necessary packages to set the stage for our upcoming tasks. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  from pathlib import Path
  from timeit import default_timer as timer
  import random
  from PIL import Image
  import numpy as np
  import matplotlib.pyplot as plt
  from tqdm.auto import tqdm
  
  import torch
  from torch import nn
  from torch.utils.data import DataLoader
  from torchvision import datasets, transforms


  from santa_functions import (walk_through_dir,
                             accuracy_fn,
                             print_train_time,
                             plot_transformed_images)
  from CNN_Model import SantaCNN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; All the libraries, except for the final two, consist of built-in modules housing valuable functions and classes essential for our Santa-spotting mission. You can locate the last two in &lt;a href=&quot;//&quot;&gt;this repository&lt;/a&gt;, which also hosts the utilized data. Additionally, I've included two Python scripts allowing you to effortlessly rename all files within a folder. I found this handy for organizing the myriad photos collected, ensuring seamless tracking when needed. &lt;/p&gt;

&lt;p&gt; After securing the aforementioned files, it's prudent to verify that everything is in order. Consequently, I've provided some lines of code for a quick cross-check. To initiate this, we establish the path to the directory containing our data. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# Directory containing data
data_path = Path(&quot;/your/path/to/Is that Santa? ðŸŽ…/project&quot;)
image_path = data_path / &quot;train&quot;
  
walk_through_dir(image_path)
  
# Setup train and testing directory

train_dir = &quot;/path/to/data/train&quot;
test_dir = &quot;/path/to/data/test&quot;
  
walk_through_dir(train_dir)
walk_through_dir(test_dir)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ensure that within the designated path (e.g., test_dir), a specific number of images is presentâ€”308 depicting Santa and an equivalent number portraying non-Santa scenarios. Following this, our next task is to select a &quot;random&quot; image (seed predetermined) and scrutinize its dimensional particulars.
&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  random.seed(42)
  image_path_list = list(image_path.glob(&quot;*/*.jpg&quot;))
  random_image_path = random.choice(image_path_list)
  image_class = random_image_path.parent.stem
  img = Image.open(random_image_path)
  
  
  print(f&quot;Random image path: {random_image_path}&quot;)
  print(f&quot;Image class: {image_class}&quot;)
  print(f&quot;Image height: {img.height}&quot;)
  print(f&quot;Image width: {img.width}&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where we receive the details as an outcome:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
Random image path: /path/to/selected/image/265_NotSanta.jpg
Image class: non_santa
Image height: 574
Image width: 1148
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But let's spot the not so Santa to be witness of this madness:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
img_as_array = np.asarray(img)
plt.figure(figsize=(10, 7))
plt.imshow(img_as_array)
plt.title(f&quot;Image class: {image_class} | Image shape: {img_as_array.shape} -&amp;gt; [height, width, color_channels]&quot;)
plt.axis(False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/jingle_no_jingle/sample_pic.png&quot; alt=&quot;Sample_Santa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Upon closer inspection, you'll observe that the included images exhibit varying dimensions. This variability demands attention to enhance our model's efficacy. Consequently, we'll implement specific transformations. Initially, we'll standardize all images to a uniform size of 64 by 64. Following this, we'll introduce random flips with a 50% probability for certain images. This strategic move aims to prevent the model from fixating on the horizontal direction as a defining feature of Santa, ensuring a more robust learning process. Lastly, to seamlessly integrate these images into our model, we'll convert them into tensors.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
data_transform = transforms.Compose([
    transforms.Resize(size=(64, 64)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor()
])

plot_transformed_images(image_path_list,
                        transform=data_transform,
                        n=3)
train_data = datasets.ImageFolder(root=train_dir,
                                  transform=data_transform,
                                  target_transform=None)

test_data = datasets.ImageFolder(root=test_dir,
                                 transform=data_transform)

print(f&quot;Train data:\n{train_data}\nTest data:\n{test_data}&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the upcoming lines, we'll establish the image classes present in the training and testing folders. As you might anticipate, these classes are none other than Santa and Not Santa!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
class_names = train_data.classes
class_names

class_dict = train_data.class_to_idx
class_dict
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Finally, in order to use our data iteratively in batches and not all at once, we are going to use the &lt;code&gt;DataLoader&lt;/code&gt; format, build-in &lt;code&gt;torch.utils.data&lt;/code&gt; as imported from above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
BATCH_SIZE = 32
train_dataloader = DataLoader(train_data,
    batch_size=BATCH_SIZE,
    shuffle=True
)

test_dataloader = DataLoader(test_data,
    batch_size=BATCH_SIZE,
    shuffle=False
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to take a look at the objects we just created and investigate their dimensions, we can run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
train_features_batch, train_labels_batch = next(iter(train_dataloader))
train_features_batch.shape, train_labels_batch.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and we are going to see the exact shape of train and features and labels that we will import to our model:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
(torch.Size([32, 3, 64, 64]), torch.Size([32])) 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p&gt;I understand; you might be contemplating something right now. While I initially mentioned avoiding an in-depth exploration of the model's intricacies, I'd like to clarify that I won't delve into technical specifics. Instead, let's take a moment to introduce the architecture that underlies the results we'll unveil shortly.&lt;/p&gt;

&lt;p&gt;Let's meet the SantaCNN, a neural network ready to crack this festive code! This jolly model, is inspired by the TinyVGG architecture (because even Santa needs a tech upgrade), is on a mission to distinguish between Santa  and mere holiday enthusiasts.&lt;/p&gt;

&lt;p&gt;SantaCNN is a two-block ensemble, each packed with convolutional charm. The first block, a confection of convolutions (Conv2d), ReLUs, and a splash of MaxPooling magic, sets the stage for unraveling the festive mysteries. Block two follows suit, further refining the holiday essence with additional layers of convolutional wizardry.&lt;/p&gt;

&lt;p&gt;Now, here's where the magic happens: the classifier swoops in like Santa on Christmas Eve, flaunting a Flattening feat and a Linear layer to make sense of the pixelated clues. With an impressive 8192 in_features dance, our neural maestro is ready to deliver the verdict: Naughty or Nice (Santa edition).&lt;/p&gt;

&lt;p&gt;For more details take a look at the &lt;code&gt;CNN_model.py&lt;/code&gt;, in the aforementioned repo, or ask me!&lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Let's find Santa&lt;/h2&gt;

&lt;p&gt;First things first â€“ let's don our Santa goggles! In the upcoming Python script, we're not only sketching out our initial model but also summoning the powers of a loss function and an optimizer. These dynamic duos will be our guiding elves, assisting us in fine-tuning those parameters batch by batch!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
model_init = SantaCNN(input_shape=3,
    hidden_units=32,
    output_shape=len(class_names))

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model_init.parameters(),
                            lr=0.001)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we are going to establish our training and testing loop for preselected number of epochs and take a look at the results.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
torch.manual_seed(42)
train_time_start_on_cpu = timer()

epochs = 10

for epoch in tqdm(range(epochs)):
    print(f&quot;Epoch: {epoch}\n-------&quot;)
    train_loss = 0
    for batch, (X, y) in enumerate(train_dataloader):
        model_init.train()
        y_pred = model_init(X)
        loss = loss_fn(y_pred, y)
        train_loss += loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if batch % 400 == 0:
            print(f&quot;Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples&quot;)
    train_loss /= len(train_dataloader)

    test_loss, test_acc = 0, 0
    model_init.eval()
    with torch.inference_mode():
        for X, y in test_dataloader:
            test_pred = model_init(X)
            test_loss += loss_fn(test_pred, y)
            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))
        test_loss /= len(test_dataloader)
        test_acc /= len(test_dataloader)

    print(f&quot;\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\n&quot;)

train_time_end_on_cpu = timer()
total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,
                                            end=train_time_end_on_cpu,
                                            device=str(next(model_init.parameters()).device))
total_train_time_model_0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; And you are wondering right now, what are the results, right? Well the last iteration return the following results: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
Train loss: 0.00240 | Test loss: 0.33573, Test acc: 91.88%

Train time on cpu: 280.503 seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pretty good, huh? Well, off you go â€“ import those selfies and delve deeper into the mystical realm of Santa Claus spirit. Wishing you better luck than I had; may you uncover his elusive presence hidden somewhere in the background! ðŸŽ…ðŸ” &lt;/p&gt;

&lt;p&gt;Well, I hope you enjoyed this, and the rest of this year's posts as much as I did and learned something!&lt;/p&gt;

&lt;p&gt;Be safe, code safer!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="project" />
      
        <category term="Deep Learning" />
      
        <category term="PyTorch Vision" />
      

      
        <summary type="html">Season's Greetings, data scientists and tech enthusiasts! In the spirit of ho-ho-hilarity and cutting-edge Christmas cheer, I present to you a Christmas-themed trip into the world of PyTorch image classification. Armed with the power of pixels and powered by the magic of MacOS, this jolly project endeavors to answer the age-old question: Is Santa Claus photo bombing your holiday snapshots? Join me on this merry adventure and get ready for a sleigh ride through code, Christmas spirit, and a dash of high-tech merriment! Before we dive into the jingle of PyTorch and the festive magic of image classification, let's address the elephant in the room â€“ a.k.a. the neural network. If you're already familiar with the ins and outs of neural networks, fantastic! If not, take a brief pause and explore the basics in this introductory post. Fear not! I won't be unwrapping the intricacies of neural network structures here. Instead, we're keeping it as simple as Santa's route to your chimney, because after all, his visit is just around the corner!</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Uncovering Topics in BBC News with Latent Dirichlet Allocation in R</title>
      <link href="http://localhost:4000/LDA_R" rel="alternate" type="text/html" title="Uncovering Topics in BBC News with Latent Dirichlet Allocation in R" />
      <published>2023-10-17T00:00:00+03:00</published>
      <updated>2023-10-17T00:00:00+03:00</updated>
      <id>http://localhost:4000/LDA_R</id>
      <content type="html" xml:base="http://localhost:4000/LDA_R">&lt;p&gt; Welcome everyone! Keeping in mind the post about Latent Dirichlet Allocation (in case you have not read it yet and you are interested, you can read it &lt;a href=&quot;/the-editor/&quot;&gt;here&lt;/a&gt;), I am going explore the computational part with you on a dataset containing BBC News (you can find the corresponding data &lt;a href=&quot;/the-editor/&quot;&gt;here&lt;/a&gt;). With the assistance of R-programming language, we are going to find an optimal number of topics, in which we can cluster BBC News and tidy the archived collection of BBC News we have in our dataset. &lt;/p&gt;

&lt;p&gt; The agenda is as follows. Initially we are going to load the required libraries in order to make this a successful journey. Later on, we are going to go through some preprocessing steps in order to make the algorithms job a little bit easier. Our third stop is going to be the model itself. We are going to go through some arguments in order to establish a common knowledge on how each of them works. The final step is going to be some exploration and commenting on our model's result. &lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Tools are built to be used! 
The libraries that we are going to use.&lt;/h2&gt;

&lt;p&gt; Just in case you are new with R-programming language, in order to install the libraries that will be presented below, you can run the following command: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  install.packages(&quot;name_of_the_library&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; After having installed the libraries you can load them (in order to use them) as follows:  &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  library(tidyverse)
  library(tm)
  library(quarteda)
  library(ldatuning)
  library(LDAvis)
  library(tidytext)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; As you can see we are going to use:
&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;tidyverse&lt;/strong&gt; library: in order to manipulate data. This library will load all the excited and Wonderfull constructed parts of this universe a.k.a. ggplot, dplyr, tidyr, purrr, stringr, etc. You can accomplish great manipulation goals by using those libraries!&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;tm&lt;/strong&gt; library: we will need this library in order to accomplish several preprocessing steps of text data. With the help of this library one can tokenize data, convert text data to a corpus and many more exciting nlp assignments. (fan fact : tm = text mining)&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;quarteda&lt;/strong&gt; library: Even though this is another library for preprocessing steps, in combination with tm library, when used correctly, your fingers can be converted to multiple nlp ðŸ¦¸ superheroes!&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;ldatuning&lt;/strong&gt; library: this library will help us decide the number of topics we should look as optimal.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;LDAvis&lt;/strong&gt; library: this library (I won't lie to you) the existence of which I found out recently on the process of topic modelling studying, provides great ways to visualise the results with the help of JSON. I advise you to go along this whole process and end up using this library alongside in order to see the possibilities that you gain by including it in your topic modelling analysis!&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;tidytext&lt;/strong&gt; library: the go-to tool that takes messy text data and turn it into a neat and usable format for analysis. &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Preprocessing is always a must!&lt;/h2&gt;

&lt;p&gt;We will now move forward to the step which in my opinion is the most important in a  process of model building. This step if neglected or completed recklessly, can result in a model that would not perform as expected and most certainly, would not give us back the results as expected. Anyways, in order to be able to complete this step, we need to have data to preprocess. We load the data downloaded as follows :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  bbc &amp;lt;- read.csv('/Users/...your_path.../bbc_news.csv', stringsAsFactors = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The data as you may have already have seen, contain the following variables:
&lt;ul&gt;
&lt;li&gt; title : The title of the BBC News &lt;/li&gt;
&lt;li&gt; pubDate : Publication date &lt;/li&gt;
&lt;li&gt; guid : BBC News link &lt;/li&gt;
&lt;li&gt; link : BBC News link &lt;/li&gt;
&lt;li&gt; description : The description of the BBC News &lt;/li&gt;
&lt;/ul&gt;
Initially, I would like to apologise of the explanations on guid and link variables. Unfortunately, there are no explanation in the dataset source and as I result I have written the descriptions based on what I came across in the dataset. Apart from that, we are going to work only with one variable and this is description. We also are going to keep the title as well, just for reference. As a result, we going to drop the rest. This can be easily done as follows :
&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  bbc2 &amp;lt;- bbc %&amp;gt;% 
  select(title, description)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a first toward preprocessing we are going to convert this dataset into a corpus. Corpus is a collection of data (text or audio) which are organised as dataset.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  bbc.corpus &amp;lt;- corpus(bbc2, text_field = 'description')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Next in line are the changes that needed to be done in the given dataset. Namely, we need to tokenize that dataset. Apart from that we need to remove punctuation as the special characters do not contain and relevant information for topic indications. For the same reasons, we need to remove the numbers and of course the stopwords&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn1&quot; id=&quot;fnref1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;  of English language.  &lt;/p&gt;

&lt;p&gt;Following those changes we are going to stem&lt;sup class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn2&quot; id=&quot;fnref2&quot;&gt;[21]&lt;/a&gt;&lt;/sup&gt; the corpus.&lt;/p&gt;

&lt;p&gt;Finally, we are going to convert the whole corpus to lower characters. This is done so that the model will not judge same words like Legislation and legislation as two different words only for the first letter. Those steps are completed with the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  bbc.corpus.tokens &amp;lt;- tokens(bbc.corpus, remove_punct = TRUE, remove_numbers = TRUE)
  bbc.corpus.tokens &amp;lt;- tokens_remove(bbc.corpus.tokens, stopwords(&quot;english&quot;))
  bbc.corpus.tokens &amp;lt;- tokens_wordstem(bbc.corpus.tokens, language = 'english')
  bbc.corpus.tokens &amp;lt;- tokens_tolower(bbc.corpus.tokens)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At this point your object bbc.corpus.tokens should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/LDA_BBC/topic_bbc_1.png&quot; alt=&quot;bbc.corpus.tokens&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we are going to create a sparse &lt;strong&gt;document-feature matrix&lt;/strong&gt; with the &lt;em&gt;dfm&lt;/em&gt; function. This is needed in order for LDA function to work.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  dfm_bbc &amp;lt;- dfm(bbc.corpus.tokens)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is all for the preprocessing step. Before moving to the next step which the model building, I feel the need to tell you the following. NLP preprocessing depends on the context of data that you have and the goal you want to achieve. You really need to think this through. In our case, you will see some words later on like the word &lt;mark&gt;say&lt;/mark&gt; or &lt;mark&gt;seem&lt;/mark&gt;, that you may think it is better to remove after the first results due to the fact that those words appear in many topic groups. This may result in a more thorough view of your groups. If this is your goal, this is fine way to go. Therefore, If your goal is to make this model let's say more general and try to predict other text data with it I advise you to think it twice, which words should be removed, which should stay and what is the reason and result of such action.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;LDA modeling&lt;/p&gt;
&lt;p&gt;LDA thinking&lt;/p&gt;
&lt;p&gt;LDA building&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It is time for us to move forward to our model building. This will be completed with the help of LDA function. However LDA function has some arguments that needed to be filled. We will need x an object of class DocumentTermMatrix (a.k.a. our preprocessed data), &lt;strong&gt;k&lt;/strong&gt; which is our &quot;guess&quot; in the number of topics that exist in the dataset containing multiple text data, &lt;strong&gt;method&lt;/strong&gt; which is our selection on the method that the algorithm will use in order to compute the results and finally &lt;strong&gt;control&lt;/strong&gt; where we will include a list of parameter settings to help the model work. Even though we have the help, in R-Studio IDE, I would like to elaborate the arguments a little bit more. &lt;/p&gt;
&lt;p&gt;
&lt;ul&gt;
&lt;li&gt; k is our &quot;guess&quot;. The reason I used quotes earlier is that, this may not be really a guess in certain cases. In one hand, we may have an idea of the bunch of documents that we would like to cluster. In the other hand we may use a tuning tool to help us choose. Either way, this is not what the term guess means. Well, of course you could really, try your luck and guess k, but after the first initiation I am sure you will see a pattern and correct your choice! ðŸ˜‰ &lt;/li&gt;
&lt;li&gt; method is actually a choice between two different ways of working. In order to fill this argument we have to choose between &quot;VEM&quot; or &quot;Gibbs&quot;. What are those? you may say, and which one to choose? ðŸ˜µâ€ðŸ’« Well frankly I have the answer! VEM or else Variational Expectation Maximization is an optimisation-based approach where Gibbs on the other hand is a sampling Markov Chain Monte Carlo (MCMC) approach. Both have their strength and weaknesses. For example, VEM is a deterministic method (given we provide the same data, the same output we get) whereas Gibbs is a stochastic method which means that is samples from probability distributions and can provide different results from the same data, but will eventually converge to the true distribution, given some iterations occured. I will let you search around and figure out some other facts about those two as analysing how those work, as well as all their pros and cons can be really long job and is out of this scope. But a little hint is that they converge in different speeds, as you can guess, and of course they can handle large dataset in different ways.  &lt;/li&gt;
&lt;li&gt; control is a list of subargumets, like alpha, verbose, seed and many more where you can really tune the way LDA will work. &lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;Of course apart from those, there are other arguments that you may want/need to tune but the least required so that we proceed are the aforementioned ones. So before running our model, I am going to search the number of topics with the help of ldatuning::FindTopicNumber() function as I indicated earlier. Initially we need to convert our dfm to an object of class DocumentTermMatrix as described earlier.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  new.dtm &amp;lt;- convert(dfm_bbc, to = 'topicmodels')

  result &amp;lt;- FindTopicsNumber(
    new.dtm,
    topics = seq(from = 2, to = 15, by = 1),
    metrics = c(&quot;Griffiths2004&quot;, &quot;CaoJuan2009&quot;, &quot;Arun2010&quot;, &quot;Deveaud2014&quot;),
    method = &quot;Gibbs&quot;,
    control = list(seed = 1234),
    #mc.cores = 2L,
    #verbose = TRUE
  )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Due to the seed, this should result to the following metrics matrix:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/LDA_BBC/topic_bbc_2.png&quot; alt=&quot;results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The package &lt;em&gt;ldatuning&lt;/em&gt; provide to us a way to visually check the results with the usage of the function &lt;em&gt;FindTopicsNumber_plot&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  FindTopicsNumber_plot(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/LDA_BBC/topic_bbc_3.png&quot; alt=&quot;find_topics_number&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our goal here, as you can see is to choose K-candidate (number of topics) that either maximise the bottom or minimise the top. I am not going to go through the metrics and how they are calculated but I will leave a reference at the end if you are interested. From the results produces (mainly the plot ðŸ«¥) the best candidate is K=3. This is obtained by the CaoJuan2009 metric. I have excluded the usage of the other three due to the fact that they do not converge. Here I am aiming for results on the level of field like politics, sports etc. If you would like more detailed clustering, for example foreign policy, domestic policy, soccer, basketball, etc then I advise you to search for 20 or even higher number of topics. &lt;/p&gt;

&lt;p&gt;Next in line is the setting of the model's hyperparameters, a.k.a. alpha and beta. Please recall the intuition behind those two hyperparameters in order to fully understand the meaning of the results. In order to find an optimal setting, one could work with 3 or more ways.&lt;/p&gt;
&lt;p&gt;
&lt;ol&gt;
  &lt;li&gt; Use the perplexity of topic modelling LDA model. &lt;/li&gt;
  &lt;li&gt; Use the coherence measure of topic modelling LDA model. &lt;/li&gt;
  &lt;li&gt; Use intuition about the documents. &lt;/li&gt;
&lt;/ol&gt;
&lt;/p&gt;
&lt;p&gt;The last of the options is when the researcher has a brief idea about the documents, the level of vocabulary details, and the variance of words per document and topics. Even in this case, the researcher would have to try a few of the settings in order to find the optimum. Here I am going to work with the first two options. Before getting there though, I would like to give a rough definition for both notions of perplexity and topic coherence.&lt;/p&gt;

&lt;p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Perplexity&lt;/strong&gt; : In LDA topic modeling context, perplexity measures of how well an LDA model works in other words how generalised is the computed process. This is measured by the notion of how well a model can predict a new set of documents' topics based on what it learnerd from a training set. Lower perplexity means the model is better at making predictions.&lt;/li&gt;
  &lt;li&gt; &lt;strong&gt;Coherence&lt;/strong&gt; : Topic coherence is a measure that helps assess how easy it is to understand the topics created by the model computed. This measures if the words in a topic are semantically related to each other. If the coherence score is high, it means the topics are more understandable and therefore our LDA model is a fine tuned model.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;Perplexity and Coherence have their pros and cons. Here we are going to use perplexity to continue. Coherence is not hard to be computed though (hint use topic_coherence of package topicdoc). Here for the perplexity, I have decided to create two vectors with some of  possible values of alpha and beta and then compute all the perplexity values for a set of test data. The computations and results are the following&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  alpha_values &amp;lt;- seq(from =0.1,to = 1, by = 0.1)
  alpha_values
  beta_values &amp;lt;- seq(from =0.1,to = 1, by = 0.1)
  beta_values
  
  n &amp;lt;- length(alpha_values)
  result_matrix &amp;lt;- matrix(0, n * n, 3)
  
  row_num &amp;lt;- 1
  for (i in 1:n) {
    for (j in 1:n) {
      result_matrix[row_num, 1] &amp;lt;- alpha_values[i]
      result_matrix[row_num, 2] &amp;lt;- beta_values[j]
      
      m &amp;lt;- LDA(train_dtm, k=3, method = 'Gibbs', control = list(alpha = alpha_values[i], seed = 1234), beta = beta_values[j])
      perp.value &amp;lt;- perplexity(m, test_dtm)
      
      result_matrix[row_num, 3] &amp;lt;- perp.value
      row_num &amp;lt;- row_num + 1
    }
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to get the rows with the minimum perplexity value we have to run the following:

&lt;pre&gt;&lt;code&gt;
  result_matrix[result_matrix[,3] == min(result_matrix[,3]),]
&lt;/code&gt;&lt;/pre&gt;

and the result till this point should be :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/LDA_BBC/topic_bbc_4.png&quot; alt=&quot;ResList&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see here the setting for hyperparameter alpha is 0.2 whereas for beta according to perplexity measure it makes no difference. Recall the meaning of them and try to understand what 0.2 means.&lt;/p&gt;
&lt;p&gt;As a result, we are ready to build our model according to those settings and proceed to the final act of this article where we will review the results of it.&lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Model Building&lt;/h2&gt;

&lt;p&gt;After having gathered all the information needed and went through all required preprocessing step it is time to build the model. This is done with the help of &lt;em&gt;LDA&lt;/em&gt; function of package &lt;em&gt;topicmodels&lt;/em&gt;. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  model.lda &amp;lt;- LDA(new.dtm, k = 3, method = &quot;Gibbs&quot;, control = list(alpha = 0.2, seed = 1234))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From this point and on, you can check many very interesting statistics like the per-topic-per-word probability &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  news.topics &amp;lt;- tidy(m_final, matrix = 'beta')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;where you can see what is the probability of each word belonging to a topic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/LDA_BBC/topc_bbc_5.png&quot; alt=&quot;topic_term_beta&quot; /&gt;&lt;/p&gt;

&lt;p&gt;or the top terms per topic in order to draw conclusions on the actual humanly communicated context of topic&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  top.terms &amp;lt;- news.topics %&amp;gt;% 
    group_by(topic) %&amp;gt;% 
    top_n(5) %&amp;gt;% 
    ungroup() %&amp;gt;% 
    arrange(topic, -beta)

  top.terms %&amp;gt;% 
    mutate(term = reorder(term, beta)) %&amp;gt;% 
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = TRUE) +
    facet_wrap(~ topic, scales = 'free') +
    coord_flip()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/LDA_BBC/topic_bbc_6.png&quot; alt=&quot;term_beta&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see here the word say that I mentioned in the beginning... ðŸ˜‰ Here we can get an idea about each topic that the model clustered our BBC news data. I would say, a topic about sports, a topic about news related to Ukraine, and a topic of domestic (UK) news. Finally I would like you to copy paste this final part of chunk, and check out the result!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
  new.dtm.2 &amp;lt;- new.dtm[slam::row_sums(new.dtm) &amp;gt; 0, ]
  phi &amp;lt;- as.matrix(posterior(m_final)$terms)
  theta &amp;lt;- as.matrix(posterior(m_final)$topics)
  vocab &amp;lt;- colnames(phi)
  doc.length &amp;lt;- slam::row_sums(new.dtm)
  term.freq &amp;lt;- slam::col_sums(new.dtm)[match(vocab, colnames(new.dtm))]
  
  json &amp;lt;- createJSON(phi = phi, theta = theta, vocab = vocab, doc.length = doc.length, term.frequency = term.freq)
  serVis(json)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/LDA_BBC/topic_bbc_7.png&quot; alt=&quot;LDAvis_res&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This chunk will create an interactive app as the following where you can explore the capabilities available there. You can check out all the words that are included in every topic.&lt;/p&gt;

&lt;p&gt;Overall, I believe that you have a good idea and all the tools to start exploring LDA yourself. I hope you've enjoyed the implementation presented here. Feel free to copy paste any part of the code, insert your data and explore nlp world from a different perspective.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Be safe, code safer!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="project" />
      
        <category term="Machine Learning" />
      
        <category term="NLP" />
      
        <category term="Bayesian Statistics" />
      

      
        <summary type="html">Welcome everyone! Keeping in mind the post about Latent Dirichlet Allocation (in case you have not read it yet and you are interested, you can read it here), I am going explore the computational part with you on a dataset containing BBC News (you can find the corresponding data here). With the assistance of R-programming language, we are going to find an optimal number of topics, in which we can cluster BBC News and tidy the archived collection of BBC News we have in our dataset.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Topic Modelling - Latent Dirichlet Allocation</title>
      <link href="http://localhost:4000/lda_fundamentals" rel="alternate" type="text/html" title="Topic Modelling - Latent Dirichlet Allocation" />
      <published>2023-10-05T00:00:00+03:00</published>
      <updated>2023-10-05T00:00:00+03:00</updated>
      <id>http://localhost:4000/lda_fundamentals</id>
      <content type="html" xml:base="http://localhost:4000/lda_fundamentals">&lt;p&gt; Hello everyone! In this post I am going to go through an NLP subject. As you may have already read in this post's title, Topic Modelling is what I aim to explain to you as simple as possible. The reason for creating this post is due to the fact that I have searched around and it took me quite a while to find a non academic explanation of this subject, and I had to combine multiple sources to clearly understand what is going on with that topic. As a result, here I aim to gather all the information I found useful and try to explain everything as non-academic as possible. &lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt; Initially, I will provide you with a simple example so that you understand what Latent Dirichlet Allocation (LDA) is about, and how it can be used. Later I am going to state the assumptions and principles upon which LDA is built and of course go through each one so we make sure we get what is the purpose of each and every one of them. Finally I am going to present LDA process and try to explain it as simple as possible so that you have a solid idea of what happens in every step and why it is important. Before kicking off, let's  search on Google the following and see the result: &lt;em&gt;What business problems can  LDA solve&lt;/em&gt;. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Text analysis: LDA is commonly used for text analysis applications, such as topic modelling of news articles, social media posts, and customer feedback. LDA can help identify the main topics and trends in an extensive text data collection, &lt;strong&gt;enabling businesses to make data-driven decisions&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt; In the century of information text analysis is a very import task for all data scientists/analysts as to provide valuable information to businesses, but I guess you know all that, after all, I guess, this is the reason you are reading this. Anyways let's begin! &lt;/p&gt;
&lt;p&gt; Imagine that you have in front of you a document, lets say an article. The topic that the articles tried to present is the latest political news. After you have read this article then next that comes in front of you is about sports and then a third one about science. Well after the third one,  the coffee cup is empty and it is time for you to go to work. Anyways, what has just happened is that you came across three different set of vocabularies, set in correct order and following the appropriate grammatical rules in order to give to you information related to Politics, Sports and Science. &lt;/p&gt;
&lt;p&gt; After work let's say that you want to write an article in a blog of yours about the most recent news (a.k.a. Political, Sports, and Scientific ones). As a result, you start writing and select some words from Politics-oriented vocabulary, some other from Sports-oriented vocabulary and finally some other from Science-oriented vocabulary. In this way you have managed to create your latest blog post, containing the latest news. &lt;/p&gt;
&lt;p&gt; Latent Dirichlet allocation works in the exact opposite way. What I mean by that is given some documents, LDA will take a look at the vocabulary of each document and will try to give us a mixture distribution or better let's say the percentage of topics that is contained in each of the document. Well, that is all, thank you for reading. &lt;/p&gt;
&lt;p&gt; Just kidding! &lt;/p&gt;
&lt;p&gt; I believe that by now you have a rough idea what LDA does. But, how does it manages this task? &lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Assumptions and Principals&lt;/h2&gt;

&lt;p&gt; Well, LDA is based on certain assumptions and principals (which are going to be presented and explained, shortly) and based on those principals it is built to work in a way that it checks both the vocabulary per topic and the topics per document at the same time in order to assure its well preserved results. There are 4 assumptions that play a critical role for LDA: &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt; Each document is a collection of words, referred to as &lt;em&gt;&quot;Bag of words&quot;&lt;/em&gt; &lt;/li&gt;
  &lt;li&gt; Stop words like &lt;em&gt;am, is, of, a, the, but,...&lt;/em&gt; do not contain any information and their appearance in any of the topics is very probable (if not certain). As a result they can be removed from the document in a preprocessing step. &lt;/li&gt;
  &lt;li&gt; The model requires from us to provide the number of topics &lt;em&gt;K&lt;/em&gt;, beforehand. &lt;/li&gt;
  &lt;li&gt; At the beginning all topic assignments in the vocabulary are assumed to be the correct one, except the word it is analysed at that particular time. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Well I believe that the latter 3 points are clear and do not need further explanations. On the other hand, I would like to elaborate a little bit more the first point stated there. What is meant by bag of words is that the order as well as grammatical role of the words do not contain any topic related information and therefor are not considered as a valuable information to be included in the model. As a result the vocabulary from all the documents is collected and analysed as one, without worrying of the aforementioned language settings.&lt;/p&gt;
&lt;p&gt;Apart from the assumptions as I have stated earlier there are also two principles. The first principal is that &lt;strong&gt;every document is a mixture of topics&lt;/strong&gt;. This means that the model itself is not capable to answer to the question &lt;em&gt;&quot;Which is the topic of the document&quot;&lt;/em&gt; but will give us and answer for a document like the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt; Document X does contain 10% of topic 1, 30% of topic 2  and 60% of topic 3.&lt;/li&gt;
  &lt;li&gt; The second principal is that every topic is a mixture of words. This implies that every word has a strong correlation with some topics and less (or even non) with some other. You can think of the following example:&lt;/li&gt;
  &lt;li&gt; Words like &lt;em&gt;Legislation&lt;/em&gt;, &lt;em&gt;Election&lt;/em&gt; and &lt;em&gt;Government&lt;/em&gt; are highly correlated with Politics and lowly with Science, whereas words like &lt;em&gt;Research&lt;/em&gt;, &lt;em&gt;Mitosis&lt;/em&gt; and &lt;em&gt;Laboratory&lt;/em&gt; are highly correlated with Science are lowly with Politics or Sports.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Having understood the fundamentals I believe that is time to try and explore how LDA woks and go through the process itself. The structure of LDA is the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/lda_fund/lda_basic_1&quot; alt=&quot;lda_model_architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where:
&lt;ul&gt;
  &lt;li&gt; &amp;alpha; :  is the Dirichlet prior parameter for the per-document-topic proportion &lt;/li&gt;
  &lt;li&gt; &amp;theta; : is the topic distribution of a given document &lt;/li&gt;
  &lt;li&gt; z: is the topic assignment in word in the given document &lt;/li&gt;
  &lt;li&gt; w : is the selected word to be examined &lt;/li&gt;
  &lt;li&gt; N : is the collection of words from the examined document &lt;/li&gt;
  &lt;li&gt; M : is the collection of documents that we aim to cluster &lt;/li&gt;
  &lt;li&gt; &amp;phi; : is the distribution of words of a selected topic&lt;/li&gt;
  &lt;li&gt; K : is the collection of available topics &lt;/li&gt;
  &lt;li&gt; &amp;beta; : is the Dirichlet prior parameter that represents per-topic-word proportion &lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt; We really need to understand how both Dirichlet prior parameters work as good as possible in order to get on well with the process of LDA topic modelling. The Dirichlet prior parameter Î± represents per-document-topic density. This tells us that by setting Î± high, documents are assumed to be constructed of more topics and result in more specific topic distribution per document. As for parameter Î², setting it too high, it is indicated that topics are assumed to be made up of words with great variability and result in a more specific word distribution per topic. &lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;The Latent Dirichlet Allocation model&lt;/h2&gt;

&lt;p&gt; We now have a complete idea of what LDA is constructed from. As a result, I am positive that it is time to move forward to the wary LDA works. Latent Dirichlet Allocation is basically an iterative process of topic assignment for each word in each document that we include in our analysis. A key element here is the word &quot;latent&quot;, which implies as stated earlier, that all document are constructed with the same K-topics that we decide beforehand, but with a different proportion. With that said, the steps of LDA are the following: &lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt; Latent Dirichlet Allocation assigned randomly a topic to each of the word in the vocabulary &lt;/li&gt;
  &lt;li&gt; Picks a word and delete its initial assignment. Then, given all other topic assignments to the rest of the words in the vocabulary, re-assign a topic to this selected word. The process this time is not random as initially occurred. In contrast it is the product of two conditional probabilities: &lt;/li&gt;
  &lt;li&gt; After topic assignment is completed for the selected word, LDA will repeat the process as described above for all other words in the vocabulary. &lt;/li&gt;
  &lt;li&gt; Once it is completed LDA will repeat the process until there is no need for further changes. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt; The conditional probabilities that mentioned in the second step is the following: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt; A = Probability( topic &lt;code&gt;t&lt;/code&gt; | document &lt;code&gt;d&lt;/code&gt;) which represents the following. Given we evaluate document &lt;code&gt;d&lt;/code&gt;, what is the probability that this specific document is of topic &lt;code&gt;t&lt;/code&gt;. This is calculated by the number of words that are assigned to topic &lt;code&gt;t&lt;/code&gt;. Apart from that a Dirichlet-generated multinomial distribution over topics in each document is included in the calculations. The intuition behind this proportion is that if a lot of words belong to topic &lt;code&gt;t&lt;/code&gt; then it is more likely that also the current word we search the topic for also belongs to topic &lt;code&gt;t&lt;/code&gt;. &lt;/li&gt;
  &lt;li&gt; B = Probability( word &lt;code&gt;w&lt;/code&gt; | topic &lt;code&gt;t&lt;/code&gt; ) which represents that given we evaluate topic &lt;code&gt;t&lt;/code&gt; over all documents, what is the probability that word &lt;code&gt;w&lt;/code&gt; is of that specific topic. This is calculated by considering how many of the documents are assigned to that topic due to the assistance of word &lt;code&gt;w&lt;/code&gt; and a Dirichlet-generated multinomial distribution over words for each topic. This proportion tries to capture the number of documents that are in topic &lt;code&gt;t&lt;/code&gt; because of word &lt;code&gt;w&lt;/code&gt;. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result, the assignment occurs after those two proportions are calculated and the topic that word w belongs to is calculated as &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt; Probability( word &lt;code&gt;w&lt;/code&gt; is assigned to topic &lt;code&gt;t&lt;/code&gt; ) = A x B &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; The second step as you can see is the most important. As a result, we are going to stay a little bit more to that step in order to fully understand the role that both Dirichlet parameters play in LDA process. As we have seen, there are two Dirichlet-generated multinomial distributions included in order to proceed to the topic assignment in each word selected. This is due to the fact that initially, this method assigns randomly topics to each of the words, where a document may end up with zero probability of a certain topic where in reality this topic should have been included in the documentâ€™s mixture distribution of topics. In that case, Dirichlet probability distribution is included over K topics as it is a non-zero probability for the topic generated. This way, a topic with zero initial probability may be included in any future iterations if this should be done.  &lt;/p&gt;

&lt;p&gt;I believe that you now have a solid idea about topic modelling, how LDA process works and what is the meaning of every part of it. In the future I am going to come back with an example on LDA computed probably with the help of R programming Language, so stay tuned!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Be safe, code safer!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="project" />
      
        <category term="Machine Learning" />
      
        <category term="NLP" />
      
        <category term="Bayesian Statistics" />
      

      
        <summary type="html">Hello everyone! In this post I am going to go through an NLP subject. As you may have already read in this post's title, Topic Modelling is what I aim to explain to you as simple as possible. The reason for creating this post is due to the fact that I have searched around and it took me quite a while to find a non academic explanation of this subject, and I had to combine multiple sources to clearly understand what is going on with that topic. As a result, here I aim to gather all the information I found useful and try to explain everything as non-academic as possible.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Neural Network ~ Predicting a Numerical Value</title>
      <link href="http://localhost:4000/neural_network" rel="alternate" type="text/html" title="Neural Network ~ Predicting a Numerical Value" />
      <published>2023-03-04T00:00:00+02:00</published>
      <updated>2023-03-04T00:00:00+02:00</updated>
      <id>http://localhost:4000/neural_network</id>
      <content type="html" xml:base="http://localhost:4000/neural_network">&lt;p&gt;Welcome back! As part of this introductory series on Neural Networks, we will be exploring the process of building NNs and making decisions about their topology. This includes determining the number of layers, the number of neurons per layer, selecting appropriate activation functions, and implementing backpropagation. Throughout this process, we will delve into various factors to consider when building an effective neural network. Though everything is statistics (ðŸ˜‰) I will try to keep statistical formulas to the minimum possible level.&lt;/p&gt;

&lt;p&gt;Without further ado let's jump right into fun (hopefully!).  In this post we will be building two differenct neural network models to predict numerical values :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A model to predict the strength of concrete strength.&lt;/li&gt;
&lt;li&gt;A model to predict the prices of houses in Paris (Why Paris? Don't know just cause! We don't need a reason for everything, accept it ðŸ˜›)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Initially, I planned to explain the concepts and decision-making process between the code chunks. However, I understand that some readers may prefer to just jump straight into the code and results. Therefore, I will provide a summary of the topics I plan to cover at the top of the post and keep the code and results at the bottom for those who prefer that approach. No judgement here - feel free to choose what works best for you! Or is there some judgement? Just kiding, choose your way (or am I not? ðŸ˜›)&lt;/p&gt;

&lt;p&gt;When we encounter a neural network, the first thing that typically stands out to us is the layer structure. Before we dive into understanding the movement, input features, activation functions, or weights, we need to have a solid grasp of the layers in a neural network. So let's start from there.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;How to decide the number of the layers?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To be honest, there is no solid way or any rule of thumb to calculate the number of layers needed for any type of problem you are trying to solve. As a result, deciding on the number of layers can be a challenging task. However, there are some general guidelines that can help:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt; &lt;strong&gt;Start small&lt;/strong&gt;: Beginning with a simple network structure with just a few layers can help to avoid overfitting and to gradually increase the complexity as needed. Remember the goal is to find the simplest model possible not the most complex one. A small network with few layers is called shallow neural network. &lt;/li&gt;
  &lt;li&gt; &lt;strong&gt;Research pre-existing architectures&lt;/strong&gt;: If possible, consider using pre-existing network architectures that have proven effective in solving similar problems. I know you want to build your own model and you want to be independent, but your mother's cooked food is always better than yours, right? So try searching around for your mom, I mean a built model that is proven to work efficiently and the try adjusting this to your own needs. &lt;/li&gt;
  &lt;li&gt; &lt;strong&gt;Get to know your data&lt;/strong&gt;: Having a large amount of training data can support deeper and more complex neural networks. &lt;/li&gt;
  &lt;li&gt; &lt;strong&gt;Practise, practise, practise..&lt;/strong&gt;: The Mose effective way to find out the optimal number of layers your your specific needs and flavours, is through experimentation and testing difference architectures, so after completing the steps described above, play around, change the configurations and check the outcome. The time is your enemy and depending on the pc specs you run your tests on, hopefully, this won't take long per model build. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Bottom line, there is no one rule that fits the all to determin the oprimal number of layers in a neural network. Only one mom to cook the perfect dish for every occation. The key is to balance the complexity of the network with the available resources and the data in your environment. Remember though, we aim for simplicity as choosing more that 2 layers, can get computationally expensive really fast.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Fine with the layers, but how many neurons should we use per layer added in a model?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you search around the &lt;a href=&quot;https://www.google.com&quot;&gt;web&lt;/a&gt; for information on determining the optimal number of neurons per layer in a neural network, you will come across several &quot;rules of thumb&quot; that suggest a range of options. Some of those are the following: &lt;/p&gt;

&lt;p&gt;If you search for information on determining the optimal number of neurons per layer in a neural network, you may come across several &quot;rules of thumb&quot; that suggest a range of options. While these rules can provide a starting point, it's important to remember that the optimal solution may vary depending on the specific problem you are trying to solve. Therefore, it's important to also consider other factors such as the complexity of the problem, available resources, and experimentation with different architectures.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; The number of hidden neurons should be between the size of the input layer and the size of the output layer. &lt;/li&gt;
&lt;li&gt; The number of the neurons should be 2/3 the size of the input layer, plus the size of the output layer. &lt;/li&gt;
&lt;li&gt; The number of the hidden neurons should be less than twice the size of the input layer. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While these ruls can provide a starting point, it's important to remember that the optimal solution may vary depending on the problem that you are trying to answer. Once again, I feel the need to remind you that we aim for the simpliest model possible, complexity costs and slow us down. Remember after all the most beutiful mathematical formulas are the simpliest ones. &lt;/p&gt;

&lt;p&gt;While keeping that in mind, note that using less neurons than it is required may lead us to underfitting the data imported, while using more neurons than needed may end up to overfitting, high variance and increse the time it takes to train the network. &lt;/p&gt;

&lt;p&gt;A way that may give us a good intuition on the number of neurons needed is using cross-validation method with different configurations (starting with the simplest possible) and compare the Mean Square Errors (MSE) in order to stick with the option that minimises MSE. Fill free to try this in the models we are going to build and find out a better model architecture for the models provided below.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Cool cool cool.. No doubt.. How do we pass through the neurons and layers?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;iframe src=&quot;https://giphy.com/embed/XAdbHJywVjF5K&quot; width=&quot;480&quot; height=&quot;274&quot; frameborder=&quot;0&quot; class=&quot;giphy-embed&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://giphy.com/gifs/someone-gwen-stefani-XAdbHJywVjF5K&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For those two or three..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://giphy.com/embed/XAdbHJywVjF5K&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I am glad you asked.. (Did you? Hopefully you did, or I am hearing voices ðŸ‘»). If you recall from the &lt;a href=&quot;https://kavourei.github.io/neural_network_intro&quot;&gt;introductory post&lt;/a&gt; of this series, we mentioned the activation functions which we shall discuss later in this post, but I have not mentioned at all the work backpropagation though you may have come across in some posts. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://people.idsia.ch/~juergen/who-invented-backpropagation-2014.html&quot;&gt;Backpropagation&lt;/a&gt; is a fundamental algorithm in the training process. The main purpose that we use back propagation is to adjust the weights (the numbers above the arrows that you will come across later in a NN plot) of the neural network. The weights distinguish level of importance that every feature (through the neurons in the input layer) has. Recall the example of the house pricing. It is more important that it has proper roof than higher number of entrances.&lt;/p&gt;

&lt;p&gt;Backpropagation works iteratively. This means that the algorithm is used many times to adjust the weights that the difference between the predicted outcome and the actual outcome through the training process is minimized. This is also known as the error. &lt;/p&gt;

&lt;p&gt;The steps at every iteration is the following. After a full process is completed and the model has the first pair of results, the backpropagation (BP) algorithm is set to use. It starts from the end (output) to the beginning (input) of the model, that is why it is called -&lt;strong&gt;back&lt;/strong&gt;-propagation. It computes the derivative of the error with respect to each weight in the network. This tells us (don't worry it is done without your contribution) how much each choice of weight has contributed to the final error acquired at the end, hence how much this can be adjusted. Then changes are made and then the model runs from the beginning to the end once again, producing a new error, and going back again calculating new weights. This back and forth is done many many times (hopefully, not that many) until the error is minimised.&lt;/p&gt;

&lt;p&gt;With this repeation, the network grafually learns to map input data to output data more accurately. This process is often reffered as learning rate. Overall BP is a powerfull tool and helps us humans solve very complex problems. The formulation of this process to programm by hand can be really complex, but there is no need for that as R (and Python for anyone interested) gives us this as an option to choose amongst others.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;What is learning rate mentioned earlier?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Well during the process of BP, we said that the weights are adjusted. Learning rate is the steps that we allow the algorithm to take. In other words, is the amount that we allow the algorithm to adjust the weights per iteration. &lt;/p&gt;

&lt;p&gt;Now you may think that we can set this high so that the model will converge fast..&lt;/p&gt;

&lt;p&gt;Am a sorry to be the party pooper (ðŸ’©) here but setting learning rate high can lead our model to diverge. This can happen as the optimal choice of weight may be &quot;jumped over&quot; a big step of weight configuration. Image you search for a middle in the floor, you can not search while making huge steps. On the other hand, it is sure that making small steps will lead us to finding the middle though it make take time, so really small steps is not a good option as well. &lt;/p&gt;

&lt;p&gt;Guess what?!? There is no method that tells us the learning rate no matter the problem. Some of the best practises are the following : &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Loss/Error VS Learning Rate&lt;/strong&gt; : With this approach if you are patient, you have to plot the loss function vs the learning rate on a logarithmic scale, which will help us deside on a good range of learning rates to use. On the range that has been prompted, we have to use the largest possible, as we want the model to converge without many iterations.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Grid Search&lt;/strong&gt; : Again, if you are patient enough, by trying out different learning rates over the training values and evaluating the performance of the model on the validation set, can help you identify a good range of learning rates to use. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I believe that was all I wanted to discuss about Neural Network configurations and really did not used any mathematical/statistical formulas at all!!! I hope you got the ideas presented above (as simple as possible). &lt;/p&gt;

&lt;p&gt;Now it is time for the second part, the implementation. Bellow you will see two examples. The first implementation is from a really good book &quot;&lt;a href=&quot;https://www.amazon.com/Machine-Learning-techniques-predictive-modeling/dp/1788295862&quot;&gt;Machine Learning with R&lt;/a&gt;&quot; providing you with as much information needed to get introduced to the topic and the other one from &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt;. All the data can be found &lt;a href=&quot;https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/concrete.csv&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;As usuall, we are going to start with importing the libraries. We are going to use &lt;em&gt;caret&lt;/em&gt; library in order to split the data randomly. The library &lt;em&gt;neuralnet&lt;/em&gt; to build neural network models. The last library, &lt;em&gt;sigmoid&lt;/em&gt;, is worth noting as it contains all the activation functions mentioned in the previous post. I you feel like it take a moment and search for the document of this last one, in order to check what are the functions included in there, so you dop't have to define them from the ground.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
library(caret)
library(neuralnet)
library(sigmoid)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Moving forward we are going to load the data for the first example, the concrete one.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
data1_init &amp;lt;- read.csv('/Users/.../concrete.csv')

str(data1_init)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we can see we have the 8 input features and the last one that is the output to be predicted feature, the strength of the concrete. All the models perform much better with normalised data. There are many ways to do that, it's one with some unique pros and cons (It is out of this post's scope to dive into that). Some to mention are the normalise with log transformation, with Min-Max values (which we are going to use) or with stars scaling (also called standardisation). In order to perform the normalisation selected, there are two ways to go.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Define the process as a custom function &lt;/li&gt;
&lt;li&gt;With the usage of the functions &lt;em&gt;preProcess&lt;/em&gt; and predict. In the first one you choose the type through the argument &lt;em&gt;mehtod&lt;/em&gt; and then by passing through the second function you apply this method to the data you want.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let's define our normalisation function, which will be clearer how the process works and then apply the normalisation function to the data imported:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
custom_norm &amp;lt;- function(x){
  return((x-min(x))/(max(x)-min(x)))
}
data1_norm &amp;lt;- as.data.frame(lapply(data1_init, custom_norm))

str(data1_norm)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we need to split the data to train and test datasets. Though a better practise is to split data in three parts, train, validation and test datasets, we will use the simpler way as the point of the posts is to get solid foundations and build upon that on later steps and further experimentation. In order to split the data we run the following :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
set.seed(1)
trainIndex &amp;lt;- createDataPartition(data1_norm$strength,
                                  p = 0.7,
                                  list = FALSE,
                                  times = 1) 
data1_train &amp;lt;- data1_norm[trainIndex,]
data1_test &amp;lt;- data1_norm[-trainIndex,]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I use here the &lt;em&gt;set.seed(1)&lt;/em&gt; so that if you copy and paste the code chunks you end up with the same results as me. The creatDataPartition help us create index to take the train and test sample according to the vector we will use as outcome. The second argument is used to define the amount of data we will split the data upon, here we use 70%.&lt;/p&gt;

&lt;p&gt;Guess what time it is.. To build our first model. We are going to build the simplest model possible to start with and get an idea. We will use the function &lt;em&gt;neuralnet&lt;/em&gt;. As the first argument, we define the data that we take the features from and then address the formula. Before the tilde ( '~' ) we define the output variable to be predicted and in the right of it we define the input features. By adding the dot, we indicate to the model to use all the other features available.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
conc_model1 &amp;lt;- neuralnet(data = data1_train, strength ~ .)
plot(conc_model1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is our first Neural Network. Here we see the input layer (all the dots to the right), with the weights each feature is measured with the help of BP, the hidden layer (1) in the middle, where the logarithmic activation function is used (by default in neuralnet) and th outcome where layer. Bellow we can see the Error (which is the sum of squared errors (&lt;strong&gt;SSE&lt;/strong&gt;)) and the number of training steps required for the model to converge.&lt;/p&gt;

&lt;p&gt;In order to evaluate our first NN, we continue as listed below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
conc_results1 &amp;lt;- compute(conc_model1, data1_test[1:8])
conc_pred1 &amp;lt;- conc_results1$net.result
cor(conc_pred1,data1_test[9])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Correlation at 0.84, not bad for our first try, right? Let's run some more evaluation metrics though. I will use bellow the following :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; Mean Squared Error (&lt;strong&gt;MSE&lt;/strong&gt;) : It represents the difference between the original and predicted values extracted by squared the average difference over the data set. &lt;/li&gt;
&lt;li&gt; Mean Absolute Error (&lt;strong&gt;MAE&lt;/strong&gt;) : It represents the difference between the original and predicted values extracted by the averaged absolute difference over the data set. &lt;/li&gt;
&lt;li&gt; Root Mean Squared Error (&lt;strong&gt;RMSE&lt;/strong&gt;) is the error rate by the square root of MSE &lt;/li&gt;
&lt;li&gt; Coefficient of determination (&lt;strong&gt;R-squared&lt;/strong&gt;) represents the coefficient of how well the values fit compared to the original values of a vanilla model. The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is. &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;
d1 = data1_test$strength-conc_pred1
mse = mean((d1)^2)
mae = mean(abs(d1))
rmse = sqrt(mse)
R2 = 1-(sum((d1)^2)/sum((data1_test$strength-mean(data1_test$strength))^2))
cat(&quot; MAE:&quot;, mae, &quot;\n&quot;, &quot;MSE:&quot;, mse, &quot;\n&quot;, 
    &quot;RMSE:&quot;, rmse, &quot;\n&quot;, &quot;R-squared:&quot;, R2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see that overall, the results we obtain are good. But there is always room for improvement. &lt;strong&gt;Perfection is the enemy of a good result&lt;/strong&gt; after all!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
conc_model2 &amp;lt;- neuralnet(data = data1_train, strength ~ ., hidden =5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we have modified the single hidden layer, by adding 4 extra neurons (5 in total) through the argument of &lt;em&gt;hidden&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
plot(conc_model2) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By the Error provided through the plot of the new NN model, we have a sign that we have improved the situation, though let us proceed to our model evaluation metrics introduced above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
conc_results2 &amp;lt;- compute(conc_model2, data1_test[1:8])
conc_pred2 &amp;lt;- conc_results2$net.result
cor(conc_pred2,data1_test[9])

d2 = data1_test$strength-conc_pred2
mse = mean((d2)^2)
mae = mean(abs(d2))
rmse = sqrt(mse)
R2 = 1-(sum((d2)^2)/sum((data1_test$strength-mean(data1_test$strength))^2))
cat(&quot; MAE:&quot;, mae, &quot;\n&quot;, &quot;MSE:&quot;, mse, &quot;\n&quot;, 
    &quot;RMSE:&quot;, rmse, &quot;\n&quot;, &quot;R-squared:&quot;, R2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can clearly see that we have obtained from that small change. You can modify the model above by adding one or more layers and change the number of the neurons. Though this is like a sandbox where you can test different things, remember you always need to take care of the number of layers and neurons used as the most common error of the excitement while going closer to perfect prediction is overfitting.. In order to change the number of letters, we should modify the &lt;em&gt;hidden&lt;/em&gt; argument in the &lt;em&gt;neuralnet&lt;/em&gt; function. For example if we want 4 neurons in the first hidden layer and 5 in the second we should define it as &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
example_model &amp;lt;- neuralnet(..., hidden = c(4,5))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Apart from those there are many more arguments to modify your network like &lt;em&gt;startweights&lt;/em&gt; which defines the starting value of each weight, &lt;em&gt;learningrate&lt;/em&gt; which indicates the learning rate of your model, &lt;em&gt;algorithm&lt;/em&gt; which let's you change the algorithm for weights adjustment (a.k.a. change the default backpropagation usage), &lt;em&gt;act.fct&lt;/em&gt; which let's you choose activation function through your model, &lt;em&gt;err.fct&lt;/em&gt; which gives you the power to use specific error calculating function and many many more, feel free to type &lt;strong&gt;neuralnet&lt;/strong&gt; in your console to find out all of them!&lt;/p&gt;

&lt;p&gt;The second example is an implementation of two neural network models. After followin the same normalization preprocessing and splitting steps as above for the data about house prices in Paris we have a 1-3-1 model  where the hidden layers will consist from 4 neurons each and a 1-1-1 model where the hidden layer will consist from 2 neurons, and compare the results. In those models, the ReLU activation function was used. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
house_model0 &amp;lt;- neuralnet(data = house_train, price ~ .,hidden = c(2),act.fct = relu)
house_model1 &amp;lt;- neuralnet(data = house_train, price ~ .,hidden = c(4,4,4), act.fct = relu)

plot(house_model0)
plot(house_model1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let us run all the evaluation metrics in order to compare the two models.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
data.frame(Model_1 = c(cor21,mae21,mse21,rmse21,R2_21),
           Model_2= c(cor22,mae22,mse22,rmse22,R2_22),
           row.names = c('Correlation',&quot;MAE:&quot;,&quot;MSE:&quot;,&quot;RMSE:&quot;,&quot;R-squared:&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the evaluation metrics, we can say that both our models do great at predicting the house prices. The reason that I have used these examples though is not to build the models, is to show to you that a much deeper model will not always result in a better situation. Apart from the result, the complexity and computational time needed to build the second model is much more than the first one. So keep in mind when playing around that we always aim for the simplest model possible that will give us the best results.&lt;/p&gt;

&lt;p&gt;Thank you for reading this post, I hope you have learned something new and more importantly noe you are able to implement yourself neural netowrks to predict numerical values. There will be a new post about image recognition on the topic where we will use some different kind of neural networks. Anyways thank you all!&lt;/p&gt;

&lt;p&gt;At this place I would like to especially thank the readers who send me their comments (both good and bad) and general remarks, with their help I become better and keep it going. So thank you guys!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Be safe, code safer!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="project" />
      
        <category term="Deep Learning" />
      

      
        <summary type="html">Welcome back! As part of this introductory series on Neural Networks, we will be exploring the process of building NNs and making decisions about their topology. This includes determining the number of layers, the number of neurons per layer, selecting appropriate activation functions, and implementing backpropagation. Throughout this process, we will delve into various factors to consider when building an effective neural network. Though everything is statistics (ðŸ˜‰) I will try to keep statistical formulas to the minimum possible level.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">A smooth introduction to Neural Networks</title>
      <link href="http://localhost:4000/neural_network_intro" rel="alternate" type="text/html" title="A smooth introduction to Neural Networks" />
      <published>2023-02-09T00:00:00+02:00</published>
      <updated>2023-02-09T00:00:00+02:00</updated>
      <id>http://localhost:4000/neural_network_intro</id>
      <content type="html" xml:base="http://localhost:4000/neural_network_intro">&lt;h2 id=&quot;specialformatting&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Nowadays, deep learning is entering in our lives deeper and deeper (you see what I did there?!? ðŸ¤¯ðŸ¤¯ðŸ¤¯) Anyways, as I was saying the evolution of deep learning and the usage of Neural Network has taken an exponential rate. As not everyone is a computer science expert/enthusiast it is obvious that a question may rise.&lt;/p&gt;

&lt;blockquote&gt;
What is Neural Network and how such a network works?
&lt;/blockquote&gt;

&lt;p&gt;I am going to create a series of posts, starting with this one, where I will try to answer this very question stated above. Initially, in this post, I will try to present to you a brief introduction on the topic, including the components of a neural network, the steps one need to follow in order to create such a network and of course where this can help us.  I will try not to bring on the surface difficult mathematical formulas or nonsense calculations, so hopefully, you will enjoy reading this and will be waiting for the implementations.&lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Neural Networks - The first touch&lt;/h2&gt;

&lt;p&gt;In our everyday life, we are meant to complete tasks that require, or not, our effort. As you can guess the effort may differ from task to task. Some of those tasks, you would not think as ones that you put effort to complete and if I was to ask you, you would not even considering to enumerate them. Some, that I have in mind are objects detection or patterns recognition. Consider for example when you try to cross the road and check to your right and left, if any car is passing by or not. This is a task as object detection that you have to complete almost every day and you don't even think about it. &lt;/p&gt;

&lt;p&gt;Everyday doings as the one I have just mentioned or any other you could think of, is completed with the help of a system that is placed within our body. Every one of us has a network that helps them take actions and complete tasks. This network is not other than our own biological neural network. This nervous system is built up of many interconnectioned neurons (you may have heard the term &quot;nerve cells&quot;). The nervous system uses the neurons to send messages back and forth from the brain, through the spinal cord, to the nerves throughout the body. As simple as that. Whether you see a car coming then (hopefully) won't pass the road, as you have detected an object (the car). Or imagine you touch a cactus to its spine. At this very moment, the nerves in your finger send a message to your brain that this is not probably good, and this is converted to pain, more or less.&lt;/p&gt;

&lt;p&gt;Artificial neural networks (ANN) are inspired by this and attempt to mimic the way that our brain and biological nervous system works. Artificial Neural Networks were firstly introduced by &lt;a href=&quot;https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch&quot;&gt;Warren McCulloch&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Walter_Pitts&quot;&gt;Walter Pitts&lt;/a&gt;. They established the computational model in 1943 in their paper called &lt;a href=&quot;https://link.springer.com/article/10.1007/BF02478259&quot;&gt;&lt;em&gt;&quot;A Logical Calculus of the Ideas Immanent in Nervous Activity&quot;&lt;/em&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;blockquote&gt;
In this paper a mathematical model is proposed inspired by our own neural network. A binary neuron model is described, in which each neuron receives input from other neurons and generates an output based on a threshold function. At the end the authors suggested that this simple model could perform logical operations.
&lt;/blockquote&gt;

&lt;p&gt;Ok, so I believe that now you have a general idea of what is a neural network, and of course many many questions should start popping in your head, I can imagine. Let's continue and I will try to answer them. &lt;/p&gt;

&lt;blockquote&gt;
So, what does a neural network consists of?
&lt;/blockquote&gt;

&lt;p&gt;A neural network consists of 3 layers: &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;The Input Layer&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt; &lt;strong&gt;The Hidden Layers&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt; &lt;strong&gt;The Output Layer&lt;/strong&gt; &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let's take them one by one. The &lt;strong&gt;input layer&lt;/strong&gt; is there to welcome all the variables that the scientist would like to use in the model. In our example before the input layer of our biological nervous system is the finger, and the variable incoming is the cactus needle when we touched it (ðŸ«³ðŸŒµðŸ™…ðŸ½), I believe you get the idea here. &lt;/p&gt;

&lt;p&gt;Moving forward, the &lt;strong&gt;hidden layers&lt;/strong&gt; refers to the layers between the input and the output layers. The data when passed from the input layer are processed in this part of the model and generate a representation, which is passed to the next layer. The term hidden is given due to the fact that the neuron's activation (that belong in this layers) are not obvious/visible from the input layer or the output of the network and as a result this term is set. This layer allows the model, hence the scientist, to model complex relationships and abstractions of the data, which enables them to create powerful predictions and classification tasks. The numbers of the neurons and the number of the layers can be adjusted according to the needs and the complexity in order to obtain accuracy depending of the obtained data and the goal. The hidden layer for our example would be our brain, where the information imported is processed and valued.&lt;/p&gt;

&lt;p&gt;Last but obviously not less important than the rest, is the &lt;strong&gt;output layer&lt;/strong&gt;. Through this layer the outcome of the model is produced based on the information processed by the previous layers (input and hidden). The numbers of the neurons in the output layer depends on the task that the network is designed to perform. For example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Binary classification is probably one output neuron that produces the probability expected&lt;/li&gt;
&lt;li&gt;In multi-class classification there is one output neuron per class&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the cactus example of course, as you can guess the output is the pain we feel after the process that took place in our own hidden layers (a.k.a. the brain) imported from our own input layer (our stupid action to touch the cactus, just kidding, I mean our finger).&lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Neural Networks - The &quot;Magic&quot; behind fancy words&lt;/h2&gt;

&lt;p&gt;So far so good. I believe that the next obvious question would be:&lt;/p&gt;

&lt;blockquote&gt;
And how does this work?
&lt;/blockquote&gt;

&lt;p&gt;Consider the following situation. Imagine the case where you want to buy a house. What are the factors that matter in a house's price? Some that I could think of are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; The neighbourhood &lt;/li&gt;
&lt;li&gt; The available rooms &lt;/li&gt;
&lt;li&gt; The time it was built &lt;/li&gt;
&lt;li&gt; etc &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So each of those factors represent a variable, let's assume that neighbourhood is x1, available rooms x2, time it was built x3, and the rest that you can think of. So obviously some are more important than others and play a greater role in our model. As a result, every factor is assigned to a weight (let's say the weight of importance).  After a weight is assigned to each factor, we can produce the weighted sum of all those factors. Finally with the help of an activation function, this is converted and output to a number. The output that we search for (hopefully if we have done everything right ðŸ¤žðŸ¼). Bellow, with the help of a graph, those steps are represented in a simple and clear way. I believe till now everything is well understood.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/intro_to_nn/1_simple_nn_architecture.png&quot; alt=&quot;NN Architecture&quot; /&gt;&lt;/p&gt;
&lt;p&gt;ðŸ¤”ðŸ¤”ðŸ¤”&lt;/p&gt;

&lt;p&gt;Now you may want to say the following :&lt;/p&gt;

&lt;blockquote&gt;
Ok..?!?.. But what is the activation function?
&lt;/blockquote&gt;

&lt;p&gt;This is the final touch in our models, to be completed. In order to explain the use of activation functions, I would like you to think of google translate. So what is the way you use google translate?!? You select the language you are going to type in and then the one you want the words to be translated to. So activation function is more or less doing the same thing. After the Weighted sum of the factors is calculated after you end up with a number. Then the activation function is called to translate this number to a solution you are searching for. Imagine this, you are trying to create a neural network to classify a picture that shows a dog. In order to do that you set the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; 0 : The picture does not contain a dog &lt;/li&gt;
&lt;li&gt; 1 : The picture contains a dog.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am not going to get in the details of the network's architecture in this post, but basically, you want to end up to a result based on probabilities. &lt;/p&gt;

&lt;p&gt;&lt;em&gt;Just reminding: probabilities are in the set of [0,1]. Probability of 1 means that the event is certainly going to happen, whereas probability of 0 implies that the event is not going to happen, eveeeeeerrrrrrrrrrrrr..... ðŸ˜›&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So by calculating the sum of weighted factors, you are going to end up with a number that most of the cases do not belong in the [0,1] set. After all, what activation function do is that it translates the result to the desired solution (in our example, a number that belongs to the set [0,1] and tells us with what probability the picture we provided contains a dog). I believe you get why we need it in our game.&lt;/p&gt;

&lt;p&gt;More formally, with the use of an activation function we can introduce non-linearity into the network that we build, which gives us the flexibility to create networks to model more complex relationship between inputs and outputs.&lt;/p&gt;

&lt;p&gt;There are three types of activation functions Linear, Binary and Non-Linear activation functions. Most common activation functions that are used are :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;Sigmoid&lt;/a&gt; : This function is useful for binary classification problems. It can map any inputs to a value between 0 and 1. ðŸ˜®ðŸ˜² Yes, this fits our problem above! &lt;/li&gt;
&lt;li&gt; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;&gt;Rectified Linear Unit&lt;/a&gt; : A more common name is ReLU. This function returns the output only if this is a positive number otherwise it returns 0. Using ReLu can add up computational efficiency to our network &lt;/li&gt;
&lt;li&gt; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Hyperbolic_functions&quot;&gt;Hyperbolic Tangent&lt;/a&gt; : Also called Tanh. This is activation function maps any input to a value between -1 and 1. This activation function is mainly used for cyclical or periodic data. &lt;/li&gt;
&lt;li&gt; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;&gt;Softmax&lt;/a&gt; : This is like Sigmoid, but instead of just one class (dog-no dog) it is used to produce a probability distribution amongst classes and find out the most suitable one. &lt;/li&gt;
&lt;ul&gt;

&lt;p&gt;There are more activation functions if you search around though those are the ones used the most. An appropriate activation function choice is a crucial part for the performance of our network. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/intro_to_nn/2_activation_funcs.jpg&quot; alt=&quot;Activation Functions&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;specialformatting&quot;&gt;Neural Network - Uses and limits&lt;/h2&gt;

&lt;p&gt;Alas! We reached the final part, the exciting one. Artificial Neural Networks is a powerful tool and by getting to know it better and better you can complete resolve complex tasks in wide variety of fields. Some of the most common applications of neural networks are :&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt; Speech Recognition &lt;/strong&gt; &lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Image Classification &lt;/strong&gt; &lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Natural Language Processing &lt;/strong&gt; &lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Recommender Systems &lt;/strong&gt; &lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Anomaly Detection &lt;/strong&gt; &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Those are only some examples of the range of tasks one can complete with the help of Neural Networks. With the right knowledge and the patience to get the details, ANNs can become the ace up your sleeve. &lt;/p&gt;

&lt;p&gt;Of course everything has its own price. This very saying applies to our case as well. Some of the limitations you should consider are &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; Amount of data : ANNs require big amount of data which are well prepared and of course labeled, in order to train the network. &lt;/li&gt;
&lt;li&gt; Expensive : In order to train ANN can become a vary expensive procedure. When I say expensive I mean on one hand to obtain well preserved data as well the computational power needed for preparing data and training the network. &lt;/li&gt;
&lt;li&gt; Time Consuming : Artificial neural networks are highly sensitive to the hyperparameters used, such as the learning rate, the number of hidden layers, and the number of neurons in each layer. Which makes it a slow process to find the ones that fit your purpose. &lt;/li&gt;
&lt;li&gt; Overfitting : Artificial Neural Networks can easily overfit data during the training process. &lt;/li&gt;

&lt;p&gt; But still, don't let yourself down. Neural Networks is still a very useful and powerful tool. Knowing the limitations, can only be a positive thing as we shall keep in mind what to be cautious about and what to avoid. &lt;/p&gt;

&lt;p&gt;That is all I wanted to tell you about Artificial Neural Networks. I hope you got the whole process and learned a thing or two. Stay tuned for some applications on this subject. Thank you for reading this post! &lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Be safe, code safer!&lt;/p&gt;

&lt;/ul&gt;&lt;/ul&gt;&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="project" />
      
        <category term="Deep Learning" />
      

      
        <summary type="html">Introduction</summary>
      

      
      
    </entry>
  
</feed>
