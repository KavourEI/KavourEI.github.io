<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/research/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-08-07T12:57:07+03:00</updated>
  <id>http://localhost:4000/tag/research/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">KAN or MLP - A Fairer Comparison</title>
      <link href="http://localhost:4000/KANorMLP" rel="alternate" type="text/html" title="KAN or MLP - A Fairer Comparison" />
      <published>2024-07-23T00:00:00+03:00</published>
      <updated>2024-07-23T00:00:00+03:00</updated>
      <id>http://localhost:4000/KANorMLP</id>
      <content type="html" xml:base="http://localhost:4000/KANorMLP">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, we control the number of parameters and FLOPs to compare the performance of KAN and MLP. Our main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. We also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, we find that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. We hope these results provide insights for future research on KAN and other MLP alternatives.&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://github.com/yu-rp/KANbeFair&quot;&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.16674&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Shape of Motion - 4D Reconstruction from a Single Video</title>
      <link href="http://localhost:4000/ShapeOfMotion" rel="alternate" type="text/html" title="Shape of Motion - 4D Reconstruction from a Single Video" />
      <published>2024-07-18T00:00:00+03:00</published>
      <updated>2024-07-18T00:00:00+03:00</updated>
      <id>http://localhost:4000/ShapeOfMotion</id>
      <content type="html" xml:base="http://localhost:4000/ShapeOfMotion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches are limited in that they either depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. In this work, we introduce a method capable of reconstructing generic dynamic scenes, featuring explicit, full-sequence-long 3D motion, from casually captured monocular videos. We tackle the under-constrained nature of the problem with two key insights: First, we exploit the low-dimensional structure of 3D motion by representing scene motion with a compact set of SE3 motion bases. Each point's motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we utilize a comprehensive set of data-driven priors, including monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes.&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://shape-of-motion.github.io/&quot;&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.13764&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">SpreadsheetLLM - Encoding Spreadsheets for Large Language Models</title>
      <link href="http://localhost:4000/MicrosoftSpreadsheetLLM" rel="alternate" type="text/html" title="SpreadsheetLLM - Encoding Spreadsheets for Large Language Models" />
      <published>2024-07-12T00:00:00+03:00</published>
      <updated>2024-07-12T00:00:00+03:00</updated>
      <id>http://localhost:4000/MicrosoftSpreadsheetLLM</id>
      <content type="html" xml:base="http://localhost:4000/MicrosoftSpreadsheetLLM">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Spreadsheets, with their extensive two-dimensional grids, various layouts, and diverse formatting options, present notable challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.09025&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">RouteLLM-Learning to Route LLMs with Preference Data</title>
      <link href="http://localhost:4000/RouteLLM" rel="alternate" type="text/html" title="RouteLLM-Learning to Route LLMs with Preference Data" />
      <published>2024-07-01T00:00:00+03:00</published>
      <updated>2024-07-01T00:00:00+03:00</updated>
      <id>http://localhost:4000/RouteLLM</id>
      <content type="html" xml:base="http://localhost:4000/RouteLLM">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.18665&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AI Agents That Matter</title>
      <link href="http://localhost:4000/AIAgentsThatMatter" rel="alternate" type="text/html" title="AI Agents That Matter" />
      <published>2024-07-01T00:00:00+03:00</published>
      <updated>2024-07-01T00:00:00+03:00</updated>
      <id>http://localhost:4000/AIAgentsThatMatter</id>
      <content type="html" xml:base="http://localhost:4000/AIAgentsThatMatter">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;AI agents are an exciting new research direction, and agent development is driven by benchmarks. Our analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. Our focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. We design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. We prescribe a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. We hope that the steps we introduce for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.01502&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Data curation via joint example selection further accelerates multimodal learning</title>
      <link href="http://localhost:4000/DataCuration" rel="alternate" type="text/html" title="Data curation via joint example selection further accelerates multimodal learning" />
      <published>2024-06-25T00:00:00+03:00</published>
      <updated>2024-06-25T00:00:00+03:00</updated>
      <id>http://localhost:4000/DataCuration</id>
      <content type="html" xml:base="http://localhost:4000/DataCuration">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly selecting batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from larger super-batches, we also leverage recent advances in model approximation to reduce the associated computational overhead. As a result, our approach--multimodal contrastive learning with joint example selection (JEST)--surpasses state-of-the-art models with up to 13× fewer iterations and 10× less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing the level of data curation as a new dimension for neural scaling laws.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.17711&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Connecting the Dots - LLMs can Infer and Verbalize Latent Structure from Disparate Training Data</title>
      <link href="http://localhost:4000/ConnectDots" rel="alternate" type="text/html" title="Connecting the Dots - LLMs can Infer and Verbalize Latent Structure from Disparate Training Data" />
      <published>2024-06-20T00:00:00+03:00</published>
      <updated>2024-06-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/ConnectDots</id>
      <content type="html" xml:base="http://localhost:4000/ConnectDots">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. Further experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs (x,f(x)) can articulate a definition of f and compute inverses. While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures. Overall, the ability of LLMs to &quot;connect the dots&quot; without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.14546&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B</title>
      <link href="http://localhost:4000/MonteCarloTrees" rel="alternate" type="text/html" title="Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B" />
      <published>2024-06-13T00:00:00+03:00</published>
      <updated>2024-06-13T00:00:00+03:00</updated>
      <id>http://localhost:4000/MonteCarloTrees</id>
      <content type="html" xml:base="http://localhost:4000/MonteCarloTrees">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07394&quot;&gt;Monte Carlo Trees&lt;/a&gt; with Llama-3 8B solve mathematics limitations of LLMs&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Monte Carlo Trees with Llama-3 8B solve mathematics limitations of LLMs</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Depth Anything V2</title>
      <link href="http://localhost:4000/DepthAnythingV2" rel="alternate" type="text/html" title="Depth Anything V2" />
      <published>2024-06-13T00:00:00+03:00</published>
      <updated>2024-06-13T00:00:00+03:00</updated>
      <id>http://localhost:4000/DepthAnythingV2</id>
      <content type="html" xml:base="http://localhost:4000/DepthAnythingV2">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07522&quot;&gt;Samba&lt;/a&gt; architecture achieves 3.73x faster throughput with enhanced context&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1&amp;rpar; replacing all labeled real images with synthetic images, 2&amp;rpar; scaling up the capacity of our teacher model, and 3&amp;rpar; teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Samba architecture achieves 3.73x faster throughput with enhanced context</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Samba - Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</title>
      <link href="http://localhost:4000/Samba" rel="alternate" type="text/html" title="Samba - Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling" />
      <published>2024-06-11T00:00:00+03:00</published>
      <updated>2024-06-11T00:00:00+03:00</updated>
      <id>http://localhost:4000/Samba</id>
      <content type="html" xml:base="http://localhost:4000/Samba">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07522&quot;&gt;Samba&lt;/a&gt; architecture achieves 3.73x faster throughput with enhanced context&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in &lt;a href=&quot;https://github.com/microsoft/Samba&quot;&gt;this https URL&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Samba architecture achieves 3.73x faster throughput with enhanced context</summary>
      

      
      
    </entry>
  
</feed>
