<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/research/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-08-30T14:18:28+03:00</updated>
  <id>http://localhost:4000/tag/research/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</title>
      <link href="http://localhost:4000/TransformerExplainer" rel="alternate" type="text/html" title="Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" />
      <published>2024-08-06T00:00:00+03:00</published>
      <updated>2024-08-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/TransformerExplainer</id>
      <content type="html" xml:base="http://localhost:4000/TransformerExplainer">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user's browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public's education access to modern generative AI techniques. Our open-sourced tool is available at &lt;a href=&quot;https://poloclub.github.io/transformer-explainer/&quot;&gt;this https URL&lt;/a&gt;. A video demo is available at &lt;a href=&quot;https://youtu.be/ECR4oAwocjs&quot;&gt;this https URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Cho,+A&quot;&gt;Aeree Cho&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kim,+G+C&quot;&gt;Grace C. Kim&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Karpekov,+A&quot;&gt;Alexander Karpekov&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Helbling,+A&quot;&gt;Alec Helbling&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+Z+J&quot;&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lee,+S&quot;&gt;Seongmin Lee&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hoover,+B&quot;&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chau,+D+H&quot;&gt;Duen Horng Chau&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.04619&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</title>
      <link href="http://localhost:4000/HybridRAG" rel="alternate" type="text/html" title="Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" />
      <published>2024-08-06T00:00:00+03:00</published>
      <updated>2024-08-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/HybridRAG</id>
      <content type="html" xml:base="http://localhost:4000/HybridRAG">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&amp;amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&amp;amp;A format, and hence provide a natural set of pairs of ground-truth Q&amp;amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sarmah,+B&quot;&gt;Bhaskarjit Sarmah&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hall,+B&quot;&gt;Benika Hall&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Rao,+R&quot;&gt;Rohan Rao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Patel,+S&quot;&gt;Sunil Patel&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Pasquali,+S&quot;&gt;Stefano Pasquali&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Mehta,+D&quot;&gt;Dhagash Mehta&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.04948&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</title>
      <link href="http://localhost:4000/ScallingLLMTest" rel="alternate" type="text/html" title="Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" />
      <published>2024-08-06T00:00:00+03:00</published>
      <updated>2024-08-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/ScallingLLMTest</id>
      <content type="html" xml:base="http://localhost:4000/ScallingLLMTest">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a &quot;compute-optimal&quot; scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Snell,+C&quot;&gt;Charlie Snell&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lee,+J&quot;&gt;Jaehoon Lee, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+K&quot;&gt;Kelvin Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kumar,+A&quot;&gt;Aviral Kumar&lt;/a&gt;&amp;lt;/p&amp;gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.03314&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Apple Intelligence Foundation Language Models</title>
      <link href="http://localhost:4000/AppleIntelligenceFoundationLanguageModels" rel="alternate" type="text/html" title="Apple Intelligence Foundation Language Models" />
      <published>2024-07-29T00:00:00+03:00</published>
      <updated>2024-07-29T00:00:00+03:00</updated>
      <id>http://localhost:4000/AppleIntelligenceFoundationLanguageModels</id>
      <content type="html" xml:base="http://localhost:4000/AppleIntelligenceFoundationLanguageModels">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.21075&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">KAN or MLP - A Fairer Comparison</title>
      <link href="http://localhost:4000/KANorMLP" rel="alternate" type="text/html" title="KAN or MLP - A Fairer Comparison" />
      <published>2024-07-23T00:00:00+03:00</published>
      <updated>2024-07-23T00:00:00+03:00</updated>
      <id>http://localhost:4000/KANorMLP</id>
      <content type="html" xml:base="http://localhost:4000/KANorMLP">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, we control the number of parameters and FLOPs to compare the performance of KAN and MLP. Our main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. We also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, we find that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. We hope these results provide insights for future research on KAN and other MLP alternatives.&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://github.com/yu-rp/KANbeFair&quot;&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.16674&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Stretching Each Dollar- Diffusion Training from Scratch on a Micro-Budget</title>
      <link href="http://localhost:4000/DiffusionTrainingfromScratch" rel="alternate" type="text/html" title="Stretching Each Dollar- Diffusion Training from Scratch on a Micro-Budget" />
      <published>2024-07-22T00:00:00+03:00</published>
      <updated>2024-07-22T00:00:00+03:00</updated>
      <id>http://localhost:4000/DiffusionTrainingfromScratch</id>
      <content type="html" xml:base="http://localhost:4000/DiffusionTrainingfromScratch">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to address this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose to randomly mask up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only $1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive FID and high-quality generations while incurring 118× lower cost than stable diffusion models and 14× lower cost than the current state-of-the-art approach that costs $28,400. We aim to release our end-to-end training pipeline to further democratize the training of large-scale diffusion models on micro-budgets.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.15811&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Shape of Motion - 4D Reconstruction from a Single Video</title>
      <link href="http://localhost:4000/ShapeOfMotion" rel="alternate" type="text/html" title="Shape of Motion - 4D Reconstruction from a Single Video" />
      <published>2024-07-18T00:00:00+03:00</published>
      <updated>2024-07-18T00:00:00+03:00</updated>
      <id>http://localhost:4000/ShapeOfMotion</id>
      <content type="html" xml:base="http://localhost:4000/ShapeOfMotion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches are limited in that they either depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. In this work, we introduce a method capable of reconstructing generic dynamic scenes, featuring explicit, full-sequence-long 3D motion, from casually captured monocular videos. We tackle the under-constrained nature of the problem with two key insights: First, we exploit the low-dimensional structure of 3D motion by representing scene motion with a compact set of SE3 motion bases. Each point's motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we utilize a comprehensive set of data-driven priors, including monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes.&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://shape-of-motion.github.io/&quot;&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.13764&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">SpreadsheetLLM - Encoding Spreadsheets for Large Language Models</title>
      <link href="http://localhost:4000/MicrosoftSpreadsheetLLM" rel="alternate" type="text/html" title="SpreadsheetLLM - Encoding Spreadsheets for Large Language Models" />
      <published>2024-07-12T00:00:00+03:00</published>
      <updated>2024-07-12T00:00:00+03:00</updated>
      <id>http://localhost:4000/MicrosoftSpreadsheetLLM</id>
      <content type="html" xml:base="http://localhost:4000/MicrosoftSpreadsheetLLM">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Spreadsheets, with their extensive two-dimensional grids, various layouts, and diverse formatting options, present notable challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.09025&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Mixture of A Million Experts</title>
      <link href="http://localhost:4000/MixtureofAMillionExperts" rel="alternate" type="text/html" title="Mixture of A Million Experts" />
      <published>2024-07-04T00:00:00+03:00</published>
      <updated>2024-07-04T00:00:00+03:00</updated>
      <id>http://localhost:4000/MixtureofAMillionExperts</id>
      <content type="html" xml:base="http://localhost:4000/MixtureofAMillionExperts">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.04153&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AI Agents That Matter</title>
      <link href="http://localhost:4000/AIAgentsThatMatter" rel="alternate" type="text/html" title="AI Agents That Matter" />
      <published>2024-07-01T00:00:00+03:00</published>
      <updated>2024-07-01T00:00:00+03:00</updated>
      <id>http://localhost:4000/AIAgentsThatMatter</id>
      <content type="html" xml:base="http://localhost:4000/AIAgentsThatMatter">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;AI agents are an exciting new research direction, and agent development is driven by benchmarks. Our analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. Our focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. We design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. We prescribe a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. We hope that the steps we introduce for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.01502&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
</feed>
