<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/research/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-10-04T15:35:33+03:00</updated>
  <id>http://localhost:4000/tag/research/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Rereading Improves Reasoning in Large Language Models</title>
      <link href="http://localhost:4000/rereading" rel="alternate" type="text/html" title="Rereading Improves Reasoning in Large Language Models" />
      <published>2024-09-21T00:00:00+03:00</published>
      <updated>2024-09-21T00:00:00+03:00</updated>
      <id>http://localhost:4000/rereading</id>
      <content type="html" xml:base="http://localhost:4000/rereading">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, Re2, i.e., &lt;strong&gt;Re&lt;/strong&gt;-&lt;strong&gt;Re&lt;/strong&gt;ading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, Re2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, Re2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, Re2 facilitates a &quot;bidirectional&quot; encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of Re2, illustrating its potential to enable &quot;bidirectional&quot; attention mechanisms. We then evaluate Re2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal Re2's adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies. Our code is available at &lt;a href=&quot;https://github.com/Tebmer/Rereading-LLM-Reasoning/&quot;&gt;this https URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+X&quot;&gt;Xiaohan Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tao,+C&quot;&gt;Chongyang Tao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shen,+T&quot;&gt;Tao Shen&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+C&quot;&gt;Can Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+H&quot;&gt;Hongbo Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Long,+G&quot;&gt;Guodong Long&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lou,+J&quot;&gt;Jian-guang Lou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ma,+S&quot;&gt;Shuai Ma&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2309.06275&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Michelangelo-Long Context Evaluations Beyond Haystacks via Latent Structure Queries</title>
      <link href="http://localhost:4000/Michelangelo" rel="alternate" type="text/html" title="Michelangelo-Long Context Evaluations Beyond Haystacks via Latent Structure Queries" />
      <published>2024-09-20T00:00:00+03:00</published>
      <updated>2024-09-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/Michelangelo</id>
      <content type="html" xml:base="http://localhost:4000/Michelangelo">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce Michelangelo: a minimal, synthetic, and unleaked long-context reasoning evaluation for large language models which is also easy to automatically score. This evaluation is derived via a novel, unifying framework for evaluations over arbitrarily long contexts which measure the model's ability to do more than retrieve a single piece of information from its context. The central idea of the Latent Structure Queries framework (LSQ) is to construct tasks which require a model to ``chisel away'' the irrelevant information in the context, revealing a latent structure in the context. To verify a model's understanding of this latent structure, we query the model for details of the structure. Using LSQ, we produce three diagnostic long-context evaluations across code and natural-language domains intended to provide a stronger signal of long-context language model capabilities. We perform evaluations on several state-of-the-art models and demonstrate both that a) the proposed evaluations are high-signal and b) that there is significant room for improvement in synthesizing long-context information.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Vodrahalli,+K&quot;&gt;Kiran Vodrahalli&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ontanon,+S&quot;&gt;Santiago Ontanon&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tripuraneni,+N&quot;&gt;Nilesh Tripuraneni&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+K&quot;&gt;Kelvin Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jain,+S&quot;&gt;Sanil Jain&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shivanna,+R&quot;&gt;Rakesh Shivanna&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hui,+J&quot;&gt;Jeffrey Hui&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dikkala,+N&quot;&gt;Nishanth Dikkala&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kazemi,+M&quot;&gt;Mehran Kazemi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fatemi,+B&quot;&gt;Bahare Fatemi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Anil,+R&quot;&gt;Rohan Anil&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dyer,+E&quot;&gt;Ethan Dyer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shakeri,+S&quot;&gt;Siamak Shakeri&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Vij,+R&quot;&gt;Roopali Vij&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Mehta,+H&quot;&gt;Harsh Mehta&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ramasesh,+V&quot;&gt;Vinay Ramasesh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Le,+Q&quot;&gt;Quoc Le&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chi,+E&quot;&gt;Ed Chi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lu,+Y&quot;&gt;Yifeng Lu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Firat,+O&quot;&gt;Orhan Firat&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lazaridou,+A&quot;&gt;Angeliki Lazaridou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lespiau,+J&quot;&gt;Jean-Baptiste Lespiau&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Attaluri,+N&quot;&gt;Nithya Attaluri&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Olszewska,+K&quot;&gt;Kate Olszewska&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.12640&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">LLMs Still Can’t Plan; Can LRMs? A Preliminary Evaluation of OpenAI’s o1 on PlanBench</title>
      <link href="http://localhost:4000/EvaluationOpenAIo1" rel="alternate" type="text/html" title="LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench" />
      <published>2024-09-20T00:00:00+03:00</published>
      <updated>2024-09-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/EvaluationOpenAIo1</id>
      <content type="html" xml:base="http://localhost:4000/EvaluationOpenAIo1">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities. PlanBench, an extensible benchmark we developed in 2022, soon after the release of GPT3, has remained an important tool for evaluating the planning abilities of LLMs. Despite the slew of new private and open source LLMs since GPT3, progress on this benchmark has been surprisingly slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs--making it a new kind of model: a Large Reasoning Model (LRM). Using this development as a catalyst, this paper takes a comprehensive look at how well current LLMs and new LRMs do on PlanBench. As we shall see, while o1's performance is a quantum improvement on the benchmark, outpacing the competition, it is still far from saturating it. This improvement also brings to the fore questions about accuracy, efficiency, and guarantees which must be considered before deploying such systems.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Valmeekam,+K&quot;&gt;Karthik Valmeekam&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Stechly,+K&quot;&gt;Kaya Stechly&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kambhampati,+S&quot;&gt;Subbarao Kambhampati&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.13373&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Training Language Models to Self-Correct via Reinforcement Learning</title>
      <link href="http://localhost:4000/SelfCorrection" rel="alternate" type="text/html" title="Training Language Models to Self-Correct via Reinforcement Learning" />
      <published>2024-09-19T00:00:00+03:00</published>
      <updated>2024-09-19T00:00:00+03:00</updated>
      <id>http://localhost:4000/SelfCorrection</id>
      <content type="html" xml:base="http://localhost:4000/SelfCorrection">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either require multiple models or rely on a more capable model or other forms of supervision. To this end, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, we observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kumar,+A&quot;&gt;Aviral Kumar&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhuang,+V&quot;&gt;Vincent Zhuang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Agarwal,+R&quot;&gt;Rishabh Agarwal&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Su,+Y&quot;&gt;Yi Su&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Co-Reyes,+J+D&quot;&gt;John D Co-Reyes&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Singh,+A&quot;&gt;Avi Singh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Baumli,+K&quot;&gt;Kate Baumli&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Iqbal,+S&quot;&gt;Shariq Iqbal&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Bishop,+C&quot;&gt;Colton Bishop&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Roelofs,+R&quot;&gt;Rebecca Roelofs&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+L+M&quot;&gt;Lei M Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=McKinney,+K&quot;&gt;Kay McKinney&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shrivastava,+D&quot;&gt;Disha Shrivastava&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Paduraru,+C&quot;&gt;Cosmin Paduraru&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tucker,+G&quot;&gt;George Tucker&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Precup,+D&quot;&gt;Doina Precup&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Behbahani,+F&quot;&gt;Feryal Behbahani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Faust,+A&quot;&gt;Aleksandra Faust&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.12917&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Qwen2.5-Coder Technical Report</title>
      <link href="http://localhost:4000/QwenTechRep" rel="alternate" type="text/html" title="Qwen2.5-Coder Technical Report" />
      <published>2024-09-18T00:00:00+03:00</published>
      <updated>2024-09-18T00:00:00+03:00</updated>
      <id>http://localhost:4000/QwenTechRep</id>
      <content type="html" xml:base="http://localhost:4000/QwenTechRep">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general versatility. The model has been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will not only push the boundaries of research in code intelligence but also, through its permissive licensing, encourage broader adoption by developers in real-world applications.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hui,+B&quot;&gt;Binyuan Hui&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+J&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Cui,+Z&quot;&gt;Zeyu Cui&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+J&quot;&gt;Jiaxi Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+D&quot;&gt;Dayiheng Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+L&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+T&quot;&gt;Tianyu Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+J&quot;&gt;Jiajun Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+B&quot;&gt;Bowen Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dang,+K&quot;&gt;Kai Dang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+A&quot;&gt;An Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Men,+R&quot;&gt;Rui Men&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Huang,+F&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ren,+X&quot;&gt;Xingzhang Ren&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ren,+X&quot;&gt;Xuancheng Ren&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhou,+J&quot;&gt;Jingren Zhou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lin,+J&quot;&gt;Junyang Lin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.12186&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Kolmogorov-Arnold Transformer</title>
      <link href="http://localhost:4000/KolArnoldTransformers" rel="alternate" type="text/html" title="Kolmogorov-Arnold Transformer" />
      <published>2024-09-16T00:00:00+03:00</published>
      <updated>2024-09-16T00:00:00+03:00</updated>
      <id>http://localhost:4000/KolArnoldTransformers</id>
      <content type="html" xml:base="http://localhost:4000/KolArnoldTransformers">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Transformers stand as the cornerstone of mordern deep learning. Traditionally, these models rely on multi-layer perceptron (MLP) layers to mix the information between channels. In this paper, we introduce the Kolmogorov-Arnold Transformer (KAT), a novel architecture that replaces MLP layers with Kolmogorov-Arnold Network (KAN) layers to enhance the expressiveness and performance of the model. Integrating KANs into transformers, however, is no easy feat, especially when scaled up. Specifically, we identify three key challenges: (C1) Base function. The standard B-spline function used in KANs is not optimized for parallel computing on modern hardware, resulting in slower inference speeds. (C2) Parameter and Computation Inefficiency. KAN requires a unique function for each input-output pair, making the computation extremely large. (C3) Weight initialization. The initialization of weights in KANs is particularly challenging due to their learnable activation functions, which are critical for achieving convergence in deep neural networks. To overcome the aforementioned challenges, we propose three key solutions: (S1) Rational basis. We replace B-spline functions with rational functions to improve compatibility with modern GPUs. By implementing this in CUDA, we achieve faster computations. (S2) Group KAN. We share the activation weights through a group of neurons, to reduce the computational load without sacrificing performance. (S3) Variance-preserving initialization. We carefully initialize the activation weights to make sure that the activation variance is maintained across layers. With these designs, KAT scales effectively and readily outperforms traditional MLP-based transformers.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+X&quot;&gt;Xingyi Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+X&quot;&gt;Xinchao Wang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.10594&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Breaking reCAPTCHAv2</title>
      <link href="http://localhost:4000/reCaptcha" rel="alternate" type="text/html" title="Breaking reCAPTCHAv2" />
      <published>2024-09-13T00:00:00+03:00</published>
      <updated>2024-09-13T00:00:00+03:00</updated>
      <id>http://localhost:4000/reCaptcha</id>
      <content type="html" xml:base="http://localhost:4000/reCaptcha">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Our work examines the efficacy of employing advanced machine learning methods to solve captchas from Google's reCAPTCHAv2 system. We evaluate the effectiveness of automated systems in solving captchas by utilizing advanced YOLO models for image segmentation and classification. Our main result is that we can solve 100% of the captchas, while previous work only solved 68-71%. Furthermore, our findings suggest that there is no significant difference in the number of challenges humans and bots must solve to pass the captchas in reCAPTCHAv2. This implies that current AI technologies can exploit advanced image-based captchas. We also look under the hood of reCAPTCHAv2, and find evidence that reCAPTCHAv2 is heavily based on cookie and browser history data when evaluating whether a user is human or not. The code is provided alongside this paper.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Plesner,+A&quot;&gt;Andreas Plesner&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Vontobel,+T&quot;&gt;Tobias Vontobel&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wattenhofer,+R&quot;&gt;Roger Wattenhofer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.08831&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">LLaMA-Omni - Seamless Speech Interaction with Large Language Models</title>
      <link href="http://localhost:4000/LlammaOmni" rel="alternate" type="text/html" title="LLaMA-Omni - Seamless Speech Interaction with Large Language Models" />
      <published>2024-09-10T00:00:00+03:00</published>
      <updated>2024-09-10T00:00:00+03:00</updated>
      <id>http://localhost:4000/LlammaOmni</id>
      <content type="html" xml:base="http://localhost:4000/LlammaOmni">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fang,+Q&quot;&gt;Qingkai Fang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Guo,+S&quot;&gt;Shoutao Guo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhou,+Y&quot;&gt;Yan Zhou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ma,+Z&quot;&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+S&quot;&gt;Shaolei Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Feng,+Y&quot;&gt;Yang Feng&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.06666&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">GroUSE - A Benchmark to Evaluate Evaluators in Grounded Question Answering</title>
      <link href="http://localhost:4000/GroUSE" rel="alternate" type="text/html" title="GroUSE - A Benchmark to Evaluate Evaluators in Grounded Question Answering" />
      <published>2024-09-10T00:00:00+03:00</published>
      <updated>2024-09-10T00:00:00+03:00</updated>
      <id>http://localhost:4000/GroUSE</id>
      <content type="html" xml:base="http://localhost:4000/GroUSE">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge.&lt;/p&gt;
&lt;p&gt;To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4's judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection.&lt;/p&gt;
&lt;p&gt;We further show that finetuning Llama-3 on GPT-4's reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4's evaluations and calibration on reference situations.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muller,+S&quot;&gt;Sacha Muller&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Loison,+A&quot;&gt;António Loison&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Omrani,+B&quot;&gt;Bilel Omrani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Viaud,+G&quot;&gt;Gautier Viaud&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://www.arxiv.org/abs/2409.06595&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">LLMs Will Always Hallucinate, and We Need to Live With This</title>
      <link href="http://localhost:4000/AlwaysHallucinate" rel="alternate" type="text/html" title="LLMs Will Always Hallucinate, and We Need to Live With This" />
      <published>2024-09-09T00:00:00+03:00</published>
      <updated>2024-09-09T00:00:00+03:00</updated>
      <id>http://localhost:4000/AlwaysHallucinate</id>
      <content type="html" xml:base="http://localhost:4000/AlwaysHallucinate">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; As Large Language Models become more ubiquitous across domains, it becomes important to examine their inherent limitations critically. This work argues that hallucinations in language models are not just occasional errors but an inevitable feature of these systems. We demonstrate that hallucinations stem from the fundamental mathematical and logical structure of LLMs. It is, therefore, impossible to eliminate them through architectural improvements, dataset enhancements, or fact-checking mechanisms. Our analysis draws on computational theory and Godel's First Incompleteness Theorem, which references the undecidability of problems like the Halting, Emptiness, and Acceptance Problems. We demonstrate that every stage of the LLM process-from training data compilation to fact retrieval, intent classification, and text generation-will have a non-zero probability of producing hallucinations. This work introduces the concept of Structural Hallucination as an intrinsic nature of these systems. By establishing the mathematical certainty of hallucinations, we challenge the prevailing notion that they can be fully mitigated.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/stat?searchtype=author&amp;amp;query=Banerjee,+S&quot;&gt;Sourav Banerjee&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/stat?searchtype=author&amp;amp;query=Agarwal,+A&quot;&gt;Ayushi Agarwal&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/stat?searchtype=author&amp;amp;query=Singla,+S&quot;&gt;Saloni Singla&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.05746&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
</feed>
