<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/research/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-09-11T14:36:21+03:00</updated>
  <id>http://localhost:4000/tag/research/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Theory, Analysis, and Best Practices for Sigmoid Self-Attention</title>
      <link href="http://localhost:4000/SigmoidSelfAttntion" rel="alternate" type="text/html" title="Theory, Analysis, and Best Practices for Sigmoid Self-Attention" />
      <published>2024-09-06T00:00:00+03:00</published>
      <updated>2024-09-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/SigmoidSelfAttntion</id>
      <content type="html" xml:base="http://localhost:4000/SigmoidSelfAttntion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ramapuram,+J&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Danieli,+F&quot;&gt;Federico Danieli&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dhekane,+E&quot;&gt;Eeshan Dhekane&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Weers,+F&quot;&gt;Floris Weers&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Busbridge,+D&quot;&gt;Dan Busbridge&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ablin,+P&quot;&gt;Pierre Ablin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Likhomanenko,+T&quot;&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Digani,+J&quot;&gt;Jagrit Digani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Gu,+Z&quot;&gt;Zijin Gu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shidani,+A&quot;&gt;Amitis Shidani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Webb,+R&quot;&gt;Russ Webb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.04431&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers</title>
      <link href="http://localhost:4000/LLMNovelResearchIdeas" rel="alternate" type="text/html" title="Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers" />
      <published>2024-09-06T00:00:00+03:00</published>
      <updated>2024-09-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/LLMNovelResearchIdeas</id>
      <content type="html" xml:base="http://localhost:4000/LLMNovelResearchIdeas">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p &amp;lt; 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Si,+C&quot;&gt;Chenglei Si&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+D&quot;&gt;Diyi Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hashimoto,+T&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;&amp;lt;/a&amp;gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://www.arxiv.org/abs/2409.04109&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">OLMoE-Open Mixture of Experts Language Models</title>
      <link href="http://localhost:4000/OLMoE" rel="alternate" type="text/html" title="OLMoE-Open Mixture of Experts Language Models" />
      <published>2024-09-03T00:00:00+03:00</published>
      <updated>2024-09-03T00:00:00+03:00</updated>
      <id>http://localhost:4000/OLMoE</id>
      <content type="html" xml:base="http://localhost:4000/OLMoE">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muennighoff,+N&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Soldaini,+L&quot;&gt;Luca Soldaini&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Groeneveld,+D&quot;&gt;Dirk Groeneveld&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lo,+K&quot;&gt;Kyle Lo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Morrison,+J&quot;&gt;Jacob Morrison&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Min,+S&quot;&gt;Sewon Min&lt;/a&gt;,  &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shi,+W&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Walsh,+P&quot;&gt;Pete Walsh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tafjord,+O&quot;&gt;Oyvind Tafjord&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lambert,+N&quot;&gt;Nathan Lambert&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Gu,+Y&quot;&gt;Yuling Gu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Arora,+S&quot;&gt;Shane Arora&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Bhagia,+A&quot;&gt;Akshita Bhagia&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Schwenk,+D&quot;&gt;Dustin Schwenk&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wadden,+D&quot;&gt;David Wadden&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wettig,+A&quot;&gt;Alexander Wettig&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hui,+B&quot;&gt;Binyuan Hui&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dettmers,+T&quot;&gt;Tim Dettmers&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kiela,+D&quot;&gt;Douwe Kiela&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Farhadi,+A&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Smith,+N+A&quot;&gt;Noah A. Smith&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Koh,+P+W&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Singh,+A&quot;&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hajishirzi,+H&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.02060&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">In Defense of RAG in the Era of Long-Context Language Models</title>
      <link href="http://localhost:4000/DefenceOfRAG" rel="alternate" type="text/html" title="In Defense of RAG in the Era of Long-Context Language Models" />
      <published>2024-09-03T00:00:00+03:00</published>
      <updated>2024-09-03T00:00:00+03:00</updated>
      <id>http://localhost:4000/DefenceOfRAG</id>
      <content type="html" xml:base="http://localhost:4000/DefenceOfRAG">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Akkiraju,+R&quot;&gt;Rama Akkiraju&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+T&quot;&gt;Tan Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+A&quot;&gt;Anbang Xu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.01666&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">FLUX that Plays Music</title>
      <link href="http://localhost:4000/FLUXthatPlaysMusic" rel="alternate" type="text/html" title="FLUX that Plays Music" />
      <published>2024-09-01T00:00:00+03:00</published>
      <updated>2024-09-01T00:00:00+03:00</updated>
      <id>http://localhost:4000/FLUXthatPlaysMusic</id>
      <content type="html" xml:base="http://localhost:4000/FLUXthatPlaysMusic">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; This paper explores a simple extension of diffusion-based rectified flow Transformers for text-to-music generation, termed as FluxMusic. Generally, along with design in advanced Flux\footnote{this https URL} model, we transfers it into a latent VAE space of mel-spectrum. It involves first applying a sequence of independent attention to the double text-music stream, followed by a stacked single music stream for denoised patch prediction. We employ multiple pre-trained text encoders to sufficiently capture caption semantic information as well as inference flexibility. In between, coarse textual information, in conjunction with time step embeddings, is utilized in a modulation mechanism, while fine-grained textual details are concatenated with the music patch sequence as inputs. Through an in-depth study, we demonstrate that rectified flow training with an optimized architecture significantly outperforms established diffusion methods for the text-to-music task, as evidenced by various automatic metrics and human preference evaluations. Our experimental data, code, and model weights are made publicly available at: &lt;a href=&quot;https://github.com/feizc/FluxMusic&quot;&gt;this URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fei,+Z&quot;&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fan,+M&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+C&quot;&gt;Changqian Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Huang,+J&quot;&gt;Junshi Huang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.00587&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Diffusion Models Are Real-Time Game Engines</title>
      <link href="http://localhost:4000/DiffusionModels" rel="alternate" type="text/html" title="Diffusion Models Are Real-Time Game Engines" />
      <published>2024-08-27T00:00:00+03:00</published>
      <updated>2024-08-27T00:00:00+03:00</updated>
      <id>http://localhost:4000/DiffusionModels</id>
      <content type="html" xml:base="http://localhost:4000/DiffusionModels">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Valevski,+D&quot;&gt;Dani Valevski&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Leviathan,+Y&quot;&gt;Yaniv Leviathan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Arar,+M&quot;&gt;Moab Arar&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fruchter,+S&quot;&gt;Shlomi Fruchter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.14837&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Automating Thought of Search - A Journey Towards Soundness and Completeness</title>
      <link href="http://localhost:4000/AutoThoughofSearch" rel="alternate" type="text/html" title="Automating Thought of Search - A Journey Towards Soundness and Completeness" />
      <published>2024-08-20T00:00:00+03:00</published>
      <updated>2024-08-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/AutoThoughofSearch</id>
      <content type="html" xml:base="http://localhost:4000/AutoThoughofSearch">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Planning remains one of the last standing bastions for large language models (LLMs), which now turn their attention to search. Most of the literature uses the language models as world models to define the search space, forgoing soundness for the sake of flexibility. A recent work, Thought of Search (ToS), proposed defining the search space with code, having the language models produce that code. ToS requires a human in the loop, collaboratively producing a sound successor func- tion and goal test. The result, however, is worth the effort: all the tested datasets were solved with 100% accuracy. At the same time LLMs have demonstrated significant progress in code generation and refinement for complex reasoning tasks.
In this work, we automate ToS (AutoToS), completely tak- ing the human out of the loop of solving planning problems. AutoToS guides the language model step by step towards the generation of sound and complete search components, through feedback from both generic and domain specific unit tests. We achieve 100% accuracy, with minimal feedback iter- ations, using LLMs of various sizes on all evaluated domains.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; Daniel Cao, Michael Katz, Harsha Kokel, Kavitha Srinivas, Shirin Sohrabi&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/pdf/2408.11326&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model</title>
      <link href="http://localhost:4000/Transfusion" rel="alternate" type="text/html" title="Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model" />
      <published>2024-08-20T00:00:00+03:00</published>
      <updated>2024-08-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/Transfusion</id>
      <content type="html" xml:base="http://localhost:4000/Transfusion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed- modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific en- coding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Omer Levy&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/pdf/2408.11039&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">To Code, or Not To Code? Exploring Impact of Code in Pre-training</title>
      <link href="http://localhost:4000/CodeNoCoode" rel="alternate" type="text/html" title="To Code, or Not To Code? Exploring Impact of Code in Pre-training" />
      <published>2024-08-20T00:00:00+03:00</published>
      <updated>2024-08-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/CodeNoCoode</id>
      <content type="html" xml:base="http://localhost:4000/CodeNoCoode">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs’ performance, there is only limited work analyzing the precise impact of code on non-code tasks. In this work, we system- atically investigate the impact of code data on general performance. We ask “what is the impact of code data used in pre-training on a large variety of downstream tasks beyond code generation”. We conduct extensive ablations and evaluate across a broad range of natural language reasoning tasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for models with sizes ranging from 470M to 2.8B parameters. Across settings, we find a consistent results that code is a critical building block for generalization far beyond coding tasks and improvements to code quality have an outsized impact across all tasks. In particular, compared to text-only pre-training, the ad- dition of code results in up to relative increase of 8.2% in natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement in generative win-rates, and a 12x boost in code performance respectively. Our work suggests investments in code quality and preserving code during pre-training have positive impacts.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; CViraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker1&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/pdf/2408.10914&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Automated Design of Agentic Systems</title>
      <link href="http://localhost:4000/AutoDesign" rel="alternate" type="text/html" title="Automated Design of Agentic Systems" />
      <published>2024-08-15T00:00:00+03:00</published>
      <updated>2024-08-15T00:00:00+03:00</updated>
      <id>http://localhost:4000/AutoDesign</id>
      <content type="html" xml:base="http://localhost:4000/AutoDesign">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. We formulate a new research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. We further demonstrate that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, control flows, and combinations thereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, we show that our algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided we develop it safely, our work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hu,+S&quot;&gt;Shengran Hu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lu,+C&quot;&gt;Cong Lu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Clune,+J&quot;&gt;Jeff Clune&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.08435&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
</feed>
