<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/research/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-09-09T12:24:03+03:00</updated>
  <id>http://localhost:4000/tag/research/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">OLMoE-Open Mixture of Experts Language Models</title>
      <link href="http://localhost:4000/OLMoE" rel="alternate" type="text/html" title="OLMoE-Open Mixture of Experts Language Models" />
      <published>2024-09-03T00:00:00+03:00</published>
      <updated>2024-09-03T00:00:00+03:00</updated>
      <id>http://localhost:4000/OLMoE</id>
      <content type="html" xml:base="http://localhost:4000/OLMoE">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muennighoff,+N&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Soldaini,+L&quot;&gt;Luca Soldaini&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Groeneveld,+D&quot;&gt;Dirk Groeneveld&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lo,+K&quot;&gt;Kyle Lo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Morrison,+J&quot;&gt;Jacob Morrison&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Min,+S&quot;&gt;Sewon Min&lt;/a&gt;,  &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shi,+W&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Walsh,+P&quot;&gt;Pete Walsh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tafjord,+O&quot;&gt;Oyvind Tafjord&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lambert,+N&quot;&gt;Nathan Lambert&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Gu,+Y&quot;&gt;Yuling Gu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Arora,+S&quot;&gt;Shane Arora&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Bhagia,+A&quot;&gt;Akshita Bhagia&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Schwenk,+D&quot;&gt;Dustin Schwenk&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wadden,+D&quot;&gt;David Wadden&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wettig,+A&quot;&gt;Alexander Wettig&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hui,+B&quot;&gt;Binyuan Hui&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dettmers,+T&quot;&gt;Tim Dettmers&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kiela,+D&quot;&gt;Douwe Kiela&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Farhadi,+A&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Smith,+N+A&quot;&gt;Noah A. Smith&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Koh,+P+W&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Singh,+A&quot;&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hajishirzi,+H&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.02060&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">In Defense of RAG in the Era of Long-Context Language Models</title>
      <link href="http://localhost:4000/DefenceOfRAG" rel="alternate" type="text/html" title="In Defense of RAG in the Era of Long-Context Language Models" />
      <published>2024-09-03T00:00:00+03:00</published>
      <updated>2024-09-03T00:00:00+03:00</updated>
      <id>http://localhost:4000/DefenceOfRAG</id>
      <content type="html" xml:base="http://localhost:4000/DefenceOfRAG">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Akkiraju,+R&quot;&gt;Rama Akkiraju&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+T&quot;&gt;Tan Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+A&quot;&gt;Anbang Xu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.01666&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">FLUX that Plays Music</title>
      <link href="http://localhost:4000/FLUXthatPlaysMusic" rel="alternate" type="text/html" title="FLUX that Plays Music" />
      <published>2024-09-01T00:00:00+03:00</published>
      <updated>2024-09-01T00:00:00+03:00</updated>
      <id>http://localhost:4000/FLUXthatPlaysMusic</id>
      <content type="html" xml:base="http://localhost:4000/FLUXthatPlaysMusic">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; This paper explores a simple extension of diffusion-based rectified flow Transformers for text-to-music generation, termed as FluxMusic. Generally, along with design in advanced Flux\footnote{this https URL} model, we transfers it into a latent VAE space of mel-spectrum. It involves first applying a sequence of independent attention to the double text-music stream, followed by a stacked single music stream for denoised patch prediction. We employ multiple pre-trained text encoders to sufficiently capture caption semantic information as well as inference flexibility. In between, coarse textual information, in conjunction with time step embeddings, is utilized in a modulation mechanism, while fine-grained textual details are concatenated with the music patch sequence as inputs. Through an in-depth study, we demonstrate that rectified flow training with an optimized architecture significantly outperforms established diffusion methods for the text-to-music task, as evidenced by various automatic metrics and human preference evaluations. Our experimental data, code, and model weights are made publicly available at: &lt;a href=&quot;https://github.com/feizc/FluxMusic&quot;&gt;this URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fei,+Z&quot;&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fan,+M&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+C&quot;&gt;Changqian Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Huang,+J&quot;&gt;Junshi Huang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.00587&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Diffusion Models Are Real-Time Game Engines</title>
      <link href="http://localhost:4000/DiffusionModels" rel="alternate" type="text/html" title="Diffusion Models Are Real-Time Game Engines" />
      <published>2024-08-27T00:00:00+03:00</published>
      <updated>2024-08-27T00:00:00+03:00</updated>
      <id>http://localhost:4000/DiffusionModels</id>
      <content type="html" xml:base="http://localhost:4000/DiffusionModels">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Valevski,+D&quot;&gt;Dani Valevski&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Leviathan,+Y&quot;&gt;Yaniv Leviathan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Arar,+M&quot;&gt;Moab Arar&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fruchter,+S&quot;&gt;Shlomi Fruchter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.14837&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Automating Thought of Search - A Journey Towards Soundness and Completeness</title>
      <link href="http://localhost:4000/AutoThoughofSearch" rel="alternate" type="text/html" title="Automating Thought of Search - A Journey Towards Soundness and Completeness" />
      <published>2024-08-20T00:00:00+03:00</published>
      <updated>2024-08-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/AutoThoughofSearch</id>
      <content type="html" xml:base="http://localhost:4000/AutoThoughofSearch">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Planning remains one of the last standing bastions for large language models (LLMs), which now turn their attention to search. Most of the literature uses the language models as world models to define the search space, forgoing soundness for the sake of flexibility. A recent work, Thought of Search (ToS), proposed defining the search space with code, having the language models produce that code. ToS requires a human in the loop, collaboratively producing a sound successor func- tion and goal test. The result, however, is worth the effort: all the tested datasets were solved with 100% accuracy. At the same time LLMs have demonstrated significant progress in code generation and refinement for complex reasoning tasks.
In this work, we automate ToS (AutoToS), completely tak- ing the human out of the loop of solving planning problems. AutoToS guides the language model step by step towards the generation of sound and complete search components, through feedback from both generic and domain specific unit tests. We achieve 100% accuracy, with minimal feedback iter- ations, using LLMs of various sizes on all evaluated domains.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; Daniel Cao, Michael Katz, Harsha Kokel, Kavitha Srinivas, Shirin Sohrabi&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/pdf/2408.11326&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model</title>
      <link href="http://localhost:4000/Transfusion" rel="alternate" type="text/html" title="Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model" />
      <published>2024-08-20T00:00:00+03:00</published>
      <updated>2024-08-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/Transfusion</id>
      <content type="html" xml:base="http://localhost:4000/Transfusion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed- modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific en- coding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Omer Levy&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/pdf/2408.11039&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">To Code, or Not To Code? Exploring Impact of Code in Pre-training</title>
      <link href="http://localhost:4000/CodeNoCoode" rel="alternate" type="text/html" title="To Code, or Not To Code? Exploring Impact of Code in Pre-training" />
      <published>2024-08-20T00:00:00+03:00</published>
      <updated>2024-08-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/CodeNoCoode</id>
      <content type="html" xml:base="http://localhost:4000/CodeNoCoode">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs’ performance, there is only limited work analyzing the precise impact of code on non-code tasks. In this work, we system- atically investigate the impact of code data on general performance. We ask “what is the impact of code data used in pre-training on a large variety of downstream tasks beyond code generation”. We conduct extensive ablations and evaluate across a broad range of natural language reasoning tasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for models with sizes ranging from 470M to 2.8B parameters. Across settings, we find a consistent results that code is a critical building block for generalization far beyond coding tasks and improvements to code quality have an outsized impact across all tasks. In particular, compared to text-only pre-training, the ad- dition of code results in up to relative increase of 8.2% in natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement in generative win-rates, and a 12x boost in code performance respectively. Our work suggests investments in code quality and preserving code during pre-training have positive impacts.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; CViraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker1&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/pdf/2408.10914&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Automated Design of Agentic Systems</title>
      <link href="http://localhost:4000/AutoDesign" rel="alternate" type="text/html" title="Automated Design of Agentic Systems" />
      <published>2024-08-15T00:00:00+03:00</published>
      <updated>2024-08-15T00:00:00+03:00</updated>
      <id>http://localhost:4000/AutoDesign</id>
      <content type="html" xml:base="http://localhost:4000/AutoDesign">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. We formulate a new research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. We further demonstrate that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, control flows, and combinations thereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, we show that our algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided we develop it safely, our work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hu,+S&quot;&gt;Shengran Hu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lu,+C&quot;&gt;Cong Lu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Clune,+J&quot;&gt;Jeff Clune&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.08435&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</title>
      <link href="http://localhost:4000/HybridRAG" rel="alternate" type="text/html" title="Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" />
      <published>2024-08-06T00:00:00+03:00</published>
      <updated>2024-08-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/HybridRAG</id>
      <content type="html" xml:base="http://localhost:4000/HybridRAG">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&amp;amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&amp;amp;A format, and hence provide a natural set of pairs of ground-truth Q&amp;amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sarmah,+B&quot;&gt;Bhaskarjit Sarmah&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hall,+B&quot;&gt;Benika Hall&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Rao,+R&quot;&gt;Rohan Rao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Patel,+S&quot;&gt;Sunil Patel&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Pasquali,+S&quot;&gt;Stefano Pasquali&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Mehta,+D&quot;&gt;Dhagash Mehta&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.04948&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</title>
      <link href="http://localhost:4000/TransformerExplainer" rel="alternate" type="text/html" title="Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" />
      <published>2024-08-06T00:00:00+03:00</published>
      <updated>2024-08-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/TransformerExplainer</id>
      <content type="html" xml:base="http://localhost:4000/TransformerExplainer">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user's browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public's education access to modern generative AI techniques. Our open-sourced tool is available at &lt;a href=&quot;https://poloclub.github.io/transformer-explainer/&quot;&gt;this https URL&lt;/a&gt;. A video demo is available at &lt;a href=&quot;https://youtu.be/ECR4oAwocjs&quot;&gt;this https URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Cho,+A&quot;&gt;Aeree Cho&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kim,+G+C&quot;&gt;Grace C. Kim&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Karpekov,+A&quot;&gt;Alexander Karpekov&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Helbling,+A&quot;&gt;Alec Helbling&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+Z+J&quot;&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lee,+S&quot;&gt;Seongmin Lee&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hoover,+B&quot;&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chau,+D+H&quot;&gt;Duen Horng Chau&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.04619&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
</feed>
