<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/research/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-12-02T17:02:16+02:00</updated>
  <id>http://localhost:4000/tag/research/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Rapid Response:Mitigating LLM Jailbreaks with a Few Examples</title>
      <link href="http://localhost:4000/RapidResponse" rel="alternate" type="text/html" title="Rapid Response:Mitigating LLM Jailbreaks with a Few Examples" />
      <published>2024-11-12T00:00:00+02:00</published>
      <updated>2024-11-12T00:00:00+02:00</updated>
      <id>http://localhost:4000/RapidResponse</id>
      <content type="html" xml:base="http://localhost:4000/RapidResponse">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; As large language models (LLMs) grow more powerful, ensuring their safety against misuse becomes crucial. While researchers have focused on developing robust defenses, no method has yet achieved complete invulnerability to attacks. We propose an alternative approach: instead of seeking perfect adversarial robustness, we develop rapid response techniques to look to block whole classes of jailbreaks after observing only a handful of attacks. To study this setting, we develop RapidResponseBench, a benchmark that measures a defense's robustness against various jailbreak strategies after adapting to a few observed examples. We evaluate five rapid response methods, all of which use jailbreak proliferation, where we automatically generate additional jailbreaks similar to the examples observed. Our strongest method, which fine-tunes an input classifier to block proliferated jailbreaks, reduces attack success rate by a factor greater than 240 on an in-distribution set of jailbreaks and a factor greater than 15 on an out-of-distribution set, having observed just one example of each jailbreaking strategy. Moreover, further studies suggest that the quality of proliferation model and number of proliferated examples play an key role in the effectiveness of this defense. Overall, our results highlight the potential of responding rapidly to novel jailbreaks to limit LLM misuse. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Peng,+A&quot; rel=&quot;nofollow&quot;&gt;Alwin Peng&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Michael,+J&quot; rel=&quot;nofollow&quot;&gt;Julian Michael&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sleight,+H&quot; rel=&quot;nofollow&quot;&gt;Henry Sleight&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Perez,+E&quot; rel=&quot;nofollow&quot;&gt;Ethan Perez&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sharma,+M&quot; rel=&quot;nofollow&quot;&gt;Mrinank Sharma&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2411.07494&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">OpenCoder The Open Cookbook for Top-Tier Code Large Language Models</title>
      <link href="http://localhost:4000/OpenCoder" rel="alternate" type="text/html" title="OpenCoder The Open Cookbook for Top-Tier Code Large Language Models" />
      <published>2024-11-09T00:00:00+02:00</published>
      <updated>2024-11-09T00:00:00+02:00</updated>
      <id>http://localhost:4000/OpenCoder</id>
      <content type="html" xml:base="http://localhost:4000/OpenCoder">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an &quot;open cookbook&quot; for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Huang,+S&quot; rel=&quot;nofollow&quot;&gt;Siming Huang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Cheng,+T&quot; rel=&quot;nofollow&quot;&gt;Tianhao Cheng&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+J&quot; rel=&quot;nofollow&quot;&gt;J.K. Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hao,+J&quot; rel=&quot;nofollow&quot;&gt;Jiaran Hao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Song,+L&quot; rel=&quot;nofollow&quot;&gt;Liuyihan Song&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+Y&quot; rel=&quot;nofollow&quot;&gt;Yang Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+J&quot; rel=&quot;nofollow&quot;&gt;J. Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+J&quot; rel=&quot;nofollow&quot;&gt;J.H. Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+C&quot; rel=&quot;nofollow&quot;&gt;Chenchen Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chai,+L&quot; rel=&quot;nofollow&quot;&gt;Linzheng Chai&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yuan,+R&quot; rel=&quot;nofollow&quot;&gt;Ruifeng Yuan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+Z&quot; rel=&quot;nofollow&quot;&gt;Zhaoxiang Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fu,+J&quot; rel=&quot;nofollow&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+Q&quot; rel=&quot;nofollow&quot;&gt;Qian Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+G&quot; rel=&quot;nofollow&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+Z&quot; rel=&quot;nofollow&quot;&gt;Zili Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Qi,+Y&quot; rel=&quot;nofollow&quot;&gt;Yuan Qi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+Y&quot; rel=&quot;nofollow&quot;&gt;Yinghui Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chu,+W&quot; rel=&quot;nofollow&quot;&gt;Wei Chu&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2411.04905&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Mixture-of-Transformers:A Sparse and Scalable Architecture for Multi-Modal Foundation Models</title>
      <link href="http://localhost:4000/multiTransf" rel="alternate" type="text/html" title="Mixture-of-Transformers:A Sparse and Scalable Architecture for Multi-Modal Foundation Models" />
      <published>2024-11-09T00:00:00+02:00</published>
      <updated>2024-11-09T00:00:00+02:00</updated>
      <id>http://localhost:4000/multiTransf</id>
      <content type="html" xml:base="http://localhost:4000/multiTransf">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. We evaluate MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8\% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2\% of the wall-clock time and text quality in 75.6\% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs). &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liang,+W&quot; rel=&quot;nofollow&quot;&gt;Weixin Liang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+L&quot; rel=&quot;nofollow&quot;&gt;Lili Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Luo,+L&quot; rel=&quot;nofollow&quot;&gt;Liang Luo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Iyer,+S&quot; rel=&quot;nofollow&quot;&gt;Srinivasan Iyer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dong,+N&quot; rel=&quot;nofollow&quot;&gt;Ning Dong&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhou,+C&quot; rel=&quot;nofollow&quot;&gt;Chunting Zhou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ghosh,+G&quot; rel=&quot;nofollow&quot;&gt;Gargi Ghosh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lewis,+M&quot; rel=&quot;nofollow&quot;&gt;Mike Lewis&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yih,+W&quot; rel=&quot;nofollow&quot;&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zettlemoyer,+L&quot; rel=&quot;nofollow&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lin,+X+V&quot; rel=&quot;nofollow&quot;&gt;Xi Victoria Lin&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2411.04996&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Mixtures of In-Context Learners</title>
      <link href="http://localhost:4000/InContextLearners" rel="alternate" type="text/html" title="Mixtures of In-Context Learners" />
      <published>2024-11-05T00:00:00+02:00</published>
      <updated>2024-11-05T00:00:00+02:00</updated>
      <id>http://localhost:4000/InContextLearners</id>
      <content type="html" xml:base="http://localhost:4000/InContextLearners">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; In-context learning (ICL) adapts LLMs by providing demonstrations without fine-tuning the model parameters; however, it does not differentiate between demonstrations and quadratically increases the complexity of Transformer LLMs, exhausting the memory. As a solution, we propose Mixtures of In-Context Learners (MoICL), a novel approach to treat subsets of demonstrations as experts and learn a weighting function to merge their output distributions based on a training set. In our experiments, we show performance improvements on 5 out of 7 classification datasets compared to a set of strong baselines (up to +13\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of ICL by reducing the inference time needed to achieve the same performance with fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to +11\%), imbalanced (up to +49\%), or noisy demonstrations (up to +38\%) or can filter these out from datasets. Overall, MoICL is a more expressive approach to learning from demonstrations without exhausting the context window or memory. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hong,+G&quot; rel=&quot;nofollow&quot;&gt;Giwon Hong&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=van+Krieken,+E&quot; rel=&quot;nofollow&quot;&gt;Emile van Krieken&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ponti,+E&quot; rel=&quot;nofollow&quot;&gt;Edoardo Ponti&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Malkin,+N&quot; rel=&quot;nofollow&quot;&gt;Nikolay Malkin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Minervini,+P&quot; rel=&quot;nofollow&quot;&gt;Pasquale Minervini&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.20771&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">LoRA vs Full Fine-tuning:An Illusion of Equivalence</title>
      <link href="http://localhost:4000/LoraVSFT" rel="alternate" type="text/html" title="LoRA vs Full Fine-tuning:An Illusion of Equivalence" />
      <published>2024-10-28T00:00:00+02:00</published>
      <updated>2024-10-28T00:00:00+02:00</updated>
      <id>http://localhost:4000/LoraVSFT</id>
      <content type="html" xml:base="http://localhost:4000/LoraVSFT">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to match the performance of fully fine-tuned models on various tasks with an extreme reduction in the number of trainable parameters. Even in settings where both methods learn similarly accurate models, \emph{are their learned solutions really equivalent?} We study how different fine-tuning methods change pre-trained models by analyzing the model's weight matrices through the lens of their spectral properties. We find that full fine-tuning and LoRA yield weight matrices whose singular value decompositions exhibit very different structure; moreover, the fine-tuned models themselves show distinct generalization behaviors when tested outside the adaptation task's distribution. More specifically, we first show that the weight matrices trained with LoRA have new, high-ranking singular vectors, which we call \emph{intruder dimensions}. Intruder dimensions do not appear during full fine-tuning. Second, we show that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. Higher-rank, rank-stabilized LoRA models closely mirror full fine-tuning, even when performing on par with lower-rank LoRA models on the same tasks. These results suggest that models updated with LoRA and full fine-tuning access different parts of parameter space, even when they perform equally on the fine-tuned distribution. We conclude by examining why intruder dimensions appear in LoRA fine-tuned models, why they are undesirable, and how their effects can be minimized. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shuttleworth,+R&quot; rel=&quot;nofollow&quot;&gt;Reece Shuttleworth&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Andreas,+J&quot; rel=&quot;nofollow&quot;&gt;Jacob Andreas&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Torralba,+A&quot; rel=&quot;nofollow&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sharma,+P&quot; rel=&quot;nofollow&quot;&gt;Pratyusha Sharma&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.21228&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">MrT5:Dynamic Token Merging for Efficient Byte-level Language Models</title>
      <link href="http://localhost:4000/DynamicToken" rel="alternate" type="text/html" title="MrT5:Dynamic Token Merging for Efficient Byte-level Language Models" />
      <published>2024-10-28T00:00:00+02:00</published>
      <updated>2024-10-28T00:00:00+02:00</updated>
      <id>http://localhost:4000/DynamicToken</id>
      <content type="html" xml:base="http://localhost:4000/DynamicToken">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Models that rely on subword tokenization have significant drawbacks, such as sensitivity to character-level noise like spelling errors and inconsistent compression rates across different languages and scripts. While character- or byte-level models like ByT5 attempt to address these concerns, they have not gained widespread adoption -- processing raw byte streams without tokenization results in significantly longer sequence lengths, making training and inference inefficient. This work introduces MrT5 (MergeT5), a more efficient variant of ByT5 that integrates a token deletion mechanism in its encoder to dynamically shorten the input sequence length. After processing through a fixed number of encoder layers, a learnt delete gate determines which tokens are to be removed and which are to be retained for subsequent layers. MrT5 effectively ``merges'' critical information from deleted tokens into a more compact sequence, leveraging contextual information from the remaining tokens. In continued pre-training experiments, we find that MrT5 can achieve significant gains in inference runtime with minimal effect on performance. When trained on English text, MrT5 demonstrates the capability to transfer its deletion feature zero-shot across several languages, with significant additional improvements following multilingual training. Furthermore, MrT5 shows comparable accuracy to ByT5 on downstream evaluations such as XNLI and character-level tasks while reducing sequence lengths by up to 80%. Our approach presents a solution to the practical limitations of existing byte-level models. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kallini,+J&quot; rel=&quot;nofollow&quot;&gt;Julie Kallini&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Murty,+S&quot; rel=&quot;nofollow&quot;&gt;Shikhar Murty&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Manning,+C+D&quot; rel=&quot;nofollow&quot;&gt;Christopher D. Manning&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Potts,+C&quot; rel=&quot;nofollow&quot;&gt;Christopher Potts&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Csord%C3%A1s,+R&quot; rel=&quot;nofollow&quot;&gt;Róbert Csordás&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.20771&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Large Language Models Reflect the Ideology of their Creators</title>
      <link href="http://localhost:4000/CreatorsIdeology" rel="alternate" type="text/html" title="Large Language Models Reflect the Ideology of their Creators" />
      <published>2024-10-24T00:00:00+03:00</published>
      <updated>2024-10-24T00:00:00+03:00</updated>
      <id>http://localhost:4000/CreatorsIdeology</id>
      <content type="html" xml:base="http://localhost:4000/CreatorsIdeology">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use.
In this paper, we uncover notable diversity in the ideological stance exhibited across different LLMs and languages in which they are accessed. We do this by prompting a diverse panel of popular LLMs to describe a large number of prominent and controversial personalities from recent world history, both in English and in Chinese. By identifying and analyzing moral assessments reflected in the generated descriptions, we find consistent normative differences between how the same LLM responds in Chinese compared to English. Similarly, we identify normative disagreements between Western and non-Western LLMs about prominent actors in geopolitical conflicts. Furthermore, popularly hypothesized disparities in political goals among Western models are reflected in significant normative differences related to inclusion, social inequality, and political scandals.
Our results show that the ideological stance of an LLM often reflects the worldview of its creators. This raises important concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically `unbiased', and it poses risks for political instrumentalization. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Buyl,+M&quot;&gt;Maarten Buyl&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Rogiers,+A&quot;&gt;Alexander Rogiers&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Noels,+S&quot;&gt;Sander Noels&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dominguez-Catena,+I&quot;&gt;Iris Dominguez-Catena&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Heiter,+E&quot;&gt;Edith Heiter&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Romero,+R&quot;&gt;Raphael Romero&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Johary,+I&quot;&gt;Iman Johary&lt;/a&gt;
, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Mara,+A&quot;&gt;Alexandru-Cristian Mara&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lijffijt,+J&quot;&gt;Jefrey Lijffijt&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=De+Bie,+T&quot;&gt;Tijl De Bie&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.18417&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">A Theoretical Understanding of Chain-of-Thought:Coherent Reasoning and Error-Aware Demonstration</title>
      <link href="http://localhost:4000/CoT" rel="alternate" type="text/html" title="A Theoretical Understanding of Chain-of-Thought:Coherent Reasoning and Error-Aware Demonstration" />
      <published>2024-10-21T00:00:00+03:00</published>
      <updated>2024-10-21T00:00:00+03:00</updated>
      <id>http://localhost:4000/CoT</id>
      <content type="html" xml:base="http://localhost:4000/CoT">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration. Our experiments validate the effectiveness of the proposed approach. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Cui,+Y&quot; rel=&quot;nofollow&quot;&gt;Yingqian Cui&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=He,+P&quot; rel=&quot;nofollow&quot;&gt;Pengfei He&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tang,+X&quot; rel=&quot;nofollow&quot;&gt;Xianfeng Tang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=He,+Q&quot; rel=&quot;nofollow&quot;&gt;Qi He&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Luo,+C&quot; rel=&quot;nofollow&quot;&gt;Chen Luo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tang,+J&quot; rel=&quot;nofollow&quot;&gt;Jiliang Tang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xing,+Y&quot; rel=&quot;nofollow&quot;&gt;Yue Xing&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.16540&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Looking Inward:Language Models Can Learn About Themselves by Introspection</title>
      <link href="http://localhost:4000/Introspection" rel="alternate" type="text/html" title="Looking Inward:Language Models Can Learn About Themselves by Introspection" />
      <published>2024-10-17T00:00:00+03:00</published>
      <updated>2024-10-17T00:00:00+03:00</updated>
      <id>http://localhost:4000/Introspection</id>
      <content type="html" xml:base="http://localhost:4000/Introspection">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model's training data.&lt;/p&gt;

&lt;p&gt;We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, &quot;Given the input P, would your output favor the short- or long-term option?&quot; If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior even if M2 is trained on M1's ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).&lt;/p&gt;

&lt;p&gt;In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Binder,+F+J&quot; rel=&quot;nofollow&quot;&gt;Felix J Binder&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chua,+J&quot; rel=&quot;nofollow&quot;&gt;James Chua&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Korbak,+T&quot; rel=&quot;nofollow&quot;&gt;Tomek Korbak&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sleight,+H&quot; rel=&quot;nofollow&quot;&gt;Henry Sleight&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hughes,+J&quot; rel=&quot;nofollow&quot;&gt;John Hughes&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Long,+R&quot; rel=&quot;nofollow&quot;&gt;Robert Long&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Perez,+E&quot; rel=&quot;nofollow&quot;&gt;Ethan Perez&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Turpin,+M&quot; rel=&quot;nofollow&quot;&gt;Miles Turpin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Evans,+O&quot; rel=&quot;nofollow&quot;&gt;Owain Evans&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.13787&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Agent-as-a-Judge:Evaluate Agents with Agents</title>
      <link href="http://localhost:4000/AgentJudge" rel="alternate" type="text/html" title="Agent-as-a-Judge:Evaluate Agents with Agents" />
      <published>2024-10-16T00:00:00+03:00</published>
      <updated>2024-10-16T00:00:00+03:00</updated>
      <id>http://localhost:4000/AgentJudge</id>
      <content type="html" xml:base="http://localhost:4000/AgentJudge">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhuge,+M&quot;&gt;Mingchen Zhuge&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhao,+C&quot;&gt;Changsheng Zhao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ashley,+D&quot;&gt;Dylan Ashley&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+W&quot;&gt;Wenyi Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Khizbullin,+D&quot;&gt;Dmitrii Khizbullin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xiong,+Y&quot;&gt;Yunyang Xiong&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+Z&quot;&gt;Zechun Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chang,+E&quot;&gt;Ernie Chang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Krishnamoorthi,+R&quot;&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tian,+Y&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shi,+Y&quot;&gt;Yangyang Shi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chandra,+V&quot;&gt;Vikas Chandra&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Schmidhuber,+J&quot;&gt;Jürgen Schmidhuber&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.10934&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
</feed>
