<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/research/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-10-13T10:44:15+03:00</updated>
  <id>http://localhost:4000/tag/research/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Intelligence at the Edge of Chaos</title>
      <link href="http://localhost:4000/Chaos" rel="alternate" type="text/html" title="Intelligence at the Edge of Chaos" />
      <published>2024-10-03T00:00:00+03:00</published>
      <updated>2024-10-03T00:00:00+03:00</updated>
      <id>http://localhost:4000/Chaos</id>
      <content type="html" xml:base="http://localhost:4000/Chaos">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, we evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. Our findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. We conjecture that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+S&quot;&gt;Shiyang Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Patel,+A&quot;&gt;Aakash Patel&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Rizvi,+S+A&quot;&gt;Syed A Rizvi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+N&quot;&gt;Nianchen Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=He,+S&quot;&gt;Sizhuang He&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Karbasi,+A&quot;&gt;Amin Karbasi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zappala,+E&quot;&gt;Emanuele Zappala&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=van+Dijk,+D&quot;&gt;David van Dijk&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.02536&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Were RNNs All We Needed?</title>
      <link href="http://localhost:4000/RNNneeded" rel="alternate" type="text/html" title="Were RNNs All We Needed?" />
      <published>2024-10-02T00:00:00+03:00</published>
      <updated>2024-10-02T00:00:00+03:00</updated>
      <id>http://localhost:4000/RNNneeded</id>
      <content type="html" xml:base="http://localhost:4000/RNNneeded">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; The scalability limitations of Transformers regarding sequence length have renewed interest in recurrent sequence models that are parallelizable during training. As a result, many novel recurrent architectures, such as S4, Mamba, and Aaren, have been proposed that achieve comparable performance. In this work, we revisit traditional recurrent neural networks (RNNs) from over a decade ago: LSTMs (1997) and GRUs (2014). While these models were slow due to requiring to backpropagate through time (BPTT), we show that by removing their hidden state dependencies from their input, forget, and update gates, LSTMs and GRUs no longer need to BPTT and can be efficiently trained in parallel. Building on this, we introduce minimal versions (minLSTMs and minGRUs) that (1) use significantly fewer parameters than their traditional counterparts and (2) are fully parallelizable during training (175x faster for a sequence of length 512). Lastly, we show that these stripped-down versions of decade-old RNNs match the empirical performance of recent sequence models.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Feng,+L&quot;&gt;Leo Feng&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tung,+F&quot;&gt;Frederick Tung&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ahmed,+M+O&quot;&gt;Mohamed Osama Ahmed&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Bengio,+Y&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hajimirsadegh,+H&quot;&gt;Hossein Hajimirsadegh&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.01201&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Logic-of-Thought. Injecting Logic into Contexts for Full Reasoning in Large Language Models</title>
      <link href="http://localhost:4000/redefineRLHF" rel="alternate" type="text/html" title="Logic-of-Thought. Injecting Logic into Contexts for Full Reasoning in Large Language Models" />
      <published>2024-09-30T00:00:00+03:00</published>
      <updated>2024-09-30T00:00:00+03:00</updated>
      <id>http://localhost:4000/redefineRLHF</id>
      <content type="html" xml:base="http://localhost:4000/redefineRLHF">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives.
Our empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM &amp;amp; reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+T&quot;&gt;Tengyu Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Helenowski,+E&quot;&gt;Eryk Helenowski&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sankararaman,+K+A&quot;&gt;Karthik Abinav Sankararaman&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jin,+D&quot;&gt;Di Jin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Peng,+K&quot;&gt;Kaiyan Peng&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Han,+E&quot;&gt;Eric Han&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Nie,+S&quot;&gt;Shaoliang Nie&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhu,+C&quot;&gt;Chen Zhu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+H&quot;&gt;Hejia Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhou,+W&quot;&gt;Wenxuan Zhou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zeng,+Z&quot;&gt;Zhouhao Zeng&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=He,+Y&quot;&gt;Yun He&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Mandyam,+K&quot;&gt;Karishma Mandyam&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Talabzadeh,+A&quot;&gt;Arya Talabzadeh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Khabsa,+M&quot;&gt;Madian Khabsa&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Cohen,+G&quot;&gt;Gabriel Cohen&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tian,+Y&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ma,+H&quot;&gt;Hao Ma&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+S&quot;&gt;Sinong Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fang,+H&quot;&gt;Han Fang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.20370&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Diffusion-based Visual Foundation Model for High-quality Dense Prediction</title>
      <link href="http://localhost:4000/Lotus" rel="alternate" type="text/html" title="Diffusion-based Visual Foundation Model for High-quality Dense Prediction" />
      <published>2024-09-26T00:00:00+03:00</published>
      <updated>2024-09-26T00:00:00+03:00</updated>
      <id>http://localhost:4000/Lotus</id>
      <content type="html" xml:base="http://localhost:4000/Lotus">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc. Project page: &lt;a href=&quot;https://lotus3d.github.io/&quot;&gt;this https URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=He,+J&quot;&gt;Jing He&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Li,+H&quot;&gt;Haodong Li&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yin,+W&quot;&gt;Wei Yin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liang,+Y&quot;&gt;Yixun Liang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Li,+L&quot;&gt;Leheng Li&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhou,+K&quot;&gt;Kaiqiang Zhou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+H&quot;&gt;Hongbo Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+B&quot;&gt;Bingbing Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chen,+Y&quot;&gt;Ying-Cong Chen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.18124&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Logic-of-Thought. Injecting Logic into Contexts for Full Reasoning in Large Language Models</title>
      <link href="http://localhost:4000/LogicOfThough" rel="alternate" type="text/html" title="Logic-of-Thought. Injecting Logic into Contexts for Full Reasoning in Large Language Models" />
      <published>2024-09-26T00:00:00+03:00</published>
      <updated>2024-09-26T00:00:00+03:00</updated>
      <id>http://localhost:4000/LogicOfThough</id>
      <content type="html" xml:base="http://localhost:4000/LogicOfThough">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information from input context, and utilizes the generated logical information as an additional augmentation to the input prompts, thereby enhancing the capability of logical reasoning. The LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, the LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%; moreover, it improves Chain-of-Thought with Self-Consistency's performance on LogiQA by +5%; additionally, it boosts performance of Tree-of-Thoughts on ProofWriter dataset by +8%.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+T&quot;&gt;Tongxuan Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+W&quot;&gt;Wenjiang Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Huang,+W&quot;&gt;Weizhe Huang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+X&quot;&gt;Xingyu Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+J&quot;&gt;Jiaxing Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+H&quot;&gt;Hailong Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Li,+J&quot;&gt;Jing Li&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.17539&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">VPTQ-Extreme Low-bit Vector Post-Training Quantization for Large Language Models</title>
      <link href="http://localhost:4000/VPTQ" rel="alternate" type="text/html" title="VPTQ-Extreme Low-bit Vector Post-Training Quantization for Large Language Models" />
      <published>2024-09-25T00:00:00+03:00</published>
      <updated>2024-09-25T00:00:00+03:00</updated>
      <id>http://localhost:4000/VPTQ</id>
      <content type="html" xml:base="http://localhost:4000/VPTQ">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables.
In this paper, we introduce Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by 0.01-0.34 on LLaMA-2, 0.38-0.68 on Mistral-7B, 4.41-7.34 on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of 0.79-1.5% on LLaMA-2, 1% on Mistral-7B, 11-22% on LLaMA-3 on QA tasks on average. We only utilize 10.4-18.6% of the quantization algorithm execution time, resulting in a 1.6-1.8× increase in inference throughput compared to SOTA.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+Y&quot;&gt;Yifei Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wen,+J&quot;&gt;Jicheng Wen&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+Y&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ye,+S&quot;&gt;Shengyu Ye&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+L+L&quot;&gt;Li Lyna Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Cao,+T&quot;&gt;Ting Cao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Li,+C&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+M&quot;&gt;Mao Yang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.17066&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Rereading Improves Reasoning in Large Language Models</title>
      <link href="http://localhost:4000/rereading" rel="alternate" type="text/html" title="Rereading Improves Reasoning in Large Language Models" />
      <published>2024-09-21T00:00:00+03:00</published>
      <updated>2024-09-21T00:00:00+03:00</updated>
      <id>http://localhost:4000/rereading</id>
      <content type="html" xml:base="http://localhost:4000/rereading">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, Re2, i.e., &lt;strong&gt;Re&lt;/strong&gt;-&lt;strong&gt;Re&lt;/strong&gt;ading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, Re2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, Re2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, Re2 facilitates a &quot;bidirectional&quot; encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of Re2, illustrating its potential to enable &quot;bidirectional&quot; attention mechanisms. We then evaluate Re2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal Re2's adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies. Our code is available at &lt;a href=&quot;https://github.com/Tebmer/Rereading-LLM-Reasoning/&quot;&gt;this https URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+X&quot;&gt;Xiaohan Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tao,+C&quot;&gt;Chongyang Tao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shen,+T&quot;&gt;Tao Shen&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+C&quot;&gt;Can Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+H&quot;&gt;Hongbo Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Long,+G&quot;&gt;Guodong Long&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lou,+J&quot;&gt;Jian-guang Lou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ma,+S&quot;&gt;Shuai Ma&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2309.06275&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Michelangelo-Long Context Evaluations Beyond Haystacks via Latent Structure Queries</title>
      <link href="http://localhost:4000/Michelangelo" rel="alternate" type="text/html" title="Michelangelo-Long Context Evaluations Beyond Haystacks via Latent Structure Queries" />
      <published>2024-09-20T00:00:00+03:00</published>
      <updated>2024-09-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/Michelangelo</id>
      <content type="html" xml:base="http://localhost:4000/Michelangelo">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce Michelangelo: a minimal, synthetic, and unleaked long-context reasoning evaluation for large language models which is also easy to automatically score. This evaluation is derived via a novel, unifying framework for evaluations over arbitrarily long contexts which measure the model's ability to do more than retrieve a single piece of information from its context. The central idea of the Latent Structure Queries framework (LSQ) is to construct tasks which require a model to ``chisel away'' the irrelevant information in the context, revealing a latent structure in the context. To verify a model's understanding of this latent structure, we query the model for details of the structure. Using LSQ, we produce three diagnostic long-context evaluations across code and natural-language domains intended to provide a stronger signal of long-context language model capabilities. We perform evaluations on several state-of-the-art models and demonstrate both that a) the proposed evaluations are high-signal and b) that there is significant room for improvement in synthesizing long-context information.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Vodrahalli,+K&quot;&gt;Kiran Vodrahalli&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ontanon,+S&quot;&gt;Santiago Ontanon&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tripuraneni,+N&quot;&gt;Nilesh Tripuraneni&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+K&quot;&gt;Kelvin Xu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jain,+S&quot;&gt;Sanil Jain&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shivanna,+R&quot;&gt;Rakesh Shivanna&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hui,+J&quot;&gt;Jeffrey Hui&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dikkala,+N&quot;&gt;Nishanth Dikkala&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kazemi,+M&quot;&gt;Mehran Kazemi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fatemi,+B&quot;&gt;Bahare Fatemi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Anil,+R&quot;&gt;Rohan Anil&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dyer,+E&quot;&gt;Ethan Dyer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shakeri,+S&quot;&gt;Siamak Shakeri&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Vij,+R&quot;&gt;Roopali Vij&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Mehta,+H&quot;&gt;Harsh Mehta&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ramasesh,+V&quot;&gt;Vinay Ramasesh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Le,+Q&quot;&gt;Quoc Le&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chi,+E&quot;&gt;Ed Chi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lu,+Y&quot;&gt;Yifeng Lu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Firat,+O&quot;&gt;Orhan Firat&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lazaridou,+A&quot;&gt;Angeliki Lazaridou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lespiau,+J&quot;&gt;Jean-Baptiste Lespiau&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Attaluri,+N&quot;&gt;Nithya Attaluri&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Olszewska,+K&quot;&gt;Kate Olszewska&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.12640&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">LLMs Still Can’t Plan; Can LRMs? A Preliminary Evaluation of OpenAI’s o1 on PlanBench</title>
      <link href="http://localhost:4000/EvaluationOpenAIo1" rel="alternate" type="text/html" title="LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench" />
      <published>2024-09-20T00:00:00+03:00</published>
      <updated>2024-09-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/EvaluationOpenAIo1</id>
      <content type="html" xml:base="http://localhost:4000/EvaluationOpenAIo1">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities. PlanBench, an extensible benchmark we developed in 2022, soon after the release of GPT3, has remained an important tool for evaluating the planning abilities of LLMs. Despite the slew of new private and open source LLMs since GPT3, progress on this benchmark has been surprisingly slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs--making it a new kind of model: a Large Reasoning Model (LRM). Using this development as a catalyst, this paper takes a comprehensive look at how well current LLMs and new LRMs do on PlanBench. As we shall see, while o1's performance is a quantum improvement on the benchmark, outpacing the competition, it is still far from saturating it. This improvement also brings to the fore questions about accuracy, efficiency, and guarantees which must be considered before deploying such systems.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Valmeekam,+K&quot;&gt;Karthik Valmeekam&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Stechly,+K&quot;&gt;Kaya Stechly&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kambhampati,+S&quot;&gt;Subbarao Kambhampati&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.13373&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Training Language Models to Self-Correct via Reinforcement Learning</title>
      <link href="http://localhost:4000/SelfCorrection" rel="alternate" type="text/html" title="Training Language Models to Self-Correct via Reinforcement Learning" />
      <published>2024-09-19T00:00:00+03:00</published>
      <updated>2024-09-19T00:00:00+03:00</updated>
      <id>http://localhost:4000/SelfCorrection</id>
      <content type="html" xml:base="http://localhost:4000/SelfCorrection">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Existing approaches for training self-correction either require multiple models or rely on a more capable model or other forms of supervision. To this end, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, we observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kumar,+A&quot;&gt;Aviral Kumar&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhuang,+V&quot;&gt;Vincent Zhuang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Agarwal,+R&quot;&gt;Rishabh Agarwal&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Su,+Y&quot;&gt;Yi Su&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Co-Reyes,+J+D&quot;&gt;John D Co-Reyes&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Singh,+A&quot;&gt;Avi Singh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Baumli,+K&quot;&gt;Kate Baumli&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Iqbal,+S&quot;&gt;Shariq Iqbal&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Bishop,+C&quot;&gt;Colton Bishop&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Roelofs,+R&quot;&gt;Rebecca Roelofs&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+L+M&quot;&gt;Lei M Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=McKinney,+K&quot;&gt;Kay McKinney&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shrivastava,+D&quot;&gt;Disha Shrivastava&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Paduraru,+C&quot;&gt;Cosmin Paduraru&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tucker,+G&quot;&gt;George Tucker&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Precup,+D&quot;&gt;Doina Precup&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Behbahani,+F&quot;&gt;Feryal Behbahani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Faust,+A&quot;&gt;Aleksandra Faust&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.12917&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
</feed>
