<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="http://localhost:4000/tag/research/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2024-08-31T18:54:48+03:00</updated>
  <id>http://localhost:4000/tag/research/feed.xml</id>

  
  
  

  
    <title type="html">Kavour | </title>
  

  
    <subtitle>Data Science and AI News</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model</title>
      <link href="http://localhost:4000/Transfusion" rel="alternate" type="text/html" title="Transfusion - Predict the Next Token and Diffuse Images with One Multi-Modal Model" />
      <published>2024-08-20T00:00:00+03:00</published>
      <updated>2024-08-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/Transfusion</id>
      <content type="html" xml:base="http://localhost:4000/Transfusion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed- modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific en- coding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Omer Levy&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/pdf/2408.11039'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">To Code, or Not To Code? Exploring Impact of Code in Pre-training</title>
      <link href="http://localhost:4000/CodeNoCoode" rel="alternate" type="text/html" title="To Code, or Not To Code? Exploring Impact of Code in Pre-training" />
      <published>2024-08-20T00:00:00+03:00</published>
      <updated>2024-08-20T00:00:00+03:00</updated>
      <id>http://localhost:4000/CodeNoCoode</id>
      <content type="html" xml:base="http://localhost:4000/CodeNoCoode">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs’ performance, there is only limited work analyzing the precise impact of code on non-code tasks. In this work, we system- atically investigate the impact of code data on general performance. We ask “what is the impact of code data used in pre-training on a large variety of downstream tasks beyond code generation”. We conduct extensive ablations and evaluate across a broad range of natural language reasoning tasks, world knowledge tasks, code benchmarks, and LLM-as-a-judge win-rates for models with sizes ranging from 470M to 2.8B parameters. Across settings, we find a consistent results that code is a critical building block for generalization far beyond coding tasks and improvements to code quality have an outsized impact across all tasks. In particular, compared to text-only pre-training, the ad- dition of code results in up to relative increase of 8.2% in natural language (NL) reasoning, 4.2% in world knowledge, 6.6% improvement in generative win-rates, and a 12x boost in code performance respectively. Our work suggests investments in code quality and preserving code during pre-training have positive impacts.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; CViraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker1&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/pdf/2408.10914'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Automated Design of Agentic Systems</title>
      <link href="http://localhost:4000/AutoDesign" rel="alternate" type="text/html" title="Automated Design of Agentic Systems" />
      <published>2024-08-15T00:00:00+03:00</published>
      <updated>2024-08-15T00:00:00+03:00</updated>
      <id>http://localhost:4000/AutoDesign</id>
      <content type="html" xml:base="http://localhost:4000/AutoDesign">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are eventually replaced by learned solutions. We formulate a new research area, Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic system designs, including inventing novel building blocks and/or combining them in new ways. We further demonstrate that there is an unexplored yet promising approach within ADAS where agents can be defined in code and new agents can be automatically discovered by a meta agent programming ever better ones in code. Given that programming languages are Turing Complete, this approach theoretically enables the learning of any possible agentic system: including novel prompts, tool use, control flows, and combinations thereof. We present a simple yet effective algorithm named Meta Agent Search to demonstrate this idea, where a meta agent iteratively programs interesting new agents based on an ever-growing archive of previous discoveries. Through extensive experiments across multiple domains including coding, science, and math, we show that our algorithm can progressively invent agents with novel designs that greatly outperform state-of-the-art hand-designed agents. Importantly, we consistently observe the surprising result that agents invented by Meta Agent Search maintain superior performance even when transferred across domains and models, demonstrating their robustness and generality. Provided we develop it safely, our work illustrates the potential of an exciting new research direction toward automatically designing ever-more powerful agentic systems to benefit humanity.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+S'&gt;Shengran Hu&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+C'&gt;Cong Lu&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Clune,+J'&gt;Jeff Clune&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2408.08435'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</title>
      <link href="http://localhost:4000/HybridRAG" rel="alternate" type="text/html" title="Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" />
      <published>2024-08-06T00:00:00+03:00</published>
      <updated>2024-08-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/HybridRAG</id>
      <content type="html" xml:base="http://localhost:4000/HybridRAG">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&amp;A format, and hence provide a natural set of pairs of ground-truth Q&amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt;&lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Sarmah,+B'&gt;Bhaskarjit Sarmah&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Hall,+B'&gt;Benika Hall&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Rao,+R'&gt;Rohan Rao&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Patel,+S'&gt;Sunil Patel&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Pasquali,+S'&gt;Stefano Pasquali&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Mehta,+D'&gt;Dhagash Mehta&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2408.04948'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</title>
      <link href="http://localhost:4000/TransformerExplainer" rel="alternate" type="text/html" title="Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" />
      <published>2024-08-06T00:00:00+03:00</published>
      <updated>2024-08-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/TransformerExplainer</id>
      <content type="html" xml:base="http://localhost:4000/TransformerExplainer">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user's browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public's education access to modern generative AI techniques. Our open-sourced tool is available at &lt;a href='https://poloclub.github.io/transformer-explainer/'&gt;this https URL&lt;/a&gt;. A video demo is available at &lt;a href='https://youtu.be/ECR4oAwocjs'&gt;this https URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt;&lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Cho,+A'&gt;Aeree Cho&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+G+C'&gt;Grace C. Kim&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Karpekov,+A'&gt;Alexander Karpekov&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Helbling,+A'&gt;Alec Helbling&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z+J'&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+S'&gt;Seongmin Lee&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Hoover,+B'&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Chau,+D+H'&gt;Duen Horng Chau&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2408.04619'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</title>
      <link href="http://localhost:4000/ScallingLLMTest" rel="alternate" type="text/html" title="Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters" />
      <published>2024-08-06T00:00:00+03:00</published>
      <updated>2024-08-06T00:00:00+03:00</updated>
      <id>http://localhost:4000/ScallingLLMTest</id>
      <content type="html" xml:base="http://localhost:4000/ScallingLLMTest">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a &quot;compute-optimal&quot; scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt;&lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Snell,+C'&gt;Charlie Snell&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+J'&gt;Jaehoon Lee, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+K'&gt;Kelvin Xu&lt;/a&gt;, &lt;a href='https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+A'&gt;Aviral Kumar&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2408.03314'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Apple Intelligence Foundation Language Models</title>
      <link href="http://localhost:4000/AppleIntelligenceFoundationLanguageModels" rel="alternate" type="text/html" title="Apple Intelligence Foundation Language Models" />
      <published>2024-07-29T00:00:00+03:00</published>
      <updated>2024-07-29T00:00:00+03:00</updated>
      <id>http://localhost:4000/AppleIntelligenceFoundationLanguageModels</id>
      <content type="html" xml:base="http://localhost:4000/AppleIntelligenceFoundationLanguageModels">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2407.21075'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">KAN or MLP - A Fairer Comparison</title>
      <link href="http://localhost:4000/KANorMLP" rel="alternate" type="text/html" title="KAN or MLP - A Fairer Comparison" />
      <published>2024-07-23T00:00:00+03:00</published>
      <updated>2024-07-23T00:00:00+03:00</updated>
      <id>http://localhost:4000/KANorMLP</id>
      <content type="html" xml:base="http://localhost:4000/KANorMLP">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, we control the number of parameters and FLOPs to compare the performance of KAN and MLP. Our main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. We also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, we find that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. We hope these results provide insights for future research on KAN and other MLP alternatives.&lt;/p&gt;

&lt;p&gt; &lt;a href='https://github.com/yu-rp/KANbeFair'&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2407.16674'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Stretching Each Dollar- Diffusion Training from Scratch on a Micro-Budget</title>
      <link href="http://localhost:4000/DiffusionTrainingfromScratch" rel="alternate" type="text/html" title="Stretching Each Dollar- Diffusion Training from Scratch on a Micro-Budget" />
      <published>2024-07-22T00:00:00+03:00</published>
      <updated>2024-07-22T00:00:00+03:00</updated>
      <id>http://localhost:4000/DiffusionTrainingfromScratch</id>
      <content type="html" xml:base="http://localhost:4000/DiffusionTrainingfromScratch">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to address this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose to randomly mask up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only $1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive FID and high-quality generations while incurring 118× lower cost than stable diffusion models and 14× lower cost than the current state-of-the-art approach that costs $28,400. We aim to release our end-to-end training pipeline to further democratize the training of large-scale diffusion models on micro-budgets.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2407.15811'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Shape of Motion - 4D Reconstruction from a Single Video</title>
      <link href="http://localhost:4000/ShapeOfMotion" rel="alternate" type="text/html" title="Shape of Motion - 4D Reconstruction from a Single Video" />
      <published>2024-07-18T00:00:00+03:00</published>
      <updated>2024-07-18T00:00:00+03:00</updated>
      <id>http://localhost:4000/ShapeOfMotion</id>
      <content type="html" xml:base="http://localhost:4000/ShapeOfMotion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches are limited in that they either depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. In this work, we introduce a method capable of reconstructing generic dynamic scenes, featuring explicit, full-sequence-long 3D motion, from casually captured monocular videos. We tackle the under-constrained nature of the problem with two key insights: First, we exploit the low-dimensional structure of 3D motion by representing scene motion with a compact set of SE3 motion bases. Each point's motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we utilize a comprehensive set of data-driven priors, including monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes.&lt;/p&gt;

&lt;p&gt; &lt;a href='https://shape-of-motion.github.io/'&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2407.13764'&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kavour</name>
        
        
      </author>

      

      
        <category term="research" />
      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
</feed>
