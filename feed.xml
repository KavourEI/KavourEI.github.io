<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-08-08T14:09:31+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">LangGraph Studio - The first agent IDE</title><link href="http://localhost:4000/LanggraphStudio" rel="alternate" type="text/html" title="LangGraph Studio - The first agent IDE" /><published>2024-08-01T00:00:00+03:00</published><updated>2024-08-01T00:00:00+03:00</updated><id>http://localhost:4000/LanggraphStudio</id><content type="html" xml:base="http://localhost:4000/LanggraphStudio">&lt;p&gt;LangGraph Studio provides a specialized agent IDE for visualizing, interacting with, and debugging complex agentic applications.&lt;/p&gt;

&lt;p&gt; LangChain has unveiled LangGraph Studio, a revolutionary Integrated Development Environment (IDE) designed specifically for AI agents. As the first of its kind, LangGraph Studio offers a powerful platform that allows developers to build, test, and deploy AI agents with unprecedented ease and efficiency. This blog post explores the key features and capabilities of LangGraph Studio, and how it is set to transform the way AI agents are developed.&lt;/p&gt;

&lt;h3&gt;The Need for a Dedicated Agent IDE&lt;/h3&gt;

&lt;p&gt; AI agents—autonomous systems that perform tasks based on user inputs—are becoming increasingly complex and integral to modern applications. Despite their growing importance, there has been a lack of specialized tools tailored to the unique challenges of developing these agents. Traditional development environments are often not optimized for the iterative, experimental, and highly interactive process required to create sophisticated AI agents to cover each and everyone's needs and ideas.&lt;/p&gt;

&lt;p&gt; Recognizing this gap, LangChain has introduced LangGraph Studio, the first IDE designed specifically to support the end-to-end development of AI agents. This tool provides developers with everything they need to streamline the process of creating, refining, and deploying AI agents, making it easier to bring innovative AI solutions to life.&lt;/p&gt;

&lt;iframe width=&quot;933&quot; height=&quot;525&quot; src=&quot;https://www.youtube.com/embed/pLPJoFvq4_M&quot; title=&quot;LangGraph Studio: The first agent IDE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;Key Features of LangGraph Studio&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt; Visual Workflow Editor&lt;/strong&gt;: LangGraph Studio features an intuitive visual editor that allows developers to design and manage complex agent workflows with ease. By dragging and dropping components, developers can quickly build sophisticated agent logic without writing extensive code.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt; Integrated Debugging Tools&lt;/strong&gt;: Debugging AI agents can be challenging, especially when dealing with intricate decision-making processes. LangGraph Studio addresses this by providing integrated debugging tools that offer real-time insights into how agents are processing inputs and making decisions. This allows developers to identify and fix issues more efficiently.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt; Support for Iterative Development&lt;/strong&gt;: AI agent development often requires multiple iterations to fine-tune behavior and improve performance. LangGraph Studio is built with this in mind, enabling developers to rapidly prototype, test, and refine agents within the same environment.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt; Seamless Deployment&lt;/strong&gt;: Once an AI agent is ready, LangGraph Studio simplifies the deployment process. With built-in tools for packaging and deploying agents to various environments, developers can move from development to production quickly and with confidence.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt; Extensibility and Integration&lt;/strong&gt;: LangGraph Studio is designed to be extensible, allowing developers to integrate their existing tools and libraries. Whether working with custom data sources, leveraging external APIs, or incorporating additional AI models, LangGraph Studio can accommodate diverse development needs.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt; Collaboration Features&lt;/strong&gt;: Recognizing that AI development is often a team effort, LangGraph Studio includes features that facilitate collaboration. Teams can share projects, review changes, and work together in real-time, enhancing productivity and innovation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Potential Impact of LangGraph Studio&lt;/h3&gt;

&lt;p&gt;LangGraph Studio is set to have a profound impact on the AI development landscape. By providing a specialized IDE for AI agents, LangChain is enabling developers to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt; &lt;strong&gt; Accelerate Development Cycles&lt;/strong&gt;: With tools that streamline every aspect of AI agent development, LangGraph Studio allows developers to bring products to market faster.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt; Improve Agent Quality&lt;/strong&gt;: The integrated debugging and testing tools help ensure that AI agents perform reliably and as intended, reducing the likelihood of errors in production.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt; Enhance Collaboration&lt;/strong&gt;: The collaboration features make it easier for teams to work together on complex projects, leading to more innovative and polished AI solutions.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt; Expand AI Capabilities&lt;/strong&gt;: By making it easier to develop and deploy AI agents, LangGraph Studio opens the door to new and more sophisticated AI applications across various industries.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt; Getting started with Lagngraph Studio&lt;/h3&gt;

&lt;p&gt;LangGraph Studio is a desktop app, currently available for Apple Silicon. You can download a version &lt;a href=&quot;https://github.com/langchain-ai/langgraph-studio?ref=blog.langchain.dev&quot;&gt;here&lt;/a&gt;. Support for more platforms is coming soon.&lt;/p&gt;

&lt;p&gt;After you download and open LangGraph Studio, you will be prompted to log in with your LangSmith account. All users of LangSmith (including those with free accounts) currently have access to LangGraph Studio while it is in beta. You can sign up for a LangSmith account &lt;a href=&quot;https://smith.langchain.com/?ref=blog.langchain.dev&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After downloading LangSmith, you can open a directory. At a bare minimum, this directory needs to contain a Python file with a graph defined in it.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;LangGraph Studio represents a significant advancement in the field of AI development, offering the first dedicated IDE for AI agents. With its comprehensive set of features, from visual workflow editing to integrated debugging and seamless deployment, LangGraph Studio empowers developers to create, refine, and deploy AI agents more efficiently than ever before. As AI continues to evolve and integrate into more aspects of our lives, tools like LangGraph Studio will be essential for driving innovation and ensuring the reliability of AI-driven solutions.&lt;/p&gt;

&lt;p&gt; To find out more detailed information about LangGraph Studio, its capabilities and the ways you can integrate all its possibilities to your workflow, you can explore LangChain’s official announcement &lt;a href=&quot;https://blog.langchain.dev/langgraph-studio-the-first-agent-ide/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">LangGraph Studio provides a specialized agent IDE for visualizing, interacting with, and debugging complex agentic applications.</summary></entry><entry><title type="html">Introducing the Galileo Hallucination Index - A New Benchmark for AI Accuracy</title><link href="http://localhost:4000/HallucinationIndex" rel="alternate" type="text/html" title="Introducing the Galileo Hallucination Index - A New Benchmark for AI Accuracy" /><published>2024-07-31T00:00:00+03:00</published><updated>2024-07-31T00:00:00+03:00</updated><id>http://localhost:4000/HallucinationIndex</id><content type="html" xml:base="http://localhost:4000/HallucinationIndex">&lt;p&gt; Hallucination is a huge issue in the field of artificial intelligence. The accuracy and reliability of AI-generated content has become increasingly important in the last few years since people tend to rely on AI more and more in an exponential pace. Galileo, a pioneering platform in AI model evaluation, has introduced the Hallucination Index, a groundbreaking tool designed to measure and mitigate AI hallucinations. This blog post explores the Hallucination Index.&lt;/p&gt;

&lt;h3&gt;First of all for those that are not confident with the term, what is hallucination in the context of AI?&lt;/h3&gt;

&lt;p&gt;AI hallucinations refer to instances where an AI model generates content that is incorrect, misleading, or entirely fabricated, despite presenting it with confidence. These errors can undermine the credibility and effectiveness of AI systems, especially in critical applications like healthcare, finance, and law. Addressing this challenge is essential for building trust in AI technologies.&lt;/p&gt;

&lt;h3&gt; Let's take a step forward, what is the Galileo Hallucination Index?&lt;/h3&gt;

&lt;p&gt; The Galileo Hallucination Index is a new tool designed to quantify and analyze the frequency and severity of hallucinations in AI models. By providing a clear and standardized metric, the Hallucination Index enables developers and researchers to better understand where and why these errors occur, allowing for more targeted improvements in AI models. You can take a look at the results in the Gallileo's official blog post. You may end up surprised by the results you will discover there.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt; The Hallucination Index is a significant advancement in the field of AI, offering a robust tool for measuring and mitigating one of the most challenging issues in AI development. By providing a clear, quantitative measure of AI hallucinations, Galileo is empowering developers and researchers to build more reliable and trustworthy AI systems. As AI continues to integrate into critical aspects of our lives, tools like the Hallucination Index will be essential for ensuring that these technologies are both accurate and responsible.&lt;/p&gt;

&lt;p&gt;To find out more, check more detailed results and model comparisons or just check it out yourself take a look at &lt;a href=&quot;https://www.rungalileo.io/hallucinationindex#scr&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Hallucination is a huge issue in the field of artificial intelligence. The accuracy and reliability of AI-generated content has become increasingly important in the last few years since people tend to rely on AI more and more in an exponential pace. Galileo, a pioneering platform in AI model evaluation, has introduced the Hallucination Index, a groundbreaking tool designed to measure and mitigate AI hallucinations. This blog post explores the Hallucination Index.</summary></entry><entry><title type="html">RAG - Query Transformation</title><link href="http://localhost:4000/rag2" rel="alternate" type="text/html" title="RAG - Query Transformation" /><published>2024-07-29T00:00:00+03:00</published><updated>2024-07-29T00:00:00+03:00</updated><id>http://localhost:4000/rag2</id><content type="html" xml:base="http://localhost:4000/rag2">&lt;p&gt; Welcome back, I hope you enjoyed the &lt;a href=&quot;https://kavourei.github.io/rag1&quot;&gt;first part&lt;/a&gt; of this series where we are going to explore a portion portion of RAG tool. It is higly suggested that you take a look at all the projects of this series step by step and more importantly to code along this project. If you don't code it out you won't get it.&lt;/p&gt;

&lt;p&gt; In this notebook we are going to take a look how to create an assistant that will help us modify our question. This technique is called Query transformation. Imagine Query Transformation as your search request going through a makeover montage in a movie. Your original query walks in a bit plain and straightforward, and then gets spruced up with the latest styles and smarts to become the most efficient, dashing version of itself. By rephrasing, optimizing, and enhancing your search terms, Query Transformation ensures that what you’re asking for is crystal clear and ready to fetch the best possible results. It’s like sending your query to a high-end stylist who makes sure it’s dressed to impress and ready to get exactly what you need! &lt;/p&gt;

&lt;p&gt; It is taken for granted that you already have created a &lt;code&gt;.env&lt;/code&gt; file as requested in the first part of this series.&lt;/p&gt;

&lt;h2&gt; Modules Required &lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;import os
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import AzureChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import AzureOpenAIEmbeddings
from langchain.load import loads, dumps
from typing import List

load_dotenv()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; The &lt;i&gt;&quot;new&quot;&lt;/i&gt; modules we are going to use in this section are &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;langchain.prompts.ChatPromptTemplate&lt;/strong&gt;: &lt;code&gt;ChatPromptTemplate&lt;/code&gt; is used to create templates for chat prompts. Using these templates can be used to standardize and structure the prompts that are fed into the language model, ensuring consistency and improving the quality of the generated responses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;langchain.load.loads&lt;/strong&gt;: &lt;code&gt;loads&lt;/code&gt; function is used to load data or configurations from a serialized format (such as JSON or YAML).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;langchain.load.dumps&lt;/strong&gt;: &lt;code&gt;dumps&lt;/code&gt; function in the langchain library is used for serializing data or configurations into a specific format (like JSON or YAML).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;typing.List&lt;/strong&gt;: &lt;code&gt;List&lt;/code&gt; class from this module is used to indicate that a variable is expected to be a list of a certain type.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; The main ingedient in our recipy in this series is always an LLM agent, which will assist us. As a result, the first step that will take us closer to this result, is to call our LLM model&lt;/p&gt;

&lt;h2&gt; LLM Agent and its Prompt &lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;llm_075 = AzureChatOpenAI(deployment_name=os.getenv('LLM_35_Deployment'),
                         model_name=os.getenv('LLM_35_Model'),
                         azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
                         temperature=0.75,
                         )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; In this call we have set the &lt;code&gt;temperature&lt;/code&gt; argument to be 0.75 (mind that it is an argument that takes value from the close interval [0,1]). But what this argument represent, right? Imagine the temperature argument in an LLM call as the spice level in a recipe. When you set the temperature low, it’s like adding just a pinch of spice, making the responses mild, predictable, and focused. Crank up the temperature, and it’s like dumping in hot sauce, making the responses more adventurous, creative, and sometimes a bit unpredictable. So, adjusting the temperature lets you control how bold or conservative the language model’s answers will be, ensuring your conversational dish is seasoned just to your taste! Since we need it to generate new questions similar, yet better formated to ours we need to use this spiciness.&lt;/p&gt;

&lt;p&gt; Next we are going to define a prompt. During the first part of this series we used a prompt from the self, where we requested it using the hub module. Now we are going to create our own. The reason for that is to give to our agent &quot;personality&quot; or better purpose for its existance.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prompt = ChatPromptTemplate.from_template(
    &quot;&quot;&quot;
    You are a smart assistant. Your task is to create 5 questions, each phrased differently and from various perspectives, based on the given question, to retrieve relevant documents from a vector database. By offering multiple perspectives on the user's question, your aim is to help the user mitigate some of the constraints of distance-based similarity search. List these alternative questions, each on a new line. Original question: {question}
    &quot;&quot;&quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt; Chain Construction &lt;h2&gt;

&lt;p&gt; Finally, we are ready to create our very first chain, in this section. Initially, we need to pass our question in the chain. Then we insert it to the prompt in th &lt;code&gt; {question} &lt;/code&gt; place. After having our prompt ready-to-go, we &quot;send&quot; it to our LLM agent. Lastly, using &lt;code&gt;StrOutputParser()&lt;/code&gt; and &lt;code&gt;(lambda x: &quot;\n&quot;.join(x.split(&quot;\n&quot;)))&lt;/code&gt; we exporet the results in a readable and nice format for us humans! 🤖 In general before setting up a chain it is suggested to think your steps one by one as simple as possible, define your functions/tools (if needed) and then set it up. Do not start from defining everything, as lated on you will miss something or will need to modify them as you need those steps to be &quot;connected&quot; somehow.&lt;/p&gt;

&lt;p&gt; By connected, I mean the &lt;code&gt; {question} &lt;/code&gt; part in the prompt building, or in other cases more information like &lt;code&gt; {context} &lt;/code&gt; etc. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
generate_queries = (
        {&quot;question&quot;: RunnablePassthrough()}
        | prompt
        | llm_075
        | StrOutputParser()
        | (lambda x: &quot;\n&quot;.join(x.split(&quot;\n&quot;)))
)

result = generate_queries.invoke(&quot;What do you know about Query Transforamtion?&quot;)
print(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt; Fusion Scores and uniqueness &lt;/h2&gt;

&lt;p&gt;Since we have generated our &lt;i&gt;better-formatted&lt;/i&gt; question, we need to use them to get better-more relevant answers to our questions. There are many options, that you could experiment with. Here we are going to explore &lt;a href=&quot;https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking&quot;&gt;RAG-Fusion&lt;/a&gt;. Through this process, relevant information for each question is retrieved. A union of the retrievals is created that keeps only the unique of them. Finally, a rank is measures and depending on our preference an answer is presented.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def rrf(results: List[List], k=60):
    fused_scores = {}
    for docs in results:        
        for rank, doc in enumerate(docs):
            doc_str = dumps(doc)
            if doc_str not in fused_scores:
                fused_scores[doc_str] = 0
            previous_score = fused_scores[doc_str]
            fused_scores[doc_str] += 1 / (rank + k)
        reranked_results = [
            (loads(doc), score)
            for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
        ]        
        return reranked_results
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Initially a dictionary is created to save the fused scores for each unique document retrieved (&lt;code&gt;fused_scores&lt;/code&gt;). After that we iterate through each ranked document, and each document depending on its rank. We then create a key for each document. Either we add the document in the list (if it does not exists) or retrieve its score. Based on the question, we update the score using the provided RRF formula &lt;code&gt;1/(rank + k)&lt;/code&gt;. Finally, we return the list of containing each document and its fused score in the format of tuples.&lt;/p&gt;

&lt;p&gt; After having this tool defined and set up in our toolbox, we can either us the previously defined prompt, or create a new one to continue with. Just for practise we are going to set up a new one where we are going to generate 3 questions, and not 5 as we did in the previously created prompth.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;prompt = ChatPromptTemplate.from_template(
    &quot;&quot;&quot;
    You are a smart assistant. Your task is to create 3 questions, each phrased differently and from various perspectives, based on the provided question, to retrieve relevant documents from a vector database. By offering multiple perspectives on the user's question, your aim is to help the user address some of the limitations of distance-based similarity search. List these alternative questions, each on a new line. Original question: {question}
    &quot;&quot;&quot;
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; So we have our LLM agent ready-to-go, its purpose of existance set, a new tool to generate the scores and retrieve the most relevant information. So you may wonder what is left. We now are ready to define a new chain to include everything we did.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;generate_queries = (
        {&quot;question&quot;: RunnablePassthrough()}
        | prompt
        | llm_075
        | StrOutputParser()
        | (lambda x: x.split(&quot;\n&quot;))
)

fusion_retrieval_chain = (
        {'question': RunnablePassthrough()}
        | generate_queries
        | retriever.map()
        | rrf
)

fusion_retrieval_chain.invoke(&quot;What are the benefits of Bert?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt; Food for thought &lt;/h2&gt;

&lt;p&gt; There are multiple other ways that you could modify the purpose of the LLM agent, your assistance, so that it could help you the way you want. For example, imagine that you have a complex question, where you are not sure how to provide/invoke it to your shiny and brand new chain. Or,  when providing it to your chain the results, you are getting back are not satisfying. Giving your agent a new slightly modified purpose, everything will be again bright and shiny. For example, you could use within your chain the following prompt:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;decompostion_prompt = ChatPromptTemplate.from_template(
    &quot;&quot;&quot;
    You are a helpful assistant capable of simplifying complex questions into smaller parts.
    Your goal is to break down the given question into several sub-questions that can each be answered separately, ultimately addressing the main question.
    List these sub-questions, each separated by a newline character.
    Original question: {question}
    Output (3 queries):
    &quot;&quot;&quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Feeding this prompt to a new chain could save you from some time and simplify your question to reach to your goal, step by step.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;query_generation_chain = (
        {&quot;question&quot;: RunnablePassthrough()}
        | decompostion_prompt
        | llm_075
        | StrOutputParser()
        | (lambda x: x.split(&quot;\n&quot;))
)

questions = query_generation_chain.invoke(&quot;What are the benefits of LDA?&quot;)
questions&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; 🚀 Imagine the possibilities and think about what you could do by defining a proper prompt based on you needs. 🚀 I hope you enjoyed this as much as I did and of course, learned something from it! In the next post of this series, we will explore Hypothetical Document Embeddings. We'll create our own documents, allowing their embedding vectors to identify neighborhoods in the corpus embedding space where similar real documents can be retrieved based on vector similarity.&lt;/p&gt; 

&lt;p&gt; &lt;i&gt;Be safe, code safer!&lt;/i&gt;&lt;/p&gt;
&lt;/h2&gt;&lt;/h2&gt;</content><author><name>Kavour</name></author><category term="project" /><category term="Deep Learning" /><summary type="html">Welcome back, I hope you enjoyed the first part of this series where we are going to explore a portion portion of RAG tool. It is higly suggested that you take a look at all the projects of this series step by step and more importantly to code along this project. If you don't code it out you won't get it.</summary></entry><entry><title type="html">Meta AI Introduces Segment Anything 2.0 - Revolutionizing Image and Video Segmentation</title><link href="http://localhost:4000/MetaSam2" rel="alternate" type="text/html" title="Meta AI Introduces Segment Anything 2.0 - Revolutionizing Image and Video Segmentation" /><published>2024-07-29T00:00:00+03:00</published><updated>2024-07-29T00:00:00+03:00</updated><id>http://localhost:4000/MetaSam2</id><content type="html" xml:base="http://localhost:4000/MetaSam2">&lt;p&gt;Meta AI has once again pushed the boundaries of artificial intelligence with the release of Segment Anything 2.0 or as it is published SAM2 (Segment Anything Model). This latest iteration in image segmentation technology promises to redefine how we interact with and analyze visual data. In this blog post, we shall explore the capabilities of Segment Anything 2.0, its innovative features, and its potential impact across various industries.&lt;/p&gt;

&lt;p&gt;
&lt;video width=&quot;640&quot; height=&quot;360&quot; controls=&quot;&quot;&gt;
        &lt;source src=&quot;https://video.fskg4-1.fna.fbcdn.net/o1/v/t2/f2/m69/An90OOU7fbi7bvqEA7w4w8jjrjlXuNSMvlHN5J7M1TjlxXchTVHBxhEQQ93goUvnP27BuJgLDN9g5CJDxcg5wCFX.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6Im9lcF9oZCJ9&amp;amp;_nc_ht=video.fskg4-1.fna.fbcdn.net&amp;amp;_nc_cat=110&amp;amp;strext=1&amp;amp;vs=35ac8a12e58bf6bb&amp;amp;_nc_vs=HBkcFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HRWQyQUJ0NmpmRXNQS1FEQU8yUUhHTXZfLXB2Ym1kakFBQUYVAALIAQBLB4gScHJvZ3Jlc3NpdmVfcmVjaXBlATENc3Vic2FtcGxlX2ZwcwAQdm1hZl9lbmFibGVfbnN1YgAgbWVhc3VyZV9vcmlnaW5hbF9yZXNvbHV0aW9uX3NzaW0AKGNvbXB1dGVfc3NpbV9vbmx5X2F0X29yaWdpbmFsX3Jlc29sdXRpb24AHXVzZV9sYW5jem9zX2Zvcl92cW1fdXBzY2FsaW5nABFkaXNhYmxlX3Bvc3RfcHZxcwAVACUAHIwXQAAAAAAAAAAREQAAACaywtvMn6OyDRUCKAJDMxgLdnRzX3ByZXZpZXccF0AjR64UeuFIGBlkYXNoX2gyNjQtYmFzaWMtZ2VuMl83MjBwEgAYGHZpZGVvcy52dHMuY2FsbGJhY2sucHJvZDgSVklERU9fVklFV19SRVFVRVNUGwqIFW9lbV90YXJnZXRfZW5jb2RlX3RhZwZvZXBfaGQTb2VtX3JlcXVlc3RfdGltZV9tcwEwDG9lbV9jZmdfcnVsZQd1bm11dGVkE29lbV9yb2lfcmVhY2hfY291bnQDOTk2EW9lbV9pc19leHBlcmltZW50AAxvZW1fdmlkZW9faWQPODIyMTQxOTcwMDQzNTkzEm9lbV92aWRlb19hc3NldF9pZA84NzExOTUxNjgzOTA4NTEVb2VtX3ZpZGVvX3Jlc291cmNlX2lkEDM3Njk3MzEzOTY2Mjg2MzMcb2VtX3NvdXJjZV92aWRlb19lbmNvZGluZ19pZA85OTcwNjA2MTIwOTQ2ODIOdnRzX3JlcXVlc3RfaWQAJQIcACW%2BARsHiAFzBDg3ODACY2QKMjAyNC0wNy0yNgNyY2IDOTAwA2FwcAVWaWRlbwJjdBFDTVNfTUVESUFfTUFOQUdFUhNvcmlnaW5hbF9kdXJhdGlvbl9zAzkuNgJ0cxVwcm9ncmVzc2l2ZV9lbmNvZGluZ3MA&amp;amp;ccb=9-4&amp;amp;oh=00_AYBy7rOIBugkFQyPc4SnObt7UhEg1WR6dSrgAwDqNRmSbA&amp;amp;oe=66B52284&amp;amp;_nc_sid=1d576d&amp;amp;_nc_rid=373586427540719&amp;amp;_nc_store_type=1&quot; type=&quot;video/mp4&quot; /&gt;
        Your browser does not support the video tag.
    &lt;/video&gt;
&lt;/p&gt;

&lt;h3&gt;The Evolution of Image Segmentation&lt;/h3&gt;

&lt;p&gt;Image segmentation, the process of partitioning a digital image into multiple segments to simplify or change its representation, is a fundamental task in computer vision. It has applications ranging from medical imaging to autonomous driving. Traditional methods have often struggled with accuracy and efficiency, especially in complex scenarios. SAM2, however, leverages advanced AI to overcome these challenges and deliver state-of-the-art performance.&lt;/p&gt;

&lt;p&gt;As Mark Zuckerberg &lt;a href=&quot;https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/&quot;&gt;noted&lt;/a&gt; in an open letter last week, open source AI “has more potential than any other modern technology to increase human productivity, creativity, and quality of life,” all while accelerating economic growth and advancing groundbreaking medical and scientific research. We’ve been tremendously impressed by the progress the AI community has made using SAM, and we envisage SAM 2 unlocking even more exciting possibilities.&lt;/p&gt;

&lt;h3&gt;What is Segment Anything 2.0?&lt;/h3&gt;

&lt;p&gt; Segment Anything 2.0 is Meta AI's cutting-edge image segmentation model. Building on the success of its predecessor, this version incorporates enhanced algorithms and machine learning techniques to provide more precise and versatile segmentation capabilities. It is designed to be highly adaptable, capable of handling a wide variety of image types and segmentation tasks with remarkable accuracy.&lt;/p&gt;

&lt;h3&gt;Key Features of Segment Anything 2.0&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt; &lt;strong&gt;Unprecedented Accuracy&lt;/strong&gt;: SAM 2.0 offers significant improvements in segmentation accuracy. It can delineate objects and regions within images with exceptional precision, even in complex and cluttered scenes.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Versatile Application&lt;/strong&gt;: The model is designed to be highly flexible, making it suitable for a broad range of applications, from medical imaging and satellite imagery to everyday photos and videos.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Efficient Processing&lt;/strong&gt;: With optimized algorithms, SAM2.0 provides fast and efficient image segmentation, enabling real-time applications and reducing processing times.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;User-Friendly Interface&lt;/strong&gt;: Meta has ensured that SAM 2.0 is accessible to both developers and end-users, with intuitive tools and seamless integration options.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The advancements brought by SAM 2.0 open up numerous possibilities across various sectors:&lt;/p&gt;

&lt;p&gt;&amp;lt;/ul&amp;gt;&lt;/p&gt;
&lt;li&gt; &lt;strong&gt;Medical Imaging&lt;/strong&gt;: Healthcare professionals can use the model to segment and analyze medical images with greater accuracy, aiding in diagnostics and treatment planning.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Autonomous Vehicles&lt;/strong&gt;: Enhanced segmentation capabilities can improve object detection and scene understanding, contributing to safer and more reliable autonomous driving systems.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Agriculture&lt;/strong&gt;: Farmers can utilize the technology to analyze aerial images of crops, identifying areas that require attention and optimizing resource use.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Content Creation&lt;/strong&gt;: Graphic designers and content creators can benefit from precise image segmentation tools for photo and video editing, enabling more creative and efficient workflows.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Surveillance and Security&lt;/strong&gt;: Enhanced image segmentation can improve the accuracy of surveillance systems in identifying and tracking objects and individuals.&lt;/li&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Segment Anything 2.0 represents a significant leap forward in the field of image segmentation, offering unprecedented accuracy, versatility, and efficiency. By harnessing the power of this cutting-edge model, various industries can unlock new opportunities and efficiencies, paving the way for innovative applications and solutions.&lt;/p&gt;

&lt;p&gt;For more detailed information about Segment Anything 2.0 and its potential impact, visit Meta AI's official announcement &lt;a href=&quot;https://ai.meta.com/blog/segment-anything-2/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Meta AI has once again pushed the boundaries of artificial intelligence with the release of Segment Anything 2.0 or as it is published SAM2 (Segment Anything Model). This latest iteration in image segmentation technology promises to redefine how we interact with and analyze visual data. In this blog post, we shall explore the capabilities of Segment Anything 2.0, its innovative features, and its potential impact across various industries.</summary></entry><entry><title type="html">AI achieves silver medal solving International Mathematical Olympiad problems</title><link href="http://localhost:4000/AISilverMedal" rel="alternate" type="text/html" title="AI achieves silver medal solving International Mathematical Olympiad problems" /><published>2024-07-25T00:00:00+03:00</published><updated>2024-07-25T00:00:00+03:00</updated><id>http://localhost:4000/AISilverMedal</id><content type="html" xml:base="http://localhost:4000/AISilverMedal">&lt;h2&gt;AI Achieves Silver Medal Level in International Math Olympiad Problems: A Milestone in Computational Intelligence&lt;/h2&gt;

&lt;p&gt;Artificial Intelligence continues to make impressive strides across various domains, from healthcare to transportation, and now it has marked a significant achievement in the field of mathematics. DeepMind, a leading AI research company, recently published a blog post detailing how their AI has solved International Mathematical Olympiad (IMO) problems at a silver medal level. This breakthrough highlights not only the advancement of AI capabilities but also its potential to aid in solving complex mathematical problems that challenge even the brightest human minds.&lt;/p&gt;

&lt;p&gt;&quot;The fact that the program can come up with a non-obvious construction like this is very impressive, and well beyond what I thought was state of the art.&quot; prof Sir Timothy Gowers, (IMO Gold Medalist and Fields Medal Winner) says&lt;/p&gt;

&lt;h3&gt;The Significance of the IMO&lt;/h3&gt;

&lt;p&gt;The International Mathematical Olympiad is one of the most prestigious mathematical competitions for pre-university students worldwide. Since its inception in 1959, it has been a platform where young mathematicians showcase their problem-solving skills, often presenting problems that require deep logical reasoning, creativity, and advanced mathematical knowledge. Achieving a medal in the IMO is a testament to a participant's exceptional mathematical abilities.&lt;/p&gt;

&lt;h3&gt;DeepMind's Approach&lt;/h3&gt;

&lt;p&gt;DeepMind's AI system, referred to as AlphaMath, employs a combination of machine learning techniques and advanced problem-solving algorithms. The system was trained on a vast dataset of mathematical problems, including those from previous IMO competitions. The AI uses pattern recognition and symbolic reasoning to tackle problems, mimicking the intuitive and step-by-step approach that human mathematicians use. Here are the results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;assets/images/AIMathematicsCompSIlver.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Achieving the Silver Medal Level&lt;/h3&gt;

&lt;p&gt;The AI was tested on a set of IMO problems and performed at a level equivalent to a silver medalist. This accomplishment is particularly notable because IMO problems are known for their complexity and often require innovative solutions rather than straightforward application of known formulas or methods. Achieving a silver medal level means that the AI can solve problems that typically only the top 25% of human competitors can solve, underscoring its advanced problem-solving abilities.&lt;/p&gt;

&lt;h&gt;Implications for the Future&amp;lt;/h3&amp;gt;

&lt;p&gt;&lt;ol&gt;
&lt;li&gt; &lt;strong&gt;Educational Tools&lt;/strong&gt;: AI like AlphaMath can be used to develop educational tools that help students learn complex mathematical concepts by providing step-by-step solutions and explanations for difficult problems.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Research and Development&lt;/strong&gt;: Mathematicians and scientists can leverage AI to explore new areas of research, automate tedious calculations, and generate hypotheses for further investigation.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Cross-Disciplinary Applications&lt;/strong&gt;: The techniques developed for solving mathematical problems can be adapted to other fields requiring complex problem-solving, such as physics, engineering, and computer science.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Human-AI Collaboration&lt;/strong&gt;: This breakthrough opens up possibilities for enhanced collaboration between humans and AI, where AI can assist in verifying solutions, exploring multiple problem-solving approaches, and providing insights that may not be immediately obvious to human researchers.&lt;/li&gt;&amp;lt;/p&amp;gt;

&lt;h3&gt;Ethical Considerations&lt;/h3&gt;

&lt;p&gt;As AI continues to evolve and take on more complex tasks, it is crucial to address ethical considerations. Ensuring transparency in AI decision-making processes, maintaining data privacy, and preventing biases in AI algorithms are essential to fostering trust and reliability in AI applications.&lt;/p&gt;

&lt;p&gt;DeepMind's achievement in having its AI solve IMO problems at a silver medal level marks a significant milestone in the field of AI and mathematics. This development not only demonstrates the growing capabilities of AI but also opens up new avenues for educational advancement, research innovation, and human-AI collaboration. As we continue to push the boundaries of what AI can achieve, it is important to approach these advancements with a sense of responsibility and an eye towards the ethical implications.&lt;/p&gt;

&lt;p&gt;For more detailed insights and technical explanations, you can read DeepMind's full blog post &lt;a href=&quot;https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/&quot;&gt; here &lt;/a&gt;&lt;/p&gt;
&lt;/ol&gt;&lt;/p&gt;&lt;/h&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">AI Achieves Silver Medal Level in International Math Olympiad Problems: A Milestone in Computational Intelligence</summary></entry><entry><title type="html">Llama 3.1 - Most capable model to date</title><link href="http://localhost:4000/lama3_1" rel="alternate" type="text/html" title="Llama 3.1 - Most capable model to date" /><published>2024-07-25T00:00:00+03:00</published><updated>2024-07-25T00:00:00+03:00</updated><id>http://localhost:4000/lama3_1</id><content type="html" xml:base="http://localhost:4000/lama3_1">&lt;p&gt;The recent release of Meta's Llama 3.1 marks a significant advancement in the field of open-source large language models (LLMs). As the first openly available model to rival top proprietary models, Llama 3.1 is set to redefine capabilities in AI, offering a range of features that enhance its usability and performance. Some of the key features are the following: &lt;/p&gt;

&lt;h3&gt;Unprecedented Scale and Capability&lt;/h3&gt;

&lt;p&gt;Llama 3.1, with its 405 billion parameters, is touted as the world's largest openly available foundation model. It has been trained on over 15 trillion tokens, ensuring that it can perform exceptionally well across various tasks, including general knowledge, multilingual translation, and advanced reasoning. This model is not only about size; it also incorporates significant improvements in context length, now supporting up to 128K tokens, which allows for more complex interactions and applications.&lt;/p&gt;

&lt;h3&gt;Enhanced Multilingual and Tool Use&lt;/h3&gt;

&lt;p&gt;The upgraded versions of the 8B and 70B models now feature enhanced multilingual capabilities and improved tool use. These models are designed to support advanced applications such as long-form text summarization and coding assistance, making them versatile tools for developers and researchers alike. The model's ability to handle diverse languages and tasks positions it as a leading choice for global applications.&lt;/p&gt;

&lt;h3&gt;Open Source Commitment&lt;/h3&gt;

&lt;p&gt;Meta's commitment to open-source principles is evident in the new licensing changes that allow developers to utilize outputs from Llama models to improve their own models. This openness encourages innovation and collaboration within the AI community, enabling developers to customize the models for specific needs without the constraints typically associated with proprietary models.&lt;/p&gt;

&lt;p&gt;Apart from the innovations regarging this newlly introduced model, there has been some key innovations in training and evaluation&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Rigorous Benchmarking&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;Llama 3.1 has undergone extensive evaluation across over 150 benchmark datasets, demonstrating competitive performance against leading models such as GPT-4 and Claude 3.5 Sonnet. The evaluation included both automated assessments and human evaluations, ensuring that the model meets high standards of quality and reliability in real-world scenarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;li&gt;Advanced Training Techniques&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;The training of Llama 3.1 involved significant optimizations to the training stack, utilizing over 16,000 H100 GPUs. This large-scale training effort has allowed for improvements in both the quantity and quality of the training data. The model benefits from a rigorous quality assurance process, which enhances its overall performance and reliability.&lt;/li&gt;
&lt;/ul&gt;
&lt;li&gt;Community and Ecosystem Development&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;Meta is not only focused on the model itself but also on fostering a robust ecosystem around Llama. The introduction of the &quot;Llama Stack,&quot; a set of standardized interfaces for building AI applications, aims to facilitate interoperability among developers. This initiative encourages collaboration and innovation, allowing developers to create custom solutions that leverage the strengths of the Llama models.&lt;/li&gt;
&lt;/ul&gt;
&lt;li&gt;Safety and Ethical Considerations&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;In line with responsible AI development, Meta has implemented various safety measures, including extensive red teaming to identify and mitigate potential risks. The inclusion of safety models like Llama Guard 3 and Prompt Guard further enhances the security and reliability of applications built on Llama 3.1, ensuring that developers can deploy AI solutions with confidence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The launch of Llama 3.1 represents a pivotal moment in the landscape of AI, particularly for open-source models. With its unmatched scale, advanced capabilities, and commitment to community collaboration, Llama 3.1 is poised to drive innovation and expand the possibilities of generative AI. As developers begin to explore its potential, the future of AI looks brighter than ever, fueled by the power of open-source collaboration and cutting-edge technology.&lt;/p&gt;

&lt;p&gt;To further explore the features, potentials and more read the official announcement of &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-3-1/&quot;&gt;meta's blog&lt;/a&gt;.&lt;/p&gt;
&lt;/ol&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">The recent release of Meta's Llama 3.1 marks a significant advancement in the field of open-source large language models (LLMs). As the first openly available model to rival top proprietary models, Llama 3.1 is set to redefine capabilities in AI, offering a range of features that enhance its usability and performance. Some of the key features are the following:</summary></entry><entry><title type="html">Mistral Unveils Mistral 7B - A Cutting-Edge Language Model</title><link href="http://localhost:4000/MistralLarge2" rel="alternate" type="text/html" title="Mistral Unveils Mistral 7B - A Cutting-Edge Language Model" /><published>2024-07-24T00:00:00+03:00</published><updated>2024-07-24T00:00:00+03:00</updated><id>http://localhost:4000/MistralLarge2</id><content type="html" xml:base="http://localhost:4000/MistralLarge2">&lt;p&gt;In a significant leap for artificial intelligence, Mistral has announced the launch of Mistral 7B, a state-of-the-art language model designed to push the boundaries of what AI can achieve in natural language processing. This blog post explores the capabilities and potential impact of Mistral 7B, highlighting its innovative features and the transformative possibilities it offers.&lt;/p&gt;

&lt;h3&gt;The Evolution of Language Models&lt;/h3&gt;

&lt;p&gt;Language models have seen rapid advancements over recent years, evolving from basic text generation to understanding and interacting with human language in sophisticated ways. These models are integral to a wide array of applications, from virtual assistants to content creation, and the introduction of Mistral 7B represents a notable advancement in this field and many more like code generation, mathematics, and reasoning.&lt;/p&gt;

&lt;h3&gt;What is Mistral 7B?&lt;/h3&gt;

&lt;p&gt;Mistral 7B is a large-scale language model developed by Mistral AI. Boasting 7 billion parameters, this model leverages cutting-edge architecture and training techniques to deliver unparalleled performance in natural language understanding and generation. It is designed to be highly versatile, capable of tackling a broad range of tasks with remarkable accuracy and fluency.&lt;/p&gt;

&lt;h3&gt;Key Features of Mistral 7B&lt;/h3&gt;

&lt;p&gt;&lt;ol&gt;
&lt;li&gt; &lt;strong&gt;Enhanced Performance&lt;/strong&gt;: With 7 billion parameters, Mistral 7B offers substantial improvements in both speed and accuracy compared to its predecessors. This makes it capable of handling complex language tasks more efficiently.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;High Versatility&lt;/strong&gt;: Mistral 7B is designed to excel in various applications, from summarizing texts and answering questions to generating creative content and performing sentiment analysis. Its versatility makes it a valuable tool across multiple industries.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Advanced Training Techniques&lt;/strong&gt;: The model employs state-of-the-art training methods, ensuring it learns and adapts from vast amounts of data. This enables it to understand context better and generate more coherent and contextually appropriate responses.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;User-Friendly Interface&lt;/strong&gt;: Mistral 7B is built with usability in mind, providing developers and businesses with easy integration options to incorporate the model into their applications and workflows seamlessly.&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;

&lt;p&gt; You can use Mistral Large 2 today via la Plateforme under the name mistral-large-2407, and test it on le Chat. It is available under the version 24.07 (a YY.MM versioning system that we are applying to all our models), and the API name mistral-large-2407. Weights for the instruct model are available and are also hosted on HuggingFace.&lt;/p&gt;

&lt;p&gt; Appart from those sources you can test it out through cloud platforms. Recently Mistral AI_ has come to an agreement with Google Cloud Platform to bring Mistral AI’s models on &lt;a href=&quot;https://cloud.google.com/blog/products/ai-machine-learning/codestral-and-mistral-large-v2-on-vertex-ai?e=48754805&amp;amp;hl=en&quot;&gt;Vertex AI&lt;/a&gt;. Mistral AI’s best models are now available on Vertex AI, in addition to Azure AI Studio, Amazon Bedrock and IBM watsonx.ai.&lt;/p&gt;

&lt;p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;Google Vertex Al&lt;/th&gt;
    &lt;th&gt;Azure Al Studio&lt;/th&gt;
    &lt;th&gt;Amazon Bedrock&lt;/th&gt;
    &lt;th&gt;IBM watsonx.ai&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Mistral Large 2 (24.07)&lt;/td&gt;
    &lt;td&gt;&amp;#10004;&lt;/td&gt;
    &lt;td&gt;&amp;#10004;&lt;/td&gt;
    &lt;td&gt;&amp;#10004;&lt;/td&gt;
    &lt;td&gt;&amp;#10004;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Mistral Nemo&lt;/td&gt;
    &lt;td&gt;&amp;#10004;&lt;/td&gt;
    &lt;td&gt;&amp;#10004;&lt;/td&gt;
    &lt;td&gt;Comming Soon&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Codestral&lt;/td&gt;
    &lt;td&gt;&amp;#10004;&lt;/td&gt;
    &lt;td&gt;Comming Soon&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Finetuning&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;Comming Soon&lt;/td&gt;
    &lt;td&gt;Comming Soon&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/p&gt;

&lt;p&gt;As with any powerful AI technology, it is crucial to address ethical considerations related to Mistral 7B. Ensuring data privacy, mitigating biases in AI-generated content, and maintaining transparency in how the model operates are essential for building trust and ensuring responsible use. Mistral AI is committed to these principles, aiming to develop AI technologies that are beneficial and equitable.&lt;/p&gt;

&lt;p&gt;Mistral 7B represents a significant advancement in the field of natural language processing, offering enhanced performance, versatility, and usability. By harnessing the power of this cutting-edge language model, industries can unlock new opportunities and efficiencies, paving the way for innovative applications and solutions.&lt;/p&gt;

&lt;p&gt;For more detailed information about Mistral 7B and its potential impact, visit Mistral AI's official announcement &lt;a href=&quot;https://mistral.ai/news/mistral-large-2407/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">In a significant leap for artificial intelligence, Mistral has announced the launch of Mistral 7B, a state-of-the-art language model designed to push the boundaries of what AI can achieve in natural language processing. This blog post explores the capabilities and potential impact of Mistral 7B, highlighting its innovative features and the transformative possibilities it offers.</summary></entry><entry><title type="html">KAN or MLP - A Fairer Comparison</title><link href="http://localhost:4000/KANorMLP" rel="alternate" type="text/html" title="KAN or MLP - A Fairer Comparison" /><published>2024-07-23T00:00:00+03:00</published><updated>2024-07-23T00:00:00+03:00</updated><id>http://localhost:4000/KANorMLP</id><content type="html" xml:base="http://localhost:4000/KANorMLP">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, we control the number of parameters and FLOPs to compare the performance of KAN and MLP. Our main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. We also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, we find that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. We hope these results provide insights for future research on KAN and other MLP alternatives.&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://github.com/yu-rp/KANbeFair&quot;&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.16674&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Introduction to RAG models</title><link href="http://localhost:4000/rag1" rel="alternate" type="text/html" title="Introduction to RAG models" /><published>2024-07-22T00:00:00+03:00</published><updated>2024-07-22T00:00:00+03:00</updated><id>http://localhost:4000/rag1</id><content type="html" xml:base="http://localhost:4000/rag1">&lt;p&gt; Firstly, in case you don't know what is RAG here is an unofficial explanation. Imagine you’re on a treasure hunt, but instead of a dusty old map, you’ve got a genius guide who knows every hidden corner. That’s RAG, short for &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;. It’s like having a super-smart friend who fetches the most relevant bits of knowledge from a massive library (the retrieval part) and then crafts a perfectly tailored response just for you (the generation part). So, if your brain is a bit like a rusty old filing cabinet, think of RAG as your personal, turbo-charged librarian who’s always got the answer before you can say &quot;Google it!&quot;&lt;/p&gt;

&lt;p&gt; In this series of posts, we are going to explore LangChain tools and create RAG models for several applications. We are going to go through the little details that may or may not work in our cases and how to fix those teeny tiny configurations that may be neccesary for our purposes. Without further ado..&lt;/p&gt;

&lt;h2&gt; Dotenv File &lt;/h2&gt;

&lt;p&gt;Before moving forward to modules and script, there is a high need for a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file. Initially, you need to create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file in the folder you are going to create this project, so that you save your passwords. This is not neccesary from the functionality point of view but it is from the security point of view. Think of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file as your project’s secret diary, where it whispers all its deepest, darkest secrets like passwords, API keys, and configuration settings. You need it because you don’t want these secrets plastered all over the code like graffiti. By keeping them in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file, you ensure they stay hidden and safe, only revealing themselves to those in the know—your code. So, a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file is like having a secret stash of information that keeps your project running smoothly without spilling the beans to the world. No need for functy file name or anything just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file.&lt;/p&gt;

&lt;h2&gt; Modules Required &lt;/h2&gt;

&lt;p&gt; The python modules we are going to use for this introductory project are the following&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain import hub
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import AzureOpenAIEmbeddings
from langchain.chat_models import AzureChatOpenAI
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;os&lt;/strong&gt;: Used for accessing environment variables with os.getenv().&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;dotenv.load_dotenv&lt;/strong&gt;: Intended to load environment variables from the .env file.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_community.document_loaders.PyPDFLoader&lt;/strong&gt;: Used to load PDF documents.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_community.document_loaders.DirectoryLoader&lt;/strong&gt;: Used to load documents from a directory.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain.text_splitter.RecursiveCharacterTextSplitter&lt;/strong&gt;: Used to split documents into chunks.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_community.vectorstores.Chroma&lt;/strong&gt;: Used to create a vectorstore from document embeddings.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain.chat_models.AzureChatOpenAI&lt;/strong&gt;: Used to initialize the Azure OpenAI model for the QA chain.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain.hub&lt;/strong&gt;: Used to pull a prompt from the hub.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_core.output_parsers.StrOutputParser&lt;/strong&gt;: Used to parse the output of the QA chain.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_core.runnables.RunnablePassthrough&lt;/strong&gt;: Used in the QA chain to pass through the question.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; In the file where we will run this python script, we need to create a folder where we are going to save our data, our files. Else we can load them from a different path. For the purposes of this series, I decided to use the first option. We use PyPDFLoader to laod them as in the following script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;loader = DirectoryLoader('data/', glob = '*.pdf', loader_cls=PyPDFLoader)
documents = loader.load()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Here we see that the first argument &lt;code&gt;data/&lt;/code&gt; is the relative path where the documents are placed. WIth the second argument &lt;code&gt;*.pdf&lt;/code&gt; we define that we want to take under consideration all the files that end with the afforementioned character sequence '.pdf' (the word any is represented by '*'). Lastly, we define the loader class to use for the purposes of the files loading process &lt;code&gt;PyPDFLoader&lt;/code&gt;. Mind that there are other classes that can be used for different types of documents like &lt;code&gt;UnstructuredFileLoader&lt;/code&gt;, &lt;code&gt;TextLoader&lt;/code&gt;, &lt;code&gt;BSHTMLLoader&lt;/code&gt;, &lt;code&gt;CSVLoader&lt;/code&gt;. To find more about this check &lt;a href=&quot;https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html&quot;&gt;here&lt;/a&gt;. Let your imagination go wild! There are also other types of loaders (which we are going to explore later on) where you can load information from webpages, youtube and many many more. After that is completed we use this class and &lt;code&gt;load()&lt;/code&gt; everything exists in the file path and save them to the onject documents.&lt;/p&gt;

&lt;h2&gt; Chunks and Overlap &lt;/h2&gt;

&lt;p&gt; Next in line of the things we need to accomplish is split the documents into chunks. Chunk?!?!? Imagine you’re trying to eat a massive pizza all by yourself. Chunk_size is like deciding how many slices you cut it into so you can manage each piece without choking. Chunk overlap, on the other hand, is making sure each slice has a bit of the previous one’s crust, so you don’t miss any of the delicious toppings in between. To optimize them, you balance the slice size (chunk_size) to be just right for easy munching, and the overlap so you get all the flavors without making it too repetitive. Get it right, and you’ll devour that pizza with maximum efficiency and satisfaction!&lt;/p&gt;

&lt;p&gt; Here are some tips to help you determine the optimal chunk size if common chunking methods, such as fixed chunking, are not suitable for your use case:

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Data Preprocessing&lt;/strong&gt;: Before deciding on the best chunk size, you need to preprocess your data to ensure its quality. For instance, if your data is sourced from the web, you might need to remove HTML tags or other noise elements.&lt;/li&gt;

&lt;li&gt; &lt;strong&gt;Chunk Sizes&lt;/strong&gt;: After preprocessing, choose a range of potential chunk sizes to test. The selection should consider the nature of the content (e.g., short messages vs. lengthy documents), the embedding model you’ll use, and its token limits. Aim to find a balance between preserving context and maintaining accuracy. Start by exploring various chunk sizes, such as smaller chunks (e.g., 128 or 256 tokens) for capturing granular semantic information and larger chunks (e.g., 512 or 1024 tokens) for retaining more context.&lt;/li&gt;

&lt;li&gt; &lt;strong&gt;Evaluation of Chunk Sizes by performance results.&lt;/strong&gt;: To test different chunk sizes, use either multiple indices or a single index with multiple namespaces. Create embeddings for the selected chunk sizes using a representative dataset and save them in your index or indices. Then, run a series of queries to evaluate the quality and compare the performance of the various chunk sizes. This process is likely iterative, requiring you to test different chunk sizes against different queries until you identify the best-performing chunk size for your content and expected queries.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt; For more information regarding chunk size and chunk overlap, you may refer on &lt;a href=&quot;https://www.kaggle.com/discussions/general/503436&quot;&gt;Guide to Chunk Size and Overlap&lt;/a&gt; by Kaggle, &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies/&quot;&gt;Chunk Sizes&lt;/a&gt; by Llama Index  or &lt;a href=&quot;https://zilliz.com/learn/guide-to-chunking-sreategies-for-rag&quot;&gt;A Guide to Chunking Strategies for Retrieval Augmented Generation (RAG)&lt;/a&gt; by Zilliz. We are not goint to go through the process of deciding the optimal chunk size, as this is out of this project's scope.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32)
text_chunks = text_splitter.split_documents(documents)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now that we have collected our data, and divided into some well structured chunks, easily digestible pieces, we need to create an embeddings model. This model will assists us on creatign embeddings out of the collected and stored documents that we are interested on finding further information about. As I have already mentioned above, I have decided to use Azure OpenAI to assists us. It is up to you which model you are going to use. there are many alternatives bnoth free and paid ones for each part of our project to consider. Do not hesitate to ask me if you have any qyestions on how to change the code so that you use a different kind of model!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;embeddings = AzureOpenAIEmbeddings(
    deployment=os.getenv('OPENAI_DEPLOYMENT_NAME_EMB'),
    model=os.getenv('OPENAI_MODEL_NAME_EMB'),
    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
    openai_api_type=os.getenv('OPENAI_API_TYPE'),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Since we have our embeddings, we are now ready to create our vectorstore where we are going to save our embeddings. Imagine you’ve got a magical, super-organized pantry where every ingredient knows exactly where it belongs and can jump right into your hand when you need it. That’s a vectorstore! It’s a special kind of database where information is stored as vectors, or points in a high-dimensional space, making it super easy to find and retrieve. So, a vectorstore is like having a pantry where every spice, snack, and secret ingredient is neatly indexed and ready to leap out at your command, making your cooking—or in this case, data retrieval—fast and efficient! We have decided to use Chroma as our vectorstore service, but youare free to use any one you need. Some alternatives that you could consider are Pinecone, FAISS, Lance where you can find further information &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.sklearn.SKLearnVectorStore.html&quot;&gt;SKLearnVectorStore&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vectorstore = Chroma.from_documents(documents = text_chunks,                                    
    embedding = embeddings,
    persist_directory=&quot;data/vectorstore&quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt; Retriever &lt;/h2&gt;

&lt;p&gt; Now that we have our data vectorestore set up, we are ready to initialize our retriever. Picture the retriever in a RAG process as your ultra-savvy shopping buddy who knows exactly where everything is in the store. When you need something specific, the retriever zips around the aisles, grabbing the most relevant items off the shelves and bringing them back to you in record time. In the RAG (Retrieval-Augmented Generation) process, the retriever’s job is to fetch the most pertinent pieces of information from a vast database, so the generator can then whip up a perfectly informed response. It’s like having a shopping wizard who makes sure you always have the right ingredients for the perfect recipe!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
prompt = hub.pull('rlm/rag-prompt')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Argument &lt;code&gt;'k':5&lt;/code&gt; makes sure that our retriever will bring back to the generator the 5 most similar items, not 6, not 4. The line &lt;code&gt;hub.pull('rlm/rag-prompt')&lt;/code&gt; is used to pull a specific prompt template named 'rlm/rag-prompt' from a hub. You could define and use your own prompt for this part (which we shall experiment in a later on post). To find out more on predefined qa-prompts, go to &amp;lt;a href'https://docs.smith.langchain.com/old/category/prompt-hub'&amp;gt;Langchain Hub&amp;lt;/a&amp;gt;.&lt;/p&gt;

&lt;h2&gt; Chain Creation &lt;/h2&gt;

&lt;p&gt; The next step in our RAG project is to define the LLM (Large Language Model) that will provide the answers for us. Imagine an LLM as a super-intelligent, chatty robot that’s read every book, article, and meme on the internet and somehow remembers them all. It stands for Large Language Model, and it’s like having a best friend who’s always ready to chat, offer advice, or spin a tale, because it’s been trained on a vast mountain of text data. This robot buddy can understand your questions and whip up responses that sound like they came straight from a well-read, eloquent author. So, if you ever need a conversation partner who’s a walking encyclopedia with a knack for witty comebacks, the LLM’s got your back!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;llm = AzureChatOpenAI(
    deployment_name=os.getenv('LLM_35_Deployment'),
    model_name=os.getenv('LLM_35_Model'),
    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
    temperature=0,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Moving forward, we need to define the RAG chain. Think of a RAG chain as a magical relay race where information is passed along to make the ultimate answer. Imagine a team of information specialists: the first runner grabs the relevant facts (that’s the retriever’s job), the second runner crafts those facts into a coherent, brilliant response (thanks to the generator), and the baton gets passed seamlessly from one to the other. This chain of handoffs ensures you get a well-rounded, perfectly polished answer every time. So, a RAG chain is like a finely-tuned relay team making sure no detail gets left behind and every answer is a winner! We need the RAG chain to clearly define the steps that we need to be included from prompt to final response. Later on we are going to take a closer look on LangGraph a new tool of Langchain which assist us on defining clearly a this process. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rag_chain = (
    {&quot;context&quot;: lambda x: retriever, &quot;question&quot;: RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; You can see clearly what are the steps of our first really simple RAG chain. Initially, the contect and question are passed through associated to the lambda &quot;x&quot; object (in our case the retriever). On the next step the information (context and question) are passed to the prompt so that the instructions are provided to our llm selected model. The model analyzes and constructes an answer to our question. At the end, with the assistance of &lt;code&gt;StrOutputParser()&lt;/code&gt; it is ensures that the answer is presented in the desired human readable format.&lt;/p&gt;

&lt;p&gt;Now you are ready to ask your own questions to the model you just created to find out more about your documents.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;question = &quot;My custom question&quot;
rag_chain.invoke(question)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Well, I hope you enjoyed this as much as I did and learned something from it! Hope to see you again in the next post of this series where we are going to talk about Query transformation and how we can use LLM models so that an LLM can define our question on a different &quot;better&quot; way.&lt;/p&gt;

&lt;p&gt; &lt;i&gt;Be safe, code safer!&lt;/i&gt;&lt;/p&gt;

&lt;p&gt; This whole series is inspired by &amp;lt;a = href='https://www.sakunaharinda.xyz/ragatouille-book/intro.html#'&amp;gt;Ragatoulle&amp;lt;/a&amp;gt;. Throughout this series, as this is the main goal of this blog, I aim to provide as simple as possible explanations about the modules and functions used with addition of my personal touch wherever I find it necessary.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="project" /><category term="Deep Learning" /><summary type="html">Firstly, in case you don't know what is RAG here is an unofficial explanation. Imagine you’re on a treasure hunt, but instead of a dusty old map, you’ve got a genius guide who knows every hidden corner. That’s RAG, short for Retrieval-Augmented Generation. It’s like having a super-smart friend who fetches the most relevant bits of knowledge from a massive library (the retrieval part) and then crafts a perfectly tailored response just for you (the generation part). So, if your brain is a bit like a rusty old filing cabinet, think of RAG as your personal, turbo-charged librarian who’s always got the answer before you can say &quot;Google it!&quot;</summary></entry><entry><title type="html">Shape of Motion - 4D Reconstruction from a Single Video</title><link href="http://localhost:4000/ShapeOfMotion" rel="alternate" type="text/html" title="Shape of Motion - 4D Reconstruction from a Single Video" /><published>2024-07-18T00:00:00+03:00</published><updated>2024-07-18T00:00:00+03:00</updated><id>http://localhost:4000/ShapeOfMotion</id><content type="html" xml:base="http://localhost:4000/ShapeOfMotion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Monocular dynamic reconstruction is a challenging and long-standing vision problem due to the highly ill-posed nature of the task. Existing approaches are limited in that they either depend on templates, are effective only in quasi-static scenes, or fail to model 3D motion explicitly. In this work, we introduce a method capable of reconstructing generic dynamic scenes, featuring explicit, full-sequence-long 3D motion, from casually captured monocular videos. We tackle the under-constrained nature of the problem with two key insights: First, we exploit the low-dimensional structure of 3D motion by representing scene motion with a compact set of SE3 motion bases. Each point's motion is expressed as a linear combination of these bases, facilitating soft decomposition of the scene into multiple rigidly-moving groups. Second, we utilize a comprehensive set of data-driven priors, including monocular depth maps and long-range 2D tracks, and devise a method to effectively consolidate these noisy supervisory signals, resulting in a globally consistent representation of the dynamic scene. Experiments show that our method achieves state-of-the-art performance for both long-range 3D/2D motion estimation and novel view synthesis on dynamic scenes.&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://shape-of-motion.github.io/&quot;&gt; Project page &lt;/a&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2407.13764&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry></feed>