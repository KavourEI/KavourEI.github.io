<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-09T09:51:15+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">In Defense of RAG in the Era of Long-Context Language Models</title><link href="http://localhost:4000/DefenceOfRAG" rel="alternate" type="text/html" title="In Defense of RAG in the Era of Long-Context Language Models" /><published>2024-09-03T00:00:00+03:00</published><updated>2024-09-03T00:00:00+03:00</updated><id>http://localhost:4000/DefenceOfRAG</id><content type="html" xml:base="http://localhost:4000/DefenceOfRAG">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Akkiraju,+R&quot;&gt;Rama Akkiraju&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+T&quot;&gt;Tan Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+A&quot;&gt;Anbang Xu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.01666&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">OLMoE-Open Mixture of Experts Language Models</title><link href="http://localhost:4000/OLMoE" rel="alternate" type="text/html" title="OLMoE-Open Mixture of Experts Language Models" /><published>2024-09-03T00:00:00+03:00</published><updated>2024-09-03T00:00:00+03:00</updated><id>http://localhost:4000/OLMoE</id><content type="html" xml:base="http://localhost:4000/OLMoE">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muennighoff,+N&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Soldaini,+L&quot;&gt;Luca Soldaini&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Groeneveld,+D&quot;&gt;Dirk Groeneveld&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lo,+K&quot;&gt;Kyle Lo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Morrison,+J&quot;&gt;Jacob Morrison&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Min,+S&quot;&gt;Sewon Min&lt;/a&gt;,  &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shi,+W&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Walsh,+P&quot;&gt;Pete Walsh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tafjord,+O&quot;&gt;Oyvind Tafjord&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lambert,+N&quot;&gt;Nathan Lambert&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Gu,+Y&quot;&gt;Yuling Gu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Arora,+S&quot;&gt;Shane Arora&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Bhagia,+A&quot;&gt;Akshita Bhagia&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Schwenk,+D&quot;&gt;Dustin Schwenk&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wadden,+D&quot;&gt;David Wadden&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wettig,+A&quot;&gt;Alexander Wettig&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hui,+B&quot;&gt;Binyuan Hui&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dettmers,+T&quot;&gt;Tim Dettmers&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kiela,+D&quot;&gt;Douwe Kiela&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Farhadi,+A&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Smith,+N+A&quot;&gt;Noah A. Smith&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Koh,+P+W&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Singh,+A&quot;&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hajishirzi,+H&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.02060&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">FLUX that Plays Music</title><link href="http://localhost:4000/FLUXthatPlaysMusic" rel="alternate" type="text/html" title="FLUX that Plays Music" /><published>2024-09-01T00:00:00+03:00</published><updated>2024-09-01T00:00:00+03:00</updated><id>http://localhost:4000/FLUXthatPlaysMusic</id><content type="html" xml:base="http://localhost:4000/FLUXthatPlaysMusic">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; This paper explores a simple extension of diffusion-based rectified flow Transformers for text-to-music generation, termed as FluxMusic. Generally, along with design in advanced Flux\footnote{this https URL} model, we transfers it into a latent VAE space of mel-spectrum. It involves first applying a sequence of independent attention to the double text-music stream, followed by a stacked single music stream for denoised patch prediction. We employ multiple pre-trained text encoders to sufficiently capture caption semantic information as well as inference flexibility. In between, coarse textual information, in conjunction with time step embeddings, is utilized in a modulation mechanism, while fine-grained textual details are concatenated with the music patch sequence as inputs. Through an in-depth study, we demonstrate that rectified flow training with an optimized architecture significantly outperforms established diffusion methods for the text-to-music task, as evidenced by various automatic metrics and human preference evaluations. Our experimental data, code, and model weights are made publicly available at: &lt;a href=&quot;https://github.com/feizc/FluxMusic&quot;&gt;this URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fei,+Z&quot;&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fan,+M&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+C&quot;&gt;Changqian Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Huang,+J&quot;&gt;Junshi Huang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.00587&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Introducing LLaVA V1.5 7B on GroqCloud</title><link href="http://localhost:4000/GroqLlava" rel="alternate" type="text/html" title="Introducing LLaVA V1.5 7B on GroqCloud" /><published>2024-08-27T00:00:00+03:00</published><updated>2024-08-27T00:00:00+03:00</updated><id>http://localhost:4000/GroqLlava</id><content type="html" xml:base="http://localhost:4000/GroqLlava">&lt;p&gt;Introducing LLaVA v1.5 7B: The Next Level of Multimodal AI on GroqCloud&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://console.groq.com/?_gl=1*1v016td*_ga*OTczMzMxMDEyLjE3MjU4NjM4OTU.*_ga_4TD0X2GEZG*MTcyNTg2MjYzNS4xLjEuMTcyNTg2Mzg4Ni42MC4wLjA.&quot;&gt;GroqCloud&lt;/a&gt; has launched &lt;a href=&quot;https://huggingface.co/liuhaotian/llava-v1.5-7b&quot;&gt;LLaVA v1.5 7B&lt;/a&gt;, a state-of-the-art multimodal AI model that combines language, vision, and auditory capabilities.&lt;/p&gt;

&lt;p&gt;LLaVA stands for &lt;i&gt;Large Language and Vision Assistant&lt;/i&gt;, a powerful multimodal model that combines the strengths of language and vision. Based on OpenAI’s CLIP and a fine-tuned version of Meta’s Llama 2 7B model, LLaVA uses visual instruction tuning to support image-based natural instruction following and visual reasoning capabilities. This allows LLaVA to perform a range of tasks, including:

&lt;ul&gt;
&lt;li&gt; Visual question answering: answering questions based on image content&lt;/li&gt;
&lt;li&gt; Caption generation: generating text descriptions of images&lt;/li&gt;
&lt;li&gt; Optical Character Recognition: identifying text in image&lt;/li&gt;
&lt;li&gt; Multimodal dialogue: engaging in conversations that involve both text and images&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When trained in September 2023, LLaVA v1.5 achieved state-of-the-art performance on a total of 7 benchmarks, including 5 academic VQA benchmarks. This demonstrates the model’s exceptional capabilities in understanding and generating text based on visual inputs.&lt;/p&gt;

&lt;p&gt;Use Cases and Industry Benefits LLaVA v1.5 7B can transform industries like retail, finance, education, and manufacturing. Retailers can monitor inventory using image recognition, customer service chatbots can handle text and image queries, and factory lines can automate defect detection. In education, it can assist students by analyzing diagrams and generating explanations.&lt;/p&gt;

&lt;p&gt;Real-World Applications From visual question answering in retail to image captioning for accessibility, LLaVA v1.5 opens up endless possibilities. It can aid quality control on factory lines, automate finance audits by analyzing documents, or enhance the learning experience with detailed image explanations.&lt;/p&gt;

&lt;p&gt;Get Started with GroqCloud LLaVA v1.5 7B is now available on the &lt;a href=&quot;https://console.groq.com/?_gl=1*1pu1tte*_ga*OTczMzMxMDEyLjE3MjU4NjM4OTU.*_ga_4TD0X2GEZG*MTcyNTg2MjYzNS4xLjEuMTcyNTg2Mzg4Ni42MC4wLjA.&quot;&gt;GroqCloud Developer Console&lt;/a&gt;, allowing developers to experiment with its multimodal capabilities. This release marks GroqCloud’s expansion into supporting three modalities—image, audio, and text—offering immense potential for building innovative, real-world applications.&lt;/p&gt;

&lt;p&gt;The Future of Multimodal AI With LLaVA v1.5 7B, developers can push the boundaries of what’s possible by seamlessly integrating visual, auditory, and textual inputs, unlocking a future where AI can understand and generate complex multimodal interactions. Start building with LLaVA today on GroqCloud and lead the way in the AI revolution.&lt;/p&gt;

&lt;p&gt;Check full official article from &lt;a href=&quot;https://groq.com/introducing-llava-v1-5-7b-on-groqcloud-unlocking-the-power-of-multimodal-ai/&quot;&gt;Groq blog&lt;/a&gt;.&lt;/p&gt;
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Introducing LLaVA v1.5 7B: The Next Level of Multimodal AI on GroqCloud</summary></entry><entry><title type="html">Introducing Cerebras Inference - AI at Instant Speed</title><link href="http://localhost:4000/cerebras" rel="alternate" type="text/html" title="Introducing Cerebras Inference - AI at Instant Speed" /><published>2024-08-27T00:00:00+03:00</published><updated>2024-08-27T00:00:00+03:00</updated><id>http://localhost:4000/cerebras</id><content type="html" xml:base="http://localhost:4000/cerebras">&lt;p&gt; Cerebras has unveiled its new AI inference solution, claiming it to be the fastest in the world, outpacing NVIDIA GPU-based clouds by 20 times and delivering industry-leading cost efficiency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cerebras.ai/wp-content/uploads/2024/08/Screenshot-2024-08-26-at-11.39.02%E2%80%AFPM.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt; Powered by the third-generation Wafer Scale Engine (WSE-3), this solution can process 1,800 tokens per second for Llama3.1 8B models and 450 tokens per second for Llama3.1 70B models, all while maintaining high accuracy with native 16-bit weights. The system's exceptional memory bandwidth and unique chip design eliminate traditional bottlenecks, enabling real-time AI responses. With open API access and competitive pricing, Cerebras Inference aims to revolutionize the development and deployment of large language models (LLMs) across various industries.&lt;/p&gt;

&lt;p&gt; This breakthrough allows for more sophisticated AI workflows, such as enhanced real-time intelligence and complex tasks like code generation, which previously required extensive processing power and time. As Cerebras expands support to even larger models, its platform is set to open new possibilities in AI innovation.&lt;/p&gt;

&lt;p&gt; To read more and benefit from those changes you can find out more &lt;a href=&quot;https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Cerebras has unveiled its new AI inference solution, claiming it to be the fastest in the world, outpacing NVIDIA GPU-based clouds by 20 times and delivering industry-leading cost efficiency.</summary></entry><entry><title type="html">Revolutionizing Enterprise Applications with NVIDIA’s NIM Agent Blueprints</title><link href="http://localhost:4000/Prototype2Prompt" rel="alternate" type="text/html" title="Revolutionizing Enterprise Applications with NVIDIA’s NIM Agent Blueprints" /><published>2024-08-27T00:00:00+03:00</published><updated>2024-08-27T00:00:00+03:00</updated><id>http://localhost:4000/Prototype2Prompt</id><content type="html" xml:base="http://localhost:4000/Prototype2Prompt">&lt;p&gt; &lt;a href=&quot;https://www.nvidia.com/en-us/ai-data-science/ai-workflows/&quot;&gt;NVIDIA's NIM Agent Blueprints&lt;/a&gt; empower enterprises to build and deploy customized generative AI applications, driving business transformation and innovation across industries.&lt;/p&gt;

&lt;p&gt; The generative AI landscape is evolving rapidly, moving from simple tools for content creation to sophisticated, enterprise-level applications powered by advanced open-source models like Google Gemma, Llama 3.1, and Microsoft Phi. The introduction of NVIDIA's NIM Agent Blueprints marks a significant step forward in this transformation, offering enterprises a comprehensive toolkit to develop customized AI applications that drive business growth and efficiency.&lt;/p&gt;

&lt;p&gt; NVIDIA’s NIM Agent Blueprints are tailored AI workflows designed for specific business use cases, such as customer service chatbots, drug discovery, and enterprise data extraction. These blueprints provide developers with reference applications, code, and documentation, making it easier to build and deploy generative AI solutions. Importantly, the blueprints are not static; they enable continuous improvement through data-driven feedback loops, enhancing AI models over time.&lt;/p&gt;

&lt;p&gt; The impact of NIM Agent Blueprints extends across industries, enabling enterprises to integrate AI into their workflows with greater ease and efficiency. Companies like ServiceNow are already leveraging these tools to enhance their digital platforms, driving significant AI-driven transformation. NVIDIA’s extensive ecosystem, including partners like Accenture, Deloitte, and global cloud providers, supports the deployment and optimization of these blueprints, ensuring they are scalable and effective across various business environments.&lt;/p&gt;

&lt;p&gt; The first NIM Agent Blueprints available are:
&lt;ul&gt;
&lt;li&gt; digital human for customer service&lt;/li&gt;
&lt;li&gt; generative virtual screening for accelerated drug discovery&lt;/li&gt;
&lt;li&gt; multimodal PDF data extraction for enterprise RAG&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;p&gt; As generative AI continues to advance, the collaboration between developers and data scientists becomes increasingly vital. NIM Agent Blueprints facilitate this collaboration, enabling teams to build, refine, and scale AI applications that not only meet today’s business needs but also adapt to future challenges. With continuous updates and a robust support network, NVIDIA’s NIM Agent Blueprints are set to become a cornerstone in the next phase of enterprise AI innovation, helping companies across the globe harness the full potential of generative AI.&lt;/p&gt;

&lt;p&gt; To further dig deeper on the topic check full NVIDIA's blog article &lt;a href=&quot;https://blogs.nvidia.com/blog/nim-agent-blueprints/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">NVIDIA's NIM Agent Blueprints empower enterprises to build and deploy customized generative AI applications, driving business transformation and innovation across industries.</summary></entry><entry><title type="html">Diffusion Models Are Real-Time Game Engines</title><link href="http://localhost:4000/DiffusionModels" rel="alternate" type="text/html" title="Diffusion Models Are Real-Time Game Engines" /><published>2024-08-27T00:00:00+03:00</published><updated>2024-08-27T00:00:00+03:00</updated><id>http://localhost:4000/DiffusionModels</id><content type="html" xml:base="http://localhost:4000/DiffusionModels">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Valevski,+D&quot;&gt;Dani Valevski&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Leviathan,+Y&quot;&gt;Yaniv Leviathan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Arar,+M&quot;&gt;Moab Arar&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fruchter,+S&quot;&gt;Shlomi Fruchter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2408.14837&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">The Jamba 1.5 Open Model Family-The Most Powerful and Efficient Long Context Models</title><link href="http://localhost:4000/JambaFamilyModels" rel="alternate" type="text/html" title="The Jamba 1.5 Open Model Family-The Most Powerful and Efficient Long Context Models" /><published>2024-08-22T00:00:00+03:00</published><updated>2024-08-22T00:00:00+03:00</updated><id>http://localhost:4000/JambaFamilyModels</id><content type="html" xml:base="http://localhost:4000/JambaFamilyModels">&lt;p&gt; AI21 Labs has introduced the Jamba 1.5 family of models, designed to revolutionize enterprise-level AI with unmatched speed, efficiency, and quality. The models, Jamba 1.5 Mini and Jamba 1.5 Large, are built on the novel SSM-Transformer architecture, providing a massive 256K context window— the longest among open models—along with superior long-context handling and rapid processing speeds. The Jamba models are particularly suited for enterprise applications like document analysis and Retrieval Augmented Generation (RAG), excelling in both quality and cost efficiency.&lt;/p&gt;

&lt;p&gt;Key Highlights:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Unrivaled Context Handling&lt;/strong&gt;: Jamba 1.5 models can manage up to 256K tokens, ensuring consistent performance across long contexts. This capability is vital for complex tasks like document summarization, reducing the need for frequent data chunking and retrievals. 
&lt;img src=&quot;https://cdn.prod.website-files.com/60fd4503684b46390cc0d337/66c71115e631b0aa4bd06a97_66c710b9ad8290acfdc52f48_CW.png&quot; /&gt;
&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Speed and Efficiency&lt;/strong&gt;: Both models are up to 2.5X faster in long-context processing compared to competitors. This speed is crucial for high-demand enterprise applications, ensuring that the models can scale efficiently with business needs.
&lt;img src=&quot;https://cdn.prod.website-files.com/60fd4503684b46390cc0d337/66c71115e631b0aa4bd06a87_66c710cb7314119d3e21d680_Latency.png&quot; /&gt;
&lt;img src=&quot;https://cdn.prod.website-files.com/60fd4503684b46390cc0d337/66c72337cc7cfd6770f21337_66c72247cbaac1cb8b65e1c5_image.png&quot; /&gt;
&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Quality Performance&lt;/strong&gt;: Jamba 1.5 Mini outperforms models in its size class on benchmarks like Arena Hard, while Jamba 1.5 Large surpasses even the most advanced models, including Llama 3.1 70B and 405B. These models deliver top-tier quality and speed, making them a cost-effective solution for enterprises.
&lt;img src=&quot;https://cdn.prod.website-files.com/60fd4503684b46390cc0d337/66c71115e631b0aa4bd06a8d_66c7110a035ebd88291a78f7_Mini%2520Quality.png&quot; /&gt;
&lt;img src=&quot;https://cdn.prod.website-files.com/60fd4503684b46390cc0d337/66c71115e631b0aa4bd06a8a_66c710fef9b0bf20ca3b8f14_Large%2520Quality.png&quot; /&gt;
&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Multilingual Capabilities&lt;/strong&gt;: Beyond English, Jamba 1.5 models support several languages, including Spanish, French, and Arabic, among others, making them versatile for global applications.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Developer-Ready&lt;/strong&gt;: The models natively support advanced features such as structured JSON output and function calling, and are available for download on platforms like Hugging Face.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Advanced Architecture&lt;/strong&gt;: The SSM-Transformer design combines the quality of Transformer models with the efficiency of AI21's Mamba framework. This design allows Jamba 1.5 models to handle extensive contexts with a lower memory footprint, even on single GPUs.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Quantization Breakthrough&lt;/strong&gt;: AI21 introduces ExpertsInt8, a novel quantization technique that reduces model size and enhances performance without sacrificing quality. This innovation enables the Jamba 1.5 Large model to fit on an 8-GPU node while maintaining its full 256K context capacity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;AI21 Labs' Jamba 1.5 models set a new standard in AI performance, particularly for enterprise applications where speed, efficiency, and accuracy are paramount. These models are readily available on various cloud platforms, with more integrations on the horizon, ensuring broad accessibility for developers and businesses alike.&lt;/p&gt;

&lt;p&gt; You can read full report from the official announcement &lt;a href=&quot;https://www.ai21.com/blog/announcing-jamba-model-family&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">AI21 Labs has introduced the Jamba 1.5 family of models, designed to revolutionize enterprise-level AI with unmatched speed, efficiency, and quality. The models, Jamba 1.5 Mini and Jamba 1.5 Large, are built on the novel SSM-Transformer architecture, providing a massive 256K context window— the longest among open models—along with superior long-context handling and rapid processing speeds. The Jamba models are particularly suited for enterprise applications like document analysis and Retrieval Augmented Generation (RAG), excelling in both quality and cost efficiency.</summary></entry><entry><title type="html">NVIDIA and Mistral AI’s Mistral-NeMo-Minitron 8B Model-A Leap Forward in LLM Efficiency</title><link href="http://localhost:4000/NvidiaMistalNemoMinitron" rel="alternate" type="text/html" title="NVIDIA and Mistral AI’s Mistral-NeMo-Minitron 8B Model-A Leap Forward in LLM Efficiency" /><published>2024-08-21T00:00:00+03:00</published><updated>2024-08-21T00:00:00+03:00</updated><id>http://localhost:4000/NvidiaMistalNemoMinitron</id><content type="html" xml:base="http://localhost:4000/NvidiaMistalNemoMinitron">&lt;p&gt; NVIDIA and Mistral AI have introduced the Mistral-NeMo-Minitron 8B model, an advanced large language model (LLM) that delivers exceptional accuracy across nine popular benchmarks. This model is a pruned and distilled version of the Mistral NeMo 12B, maintaining high performance while being significantly more efficient.&lt;/p&gt;

&lt;h3&gt; Pruning and Distillation: Key Techniques&lt;/h3&gt;

&lt;p&gt; The Mistral-NeMo-Minitron 8B model is created using NVIDIA’s proven approach of model pruning and knowledge distillation. Pruning reduces the model size by removing less critical parts—specifically, by focusing on width pruning, which targets neurons, attention heads, and embedding channels. This approach, when combined with light retraining through knowledge distillation, yields a smaller, faster, and more resource-efficient model without compromising much on predictive power.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;Training tokens&lt;/th&gt;
    &lt;th&gt;Wino-Grande 5-shot&lt;/th&gt;
    &lt;th&gt;ARC Challenge 25-shot&lt;/th&gt;
    &lt;th&gt;MMLU 5-shot&lt;/th&gt;
    &lt;th&gt;Hella Swag 10-shot&lt;/th&gt;
    &lt;th&gt;GSM8K 5-shot&lt;/th&gt;
    &lt;th&gt;TruthfulQA 0-shot&lt;/th&gt;
    &lt;th&gt;XLSum en (20%) 3-shot&lt;/th&gt;
    &lt;th&gt;MBPP 0-shot&lt;/th&gt;
    &lt;th&gt;Human Eval 0-shot&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;Llama 3.1 8B&lt;/th&gt;
    &lt;td&gt;15T&lt;/td&gt;
    &lt;td&gt;77.27&lt;/td&gt;
    &lt;td&gt;57.94&lt;/td&gt;
    &lt;td&gt;65.28&lt;/td&gt;
    &lt;td&gt;81.80&lt;/td&gt;
    &lt;td&gt;48.60&lt;/td&gt;
    &lt;td&gt;45.06&lt;/td&gt;
    &lt;td&gt;30.05&lt;/td&gt;
    &lt;td&gt;42.27&lt;/td&gt;
    &lt;td&gt;24.76&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;Gemma 7B&lt;/th&gt;
    &lt;td&gt;6T&lt;/td&gt;
    &lt;td&gt;78&lt;/td&gt;
    &lt;td&gt;61&lt;/td&gt;
    &lt;td&gt;64&lt;/td&gt;
    &lt;td&gt;82&lt;/td&gt;
    &lt;td&gt;50&lt;/td&gt;
    &lt;td&gt;45&lt;/td&gt;
    &lt;td&gt;17&lt;/td&gt;
    &lt;td&gt;39&lt;/td&gt;
    &lt;td&gt;32&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;Mistral-NeMo-Minitron 8B&lt;/th&gt;
    &lt;td&gt;380B&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;80.35&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;64.42&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;69.51&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;83.03&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;58.45&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;47.56&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;31.94&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;43.77&lt;/strong&gt;&lt;/td&gt;
    &lt;td&gt;&lt;strong&gt;36.22&lt;/strong&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;th&gt;Mistral NeMo 12B&lt;/th&gt;
    &lt;td&gt;N/A&lt;/td&gt;
    &lt;td&gt;82.24&lt;/td&gt;
    &lt;td&gt;65.10&lt;/td&gt;
    &lt;td&gt;68.99&lt;/td&gt;
    &lt;td&gt;85.16&lt;/td&gt;
    &lt;td&gt;56.41&lt;/td&gt;
    &lt;td&gt;49.79&lt;/td&gt;
    &lt;td&gt;33.43&lt;/td&gt;
    &lt;td&gt;42.63&lt;/td&gt;
    &lt;td&gt;23.78&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;figcaption class=&quot;wp-element-caption&quot;&gt;&lt;em&gt;&lt;em&gt;Table 1. Accuracy of the Mistral-NeMo-Minitron 8B base model compared to the teacher Mistral-NeMo 12B, Gemma 7B, and Llama-3.1 8B base models. Bold numbers represent the best among the 8B model class&lt;/em&gt;&lt;/em&gt;&lt;/figcaption&gt;

&lt;p&gt; Model Pruning involves slimming down the model by either dropping layers (depth pruning) or reducing the size of internal components (width pruning). In this case, width pruning was employed, which reduced the MLP intermediate dimensions and hidden sizes while retaining the number of attention heads and layers. This method is preferred as it consistently outperforms depth pruning.&lt;/p&gt;

&lt;p&gt; Knowledge Distillation serves to transfer knowledge from a larger &quot;teacher&quot; model (in this case, the Mistral NeMo 12B) to the smaller &quot;student&quot; model (Mistral-NeMo-Minitron 8B). This step involves retraining the pruned model with a smaller dataset, ensuring it maintains high accuracy while being faster and more efficient.&lt;/p&gt;

&lt;h3&gt; Process and Results&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Teacher Fine-Tuning&lt;/strong&gt;: The unpruned 12B model was fine-tuned with 127 billion tokens to correct distribution shifts, ensuring optimal performance during distillation.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Width Pruning&lt;/strong&gt;: Importance scores were calculated for pruning, focusing on compressing the MLP intermediate dimension and the hidden size, while maintaining the number of attention heads and layers.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Distillation&lt;/strong&gt;: The pruned model was distilled using a carefully controlled training process, ensuring minimal loss of accuracy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; The result is a model that not only rivals but often surpasses similar-sized models in accuracy, while also being significantly more resource-efficient.&lt;/p&gt;

&lt;h3&gt; Conclusion and Future Directions&lt;/h3&gt;
&lt;p&gt; The Mistral-NeMo-Minitron 8B demonstrates the effectiveness of structured weight pruning combined with knowledge distillation, offering a scalable approach to building efficient and high-performing LLMs. NVIDIA plans to continue refining these techniques, with future efforts aimed at creating even smaller, more accurate models. These innovations will be gradually integrated into the NVIDIA NeMo framework, further advancing the field of generative AI.&lt;/p&gt;

&lt;p&gt; This advancement highlights NVIDIA’s commitment to pushing the boundaries of AI model efficiency, making powerful AI more accessible and cost-effective.&lt;/p&gt;

&lt;p&gt; To read full report, in the official NVDIA Developer blog, click &lt;a href=&quot;https://developer.nvidia.com/blog/mistral-nemo-minitron-8b-foundation-model-delivers-unparalleled-accuracy/?ncid=ref-inor-390349/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">NVIDIA and Mistral AI have introduced the Mistral-NeMo-Minitron 8B model, an advanced large language model (LLM) that delivers exceptional accuracy across nine popular benchmarks. This model is a pruned and distilled version of the Mistral NeMo 12B, maintaining high performance while being significantly more efficient.</summary></entry><entry><title type="html">Enhancing retrieval augmented generation through drafting</title><link href="http://localhost:4000/SpeculativeRAGGoogle" rel="alternate" type="text/html" title="Enhancing retrieval augmented generation through drafting" /><published>2024-08-21T00:00:00+03:00</published><updated>2024-08-21T00:00:00+03:00</updated><id>http://localhost:4000/SpeculativeRAGGoogle</id><content type="html" xml:base="http://localhost:4000/SpeculativeRAGGoogle">&lt;p&gt; In the evolving landscape of AI, large language models (LLMs) have become essential for generating human-like text. However, these models often struggle with accuracy, particularly when tasked with answering complex, knowledge-intensive questions. This challenge has given rise to Retrieval Augmented Generation (RAG) systems, which combine LLMs with external knowledge retrieval to improve the factual accuracy of responses. While RAG enhances accuracy, it comes with trade-offs in efficiency, especially when dealing with large amounts of retrieved data that require complex reasoning.&lt;/p&gt;

&lt;p&gt;To address this, a novel framework called &lt;strong&gt;Speculative RAG&lt;/strong&gt; has been introduced. This approach optimizes the balance between accuracy and efficiency by leveraging a two-step process: drafting and verification. The process begins with a smaller, specialized LLM—referred to as the &lt;i&gt;RAG drafter&lt;/i&gt;—that generates multiple draft responses based on retrieved documents. These drafts are then fed into a larger, &lt;i&gt;generalist LLM&lt;/i&gt;—acting as the verifier—to select the most accurate and contextually appropriate response.&lt;/p&gt;

&lt;p&gt;Speculative RAG employs a technique known as speculative decoding, which speeds up the inference process by allowing the specialist LLM to generate multiple drafts simultaneously. This method not only reduces the computational load on the larger LLM but also ensures that the final output is both accurate and efficient.&lt;/p&gt;

&lt;p&gt;For instance, when asked a question like &quot;Which actress starred as Doralee Rhodes in the 1980 film Nine to Five?&quot;, the RAG system retrieves relevant documents from its knowledge base. The specialist drafter quickly generates several possible answers, each backed by its rationale. The generalist verifier then assesses these drafts, filtering out any inaccuracies—such as information mistakenly drawn from a related but incorrect source, like the Nine to Five musical—and selects the correct answer.&lt;/p&gt;

&lt;p&gt;The effectiveness of Speculative RAG is demonstrated through rigorous benchmarking against standard RAG systems across datasets such as &lt;a href=&quot;https://aclanthology.org/P17-1147/&quot;&gt;TriviaQA&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2108.00573&quot;&gt;MuSiQue&lt;/a&gt;, &lt;a href=&quot;https://github.com/neemakot/Health-Fact-Checking&quot;&gt;PubHealth&lt;/a&gt;, and &lt;a href=&quot;https://allenai.org/data/arc&quot;&gt;ARC-Challenge&lt;/a&gt;. Results show that Speculative RAG not only achieves higher accuracy—outperforming traditional methods by up to 12.97% on some datasets—but also significantly reduces latency, cutting response times by as much as 51%. This dual improvement in both speed and accuracy represents a significant leap forward in the performance of RAG systems.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://storage.googleapis.com/gweb-research2023-media/images/Speculative_RAG_img2.width-800.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The success of Speculative RAG underscores the potential of collaborative AI architectures, where tasks are intelligently divided between specialized and generalist models. By offloading specific tasks to models optimized for those functions, Speculative RAG provides a robust framework for enhancing the quality of AI-generated content while maintaining efficiency. As AI continues to evolve, such innovations will be crucial in developing systems that are not only powerful but also reliable and quick, paving the way for more sophisticated and responsive AI applications.&lt;/p&gt;

&lt;p&gt; You can read Google's research full blog post &lt;a href=&quot;https://research.google/blog/speculative-rag-enhancing-retrieval-augmented-generation-through-drafting/&quot;&gt;here&lt;/a&gt; or you can check our the &lt;a href=&quot;https://arxiv.org/pdf/2407.08223&quot;&gt;official paper&lt;/a&gt; published related to the topic.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">In the evolving landscape of AI, large language models (LLMs) have become essential for generating human-like text. However, these models often struggle with accuracy, particularly when tasked with answering complex, knowledge-intensive questions. This challenge has given rise to Retrieval Augmented Generation (RAG) systems, which combine LLMs with external knowledge retrieval to improve the factual accuracy of responses. While RAG enhances accuracy, it comes with trade-offs in efficiency, especially when dealing with large amounts of retrieved data that require complex reasoning.</summary></entry></feed>