<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-10-18T12:40:27+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">Introducing Internal Knowledge Search and Spaces by Perplexity</title><link href="http://localhost:4000/perplexity" rel="alternate" type="text/html" title="Introducing Internal Knowledge Search and Spaces by Perplexity" /><published>2024-10-17T00:00:00+03:00</published><updated>2024-10-17T00:00:00+03:00</updated><id>http://localhost:4000/perplexity</id><content type="html" xml:base="http://localhost:4000/perplexity">&lt;p&gt;Perplexity AI has launched Internal Knowledge Search and Spaces, innovative features designed to enhance information retrieval and collaboration within teams. These tools aim to streamline access to knowledge and improve productivity by allowing users to create customized environments for their specific needs.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/LqKZAHeCkEg?si=Lbi6TDCAYDz_0fvn&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In an era where information overload is a common challenge, Perplexity AI has introduced two groundbreaking features: &lt;strong&gt;Internal Knowledge Search&lt;/strong&gt; and &lt;strong&gt;Spaces&lt;/strong&gt;. These tools are designed to empower users by enhancing their ability to find relevant information quickly and collaborate effectively within their teams. By focusing on personalized knowledge management, Perplexity AI aims to transform how individuals and organizations interact with data.&lt;/p&gt;

&lt;p&gt;Effective knowledge management is crucial for organizations looking to maintain a competitive edge. As teams grow and projects become more complex, the ability to access and share information seamlessly becomes paramount. Internal Knowledge Search addresses this need by providing a powerful search capability that allows users to sift through vast amounts of data effortlessly. This feature not only enhances individual productivity but also fosters a culture of collaboration as team members can easily share insights and findings.&lt;/p&gt;

&lt;p&gt;The Internal Knowledge Search feature enables users to conduct deep searches across internal documents, notes, and other resources, making it easier to locate specific information quickly. This capability is enhanced by advanced filtering options that allow users to refine their searches based on various criteria. Meanwhile, Spaces offers a customizable environment where teams can organize their projects, share resources, and collaborate in real-time. Users can create dedicated spaces for different projects or topics, ensuring that relevant information is always at hand. Together, these features create a cohesive ecosystem that enhances both individual and team productivity.&lt;/p&gt;

&lt;p&gt;The integration of Internal Knowledge Search into the Perplexity platform is designed with user experience in mind. Users can initiate searches using natural language queries, making the process intuitive and accessible even for those who may not be tech-savvy. The results are displayed in a user-friendly format, allowing individuals to quickly identify the most pertinent information. In addition, the Spaces feature enables teams to set up their workspaces tailored to their specific workflows, facilitating smoother collaboration and communication.&lt;/p&gt;

&lt;p&gt;Organizations across various sectors can benefit from these new features. For instance, research teams can utilize Internal Knowledge Search to locate relevant studies or data points swiftly, while marketing teams can leverage Spaces to brainstorm ideas and track campaign progress in a centralized location. By streamlining access to information and enhancing collaborative efforts, these tools empower teams to work more efficiently and effectively.&lt;/p&gt;

&lt;p&gt;As Perplexity AI continues to innovate, the introduction of Internal Knowledge Search and Spaces represents a significant step toward improving how individuals manage knowledge and collaborate within teams. The company is committed to refining these features based on user feedback, ensuring they meet the evolving needs of its user base. With these tools at their disposal, organizations can look forward to a future where accessing and sharing knowledge becomes second nature.&lt;/p&gt;

&lt;p&gt;The launch of Internal Knowledge Search and Spaces by Perplexity AI marks an important milestone in the realm of knowledge management and collaboration tools. By focusing on user-friendly interfaces and powerful search capabilities, these features are set to transform how teams interact with information. As organizations strive for greater efficiency in their operations, the ability to harness internal knowledge effectively will be crucial for success.&lt;/p&gt;

&lt;p&gt;To to explore these new features further, visit Perplexity AI's official &amp;lt; a href='https://www.perplexity.ai/hub/blog/introducing-internal-knowledge-search-and-spaces?fob=fX3q7wR73cDzq5Dk'&amp;gt;blog post&amp;lt;/a&amp;gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Perplexity AI has launched Internal Knowledge Search and Spaces, innovative features designed to enhance information retrieval and collaboration within teams. These tools aim to streamline access to knowledge and improve productivity by allowing users to create customized environments for their specific needs.</summary></entry><entry><title type="html">Adobe Launches Firefly Video Model</title><link href="http://localhost:4000/AdobeFirefly" rel="alternate" type="text/html" title="Adobe Launches Firefly Video Model" /><published>2024-10-14T00:00:00+03:00</published><updated>2024-10-14T00:00:00+03:00</updated><id>http://localhost:4000/AdobeFirefly</id><content type="html" xml:base="http://localhost:4000/AdobeFirefly">&lt;p&gt;Adobe has unveiled the Firefly Video Model, expanding its suite of generative AI tools to include video capabilities designed for commercial safety. With enhanced features for video editing and integration into popular Adobe applications, this model aims to revolutionize content creation for professionals.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/puEgugluadk?si=P4KsYVEHdZMkaaMC&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;On October 14, 2024, during Adobe MAX, the world’s largest creativity conference, Adobe announced the launch of the Firefly Video Model (beta). This model marks a significant expansion of Adobe's Firefly family of generative AI models, which has already gained traction in image generation since its initial release. With over 13 billion images generated since March 2023, the Firefly suite is rapidly becoming a go-to resource for creative professionals.&lt;/p&gt;

&lt;p&gt;The Firefly Video Model is the first publicly available video model explicitly designed to be safe for commercial use. This innovative tool allows creators to generate and edit video content efficiently while ensuring compliance with licensing and copyright considerations. As part of its beta launch, Adobe is gathering feedback from a select group of creative professionals to refine the model further.&lt;/p&gt;

&lt;p&gt;The Firefly Video Model includes several standout features that enhance its functionality. The &lt;strong&gt;&lt;a href=&quot;https://blog.adobe.com/en/publish/2024/10/14/generative-extend-in-premiere-pro&quot;&gt;Generative Extend&lt;/a&gt;&lt;/strong&gt; feature allows users to seamlessly extend video clips, filling gaps in footage and smoothing transitions within Adobe Premiere Pro. Additionally, users can generate videos directly from text prompts with controls for camera angles and motion through the &lt;strong&gt;Text to Video&lt;/strong&gt; feature. The &lt;strong&gt;Image to Video&lt;/strong&gt; capability enables creators to transform still images or illustrations into dynamic video clips, adding a new dimension to their visual storytelling. Furthermore, the latest Firefly Image 3 model offers image generation that is up to four times faster than previous iterations, while the &lt;strong&gt;Generative Workspace in &lt;a href=&quot;https://blog.adobe.com/en/publish/2024/10/14/photoshop-delivers-powerful-innovation-for-image-editing-ideation-3d-design-more&quot;&gt;Photoshop&lt;/a&gt;&lt;/strong&gt; fosters creativity and collaboration by enabling designers to brainstorm and iterate concepts quickly.&lt;/p&gt;

&lt;p&gt;The Firefly Video Model integrates seamlessly with various Adobe applications including Photoshop, Illustrator, and Premiere Pro. This integration enhances workflows by allowing users to leverage generative AI capabilities across different platforms. For instance, the Generative Shape Fill and Text to Pattern features in Illustrator empower designers to create unique artwork rapidly.&lt;/p&gt;

&lt;p&gt;Adobe also introduced new offerings aimed at enterprises through Firefly Services. These include:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://blog.adobe.com/en/publish/2024/10/14/oliver-supercharges-its-ai-content-creation-with-help-from-adobe-firefly-services&quot;&gt;Dubbing and Lip Sync&lt;/a&gt;:&lt;/strong&gt; This feature uses generative AI to translate spoken dialogue into multiple languages while maintaining the original voice's sound and lip movements.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Bulk Create:&lt;/strong&gt; Aimed at enhancing efficiency, this tool allows creative professionals to edit large volumes of images quickly, streamlining tasks such as resizing and background removal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Leading brands like PepsiCo/Gatorade, IBM, Mattel, and Deloitte have already utilized Adobe Firefly to optimize their workflows and scale content creation. By automating repetitive tasks, these companies can focus more on exploring their creative visions rather than getting bogged down by technical details.&lt;/p&gt;

&lt;p&gt;Adobe emphasizes responsible innovation through its &lt;a href=&quot;https://www.adobe.com/ai/overview/ethics.html&quot;&gt;AI Ethics principles—accountability&lt;/a&gt;, responsibility, and transparency. The company has trained its Firefly models on licensed content from Adobe Stock and public domain sources. Furthermore, Adobe promotes Content Credentials as a standard for transparency in digital content creation, allowing users to understand how content was generated or modified.&lt;/p&gt;

&lt;p&gt;The Firefly Video Model is currently available in a limited public beta at firefly.adobe.com (there is a waiting list &lt;a href=&quot;https://www.adobe.com/products/firefly/features/ai-video-generator.html&quot;&gt;here&lt;/a&gt;). During this phase, generations are free of charge. Adobe plans to provide additional information regarding pricing and offers once the model moves beyond beta testing.&lt;/p&gt;

&lt;p&gt;The launch of the Firefly Video Model represents a significant leap forward in generative AI tools for creative professionals. By integrating advanced video capabilities into its existing suite of applications, Adobe is empowering users to create high-quality content more efficiently than ever before. As feedback from the beta phase rolls in, Adobe aims to refine these tools further, solidifying its position as a leader in creative technology.&lt;/p&gt;

&lt;p&gt;For more information about the Firefly Video Model and other innovations from &amp;lt;a href=https://cts.businesswire.com/ct/CT?id=smartlink&amp;amp;url=http%3A%2F%2Fwww.adobe.com&amp;amp;esheet=53380676&amp;amp;newsitemid=20230413005364&amp;amp;lan=en-US&amp;amp;anchor=www.adobe.com&amp;amp;index=10&amp;amp;md5=415c333dd2658050a4e133a58c1983ec'&amp;gt;Adobe&amp;lt;/a&amp;gt;, visit their &lt;a href=&quot;https://news.adobe.com/news/2024/10/101424-adobe-launches-firefly-video-model&quot;&gt;official website&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Adobe has unveiled the Firefly Video Model, expanding its suite of generative AI tools to include video capabilities designed for commercial safety. With enhanced features for video editing and integration into popular Adobe applications, this model aims to revolutionize content creation for professionals.</summary></entry><entry><title type="html">Zamba2-7B:Setting New Standards for Small Language Models</title><link href="http://localhost:4000/Zamba" rel="alternate" type="text/html" title="Zamba2-7B:Setting New Standards for Small Language Models" /><published>2024-10-14T00:00:00+03:00</published><updated>2024-10-14T00:00:00+03:00</updated><id>http://localhost:4000/Zamba</id><content type="html" xml:base="http://localhost:4000/Zamba">&lt;p&gt;Zyphra has introduced Zamba2-7B, a state-of-the-art small language model that outperforms leading competitors in both quality and performance. Designed for on-device and consumer GPU applications, Zamba2-7B offers exceptional efficiency and capabilities for natural language tasks.&lt;/p&gt;

&lt;p&gt;The rapid evolution of natural language processing (NLP) has led to the development of increasingly sophisticated models. Zyphra's latest offering, Zamba2-7B, is a small language model that stands out in a crowded field by delivering superior performance while maintaining a compact size. This model is particularly suited for applications requiring efficient processing on consumer-grade hardware.&lt;/p&gt;

&lt;p&gt;Zamba2-7B excels in standard language modeling evaluation sets, demonstrating remarkable latency and generation speed. Among small language models with fewer than 9 billion parameters, Zamba2-7B leads the pack in both quality and performance metrics. Its design allows it to outperform notable models from Mistral, Google’s Gemma, and Meta’s Llama3 series.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt; The key features that you will come across while using it and reasons why you should try this model out are:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Exceptional Pretraining Quality:&lt;/strong&gt; The model benefits from high-quality pretraining and annealing datasets, which significantly enhance its performance on a per-training-token basis.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hybrid Architecture:&lt;/strong&gt; Utilizing an advanced hybrid SSM-attention architecture, Zamba2-7B features Mamba layers interleaved with shared attention layers to optimize parameter efficiency.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;LoRA Projection Matrices:&lt;/strong&gt; These matrices allow for increased expressivity within each block while keeping additional parameter overhead minimal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The core architecture of Zamba2-7B builds upon the original Zamba model but introduces enhancements that improve its functionality. The backbone consists of Mamba layers interspersed with shared attention layers—two in Zamba2 compared to one in its predecessor. This innovative design minimizes the parameter cost while maximizing performance.&lt;/p&gt;

&lt;p&gt;A notable feature of the architecture is the concatenation of original model embeddings with the attention block. This approach helps maintain information across depth, resulting in better overall performance during generation tasks.&lt;/p&gt;

&lt;p&gt;Zamba2-7B was trained on 128 H100 GPUs over approximately 50 days using Zyphra's internal training framework built atop Megatron-LM. This extensive training process demonstrates that even at the 7 billion parameter scale, significant advancements can be achieved with a small team and moderate budget.&lt;/p&gt;

&lt;p&gt;The model achieves state-of-the-art inference efficiency across various metrics, including latency, throughput, and memory usage. These efficiencies make it an attractive option for enterprises looking to deploy powerful AI solutions without extensive computational resources.&lt;/p&gt;

&lt;p&gt;Zyphra is committed to democratizing access to advanced AI systems by releasing Zamba2-7B under an open-source license. This decision allows researchers, developers, and companies to leverage its capabilities freely. The broader AI community is invited to explore Zamba's unique architecture and contribute to ongoing advancements in efficient foundation models.&lt;/p&gt;

&lt;p&gt;A Hugging Face integration is available for easy access to &lt;a href=&quot;https://huggingface.co/Zyphra/Zamba2-7B&quot;&gt;Zamba2-7B&lt;/a&gt;, alongside a &lt;a href=&quot;https://github.com/Zyphra/Zamba2&quot;&gt;pure-PyTorch&lt;/a&gt; implementation that facilitates further customization and experimentation by developers.&lt;/p&gt;

&lt;p&gt;Zyphra's team aims to continue pushing the boundaries of what small language models can achieve. By exploring novel architectures and enhancing their understanding of powerful models, they are dedicated to advancing the field of AI research and application.&lt;/p&gt;

&lt;p&gt;The launch of Zamba2-7B marks a significant milestone in the development of small language models. With its exceptional performance metrics and efficient architecture, it sets a new standard for what can be achieved in natural language processing on consumer hardware. As Zyphra opens up this technology to the community, the potential for innovation in AI applications continues to grow.&lt;/p&gt;

&lt;p&gt;For more information about Zamba2-7B and its capabilities, visit Zyphra's &lt;a href=&quot;https://www.zyphra.com&quot;&gt;official website&lt;/a&gt;, or the official &lt;a href=&quot;https://www.zyphra.com/post/zamba2-7b&quot;&gt;blog post&lt;/a&gt; presenting the afforementioned results.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Zyphra has introduced Zamba2-7B, a state-of-the-art small language model that outperforms leading competitors in both quality and performance. Designed for on-device and consumer GPU applications, Zamba2-7B offers exceptional efficiency and capabilities for natural language tasks.</summary></entry><entry><title type="html">Introducing ARIA:The First Open Multimodal Native MoE Model</title><link href="http://localhost:4000/Aria" rel="alternate" type="text/html" title="Introducing ARIA:The First Open Multimodal Native MoE Model" /><published>2024-10-10T00:00:00+03:00</published><updated>2024-10-10T00:00:00+03:00</updated><id>http://localhost:4000/Aria</id><content type="html" xml:base="http://localhost:4000/Aria">&lt;p&gt;Rhymes.ai has launched ARIA, the first open multimodal native mixture of experts (MoE) model, designed to enhance AI capabilities across various applications. With its innovative architecture and advanced features, ARIA aims to set a new standard in the realm of multimodal AI, enabling more efficient processing of diverse data types.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://rhymes.ai/images/blog/Aria-intro-10062024/aria-is-onellm.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The field of artificial intelligence is rapidly evolving, with new models emerging to tackle complex tasks involving multiple data modalities. Rhymes.ai has taken a significant step forward with the introduction of ARIA, the first open multimodal native mixture of experts (MoE) model. This groundbreaking model is engineered to process and integrate various types of data—such as text, images, and audio—allowing for more sophisticated AI applications.&lt;/p&gt;

&lt;p&gt;Multimodal models are essential for understanding and generating content that reflects the complexity of real-world information. By combining different forms of data, these models can provide richer insights and more accurate outputs. ARIA's architecture leverages the strengths of MoE technology to optimize resource allocation, enabling it to handle diverse tasks efficiently while maintaining high performance.&lt;/p&gt;

&lt;p&gt;ARIA boasts several innovative features that enhance its functionality and usability. It utilizes a mixture of experts architecture that allows for dynamic routing of inputs to specialized sub-models, optimizing computational resources based on task requirements. This design not only improves efficiency but also enhances the model's ability to learn from various data types. Additionally, ARIA supports seamless integration with existing frameworks, making it easier for developers to incorporate its capabilities into their applications. The model is open-source, promoting collaboration and innovation within the AI community while ensuring transparency in its development process.&lt;/p&gt;

&lt;p&gt;The versatility of ARIA opens up numerous possibilities for its application across different domains. In healthcare, it can analyze patient records alongside medical images to provide comprehensive insights for diagnosis and treatment planning. In creative industries, ARIA can assist in generating multimedia content by integrating text descriptions with visual elements, enhancing storytelling capabilities. Furthermore, its ability to process audio data allows for advancements in speech recognition and natural language understanding, making it a valuable tool for developers working on voice-activated applications.&lt;/p&gt;

&lt;p&gt;Initial tests have shown that ARIA outperforms many existing models in various benchmarks related to multimodal tasks. Its unique architecture allows it to achieve higher accuracy rates while reducing latency during processing. This performance boost is particularly beneficial in real-time applications where quick responses are crucial.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://rhymes.ai/images/blog/Aria-intro-10062024/barChart.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As AI continues to evolve, the demand for models that can seamlessly integrate multiple data types will only increase. ARIA represents a significant advancement in this area, providing a robust framework for future developments in multimodal AI. Rhymes.ai is committed to refining ARIA further based on user feedback and ongoing research, ensuring that it remains at the forefront of technological innovation.&lt;/p&gt;

&lt;p&gt;The launch of ARIA marks a pivotal moment in the development of multimodal AI models. By combining advanced features with an open-source approach, Rhymes.ai is paving the way for more efficient and effective AI solutions across various industries. As developers and researchers explore the capabilities of ARIA, the potential for groundbreaking applications in fields such as healthcare, creative arts, and beyond becomes increasingly apparent.&lt;/p&gt;

&lt;p&gt;For more information about ARIA and its features, visit Rhymes.ai's official &lt;a href=&quot;https://rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model&quot;&gt;blog post&lt;/a&gt; where you can real case scenarios including prompts and results.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Rhymes.ai has launched ARIA, the first open multimodal native mixture of experts (MoE) model, designed to enhance AI capabilities across various applications. With its innovative architecture and advanced features, ARIA aims to set a new standard in the realm of multimodal AI, enabling more efficient processing of diverse data types.</summary></entry><entry><title type="html">MLE-bench:Evaluating Machine Learning Agents on Machine Learning Engineering</title><link href="http://localhost:4000/MLEBench" rel="alternate" type="text/html" title="MLE-bench:Evaluating Machine Learning Agents on Machine Learning Engineering" /><published>2024-10-09T00:00:00+03:00</published><updated>2024-10-09T00:00:00+03:00</updated><id>http://localhost:4000/MLEBench</id><content type="html" xml:base="http://localhost:4000/MLEBench">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (this http URL) to facilitate future research in understanding the ML engineering capabilities of AI agents.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chan,+J+S&quot;&gt;Jun Shern Chan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chowdhury,+N&quot;&gt;Neil Chowdhury&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jaffe,+O&quot;&gt;Oliver Jaffe&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Aung,+J&quot;&gt;James Aung&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sherburn,+D&quot;&gt;Dane Sherburn&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Mays,+E&quot;&gt;Evan Mays&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Starace,+G&quot;&gt;Giulio Starace&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+K&quot;&gt;Kevin Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Maksin,+L&quot;&gt;Leon Maksin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Patwardhan,+T&quot;&gt;Tejal Patwardhan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Weng,+L&quot;&gt;Lilian Weng&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=M%C4%85dry,+A&quot;&gt;Aleksander Mądry&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.07095&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">One Initialization to Rule them All:Fine-tuning via Explained Variance Adaptation</title><link href="http://localhost:4000/FTVarAdapt" rel="alternate" type="text/html" title="One Initialization to Rule them All:Fine-tuning via Explained Variance Adaptation" /><published>2024-10-09T00:00:00+03:00</published><updated>2024-10-09T00:00:00+03:00</updated><id>http://localhost:4000/FTVarAdapt</id><content type="html" xml:base="http://localhost:4000/FTVarAdapt">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are usually initialized at random with a uniform rank distribution across model weights. Recent works focus on weight-driven initialization or learning of adaptive ranks during training. Both approaches have only been investigated in isolation, resulting in slow convergence or a uniform rank distribution, in turn leading to sub-optimal performance. We propose to enhance LoRA by initializing the new weights in a data-driven manner by computing singular value decomposition on minibatches of activation vectors. Then, we initialize the LoRA matrices with the obtained right-singular vectors and re-distribute ranks among all weight matrices to explain the maximal amount of variance and continue the standard LoRA fine-tuning procedure. This results in our new method Explained Variance Adaptation (EVA). We apply EVA to a variety of fine-tuning tasks ranging from language generation and understanding to image classification and reinforcement learning. EVA exhibits faster convergence than competitors and attains the highest average score across a multitude of tasks per domain.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Paischer,+F&quot;&gt;Fabian Paischer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hauzenberger,+L&quot;&gt;Lukas Hauzenberger&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Schmied,+T&quot;&gt;Thomas Schmied&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Alkin,+B&quot;&gt;Benedikt Alkin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Deisenroth,+M+P&quot;&gt;Marc Peter Deisenroth&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hochreiter,+S&quot;&gt;Sepp Hochreiter&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.07170&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Introducing Palmyra X 004:Revolutionizing AI Tool Calling</title><link href="http://localhost:4000/Palmyra" rel="alternate" type="text/html" title="Introducing Palmyra X 004:Revolutionizing AI Tool Calling" /><published>2024-10-09T00:00:00+03:00</published><updated>2024-10-09T00:00:00+03:00</updated><id>http://localhost:4000/Palmyra</id><content type="html" xml:base="http://localhost:4000/Palmyra">&lt;p&gt; Writer.com has launched Palmyra X 004, a cutting-edge model that sets a new standard for tool calling in AI applications. With enhanced capabilities for integrating external systems, generating code, and managing extensive context, this model aims to empower developers to build more efficient and scalable AI solutions.&lt;/p&gt;

&lt;p&gt;The landscape of enterprise AI is rapidly evolving, and &lt;a href=&quot;https://writer.com&quot;&gt;Writer&lt;/a&gt; is at the forefront with the &lt;a href=&quot;https://writer.com/blog/actions-with-palmyra-x-004/&quot;&gt;introduction of Palmyra X 004&lt;/a&gt;. This latest model not only enhances the interaction between AI applications and external tools but also addresses the challenges of inefficiency and error-prone processes that have historically plagued AI integration.&lt;/p&gt;

&lt;p&gt;Tool calling refers to the ability of an AI model to interact with external systems and services, enabling it to perform complex tasks beyond its built-in knowledge. Palmyra X 004 excels in this area, allowing developers to automate workflows and reduce repetitive coding efforts. This capability is crucial for scaling AI applications effectively in a business environment.&lt;/p&gt;

&lt;p&gt; Som of the benefits you may notice while using Palmyra X004, and some reasons you may consider adding this model to your toolkit are:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Actionable Tool Calling:&lt;/strong&gt; The model can execute actions in external systems through tool calling, significantly expanding its functional capabilities.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Automatic Data Integration:&lt;/strong&gt; A &lt;a href=&quot;https://dev.writer.com/api-guides/kg-chat&quot;&gt;built-in Retrieval-Augmented Generation (RAG)&lt;/a&gt; tool facilitates seamless data integration.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Code Generation:&lt;/strong&gt; The model can generate code snippets, simplifying the development process.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;128k Context Window:&lt;/strong&gt; This feature allows for handling extensive information in a single interaction.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Structured Output Generation:&lt;/strong&gt; Coming soon, this will simplify system integration by providing formatted responses.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Palmyra X 004 has achieved remarkable results on various benchmarks, including the Berkeley Function Calling Leaderboard. It outperforms other leading models such as those from OpenAI, Anthropic, Meta, and Google. Notably, it ranks as the top model for tool calling capabilities and API selection.&lt;/p&gt;

&lt;p&gt;The model boasts an impressive overall accuracy of 78.76% in identifying and executing correct tool calls. Additionally, it excels in structuring calls with an average performance of 87.93%, demonstrating its ability to interpret user inputs accurately and generate appropriate parameters for execution. Its execution performance also stands out at 88.27%, showcasing its efficiency in carrying out actions across enterprise systems.&lt;/p&gt;

&lt;p&gt;The full-stack design of Palmyra X 004 sets it apart from other models by allowing seamless integration with existing enterprise tools and workflows. Developers can utilize low-code development tools within AI Studio to create applications that leverage built-in RAG capabilities and call upon other no-code Writer apps as microservices.&lt;/p&gt;

&lt;p&gt;This predefined RAG tool enables real-time data retrieval from a company's Knowledge Graph without requiring custom development. By showing its reasoning process and citing sources behind outputs, Palmyra X 004 ensures that each tool call is context-aware and explainable.&lt;/p&gt;

&lt;p&gt;To utilize tool calling effectively, developers must define functions within their applications that describe how the LLM should interact with external systems. For instance, a function like `get_product_info()` can be employed to retrieve product data from an API. Once defined using a JSON schema, these functions allow Palmyra X 004 to process user queries intelligently and execute the necessary actions.&lt;/p&gt;

&lt;p&gt;Writer.com provides comprehensive guides for developers looking to implement these features in their applications. By following step-by-step instructions, developers can harness the full potential of Palmyra X 004’s tool calling capabilities.&lt;/p&gt;

&lt;p&gt;The release of Palmyra X 004 marks a significant advancement in AI application development. By empowering developers with robust tools for creating scalable and efficient applications, Writer.com is redefining how enterprises can leverage AI technology to enhance their operations.&lt;/p&gt;

&lt;p&gt;The introduction of Palmyra X 004 represents a pivotal moment for AI integration within enterprise environments. With its innovative tool calling capabilities and commitment to affordability without sacrificing accuracy or reliability, Writer.com is setting new standards for what is possible with AI technology. As businesses continue to seek effective solutions for automation and efficiency, Palmyra X 004 stands ready to meet these demands head-on.&lt;/p&gt;

&lt;p&gt;Explore the capabilities of Palmyra X 004 in &lt;a href=&quot;https://writer.com/product/ai-studio/&quot;&gt;Writer AI Studio&lt;/a&gt; today and discover how you can begin building the next generation of AI-powered workflows! You can visit &lt;a href=&quot;https://writer.com/engineering/actions-with-palmyra-x-004/&quot;&gt;here&lt;/a&gt; to find out more and see read full Writer.com relevant announcements.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Writer.com has launched Palmyra X 004, a cutting-edge model that sets a new standard for tool calling in AI applications. With enhanced capabilities for integrating external systems, generating code, and managing extensive context, this model aims to empower developers to build more efficient and scalable AI solutions.</summary></entry><entry><title type="html">Neuromnia a Pioneering AI Solutions for Autism Care with Llama</title><link href="http://localhost:4000/Neuronomnia" rel="alternate" type="text/html" title="Neuromnia a Pioneering AI Solutions for Autism Care with Llama" /><published>2024-10-09T00:00:00+03:00</published><updated>2024-10-09T00:00:00+03:00</updated><id>http://localhost:4000/Neuronomnia</id><content type="html" xml:base="http://localhost:4000/Neuronomnia">&lt;p&gt;&lt;a href=&quot;https://neuromnia.com/&quot;&gt;Neuromnia&lt;/a&gt; has launched an innovative AI-driven platform leveraging Llama 3.1 to enhance Applied Behavior Analysis (ABA) therapy for individuals with &lt;a href=&quot;https://www.cdc.gov/autism/data-research/index.html#:~:text=About%201%20in%2036%20children,Read%20Summary%5D&quot;&gt;autism&lt;/a&gt;. By automating treatment planning and documentation, Neuromnia aims to improve clinician productivity and access to care while addressing workforce shortages in the ABA industry.&lt;/p&gt;

&lt;p&gt;As autism affects one in 36 children, the demand for effective care solutions is higher than ever. Neuromnia is stepping up to meet this challenge by providing clinicians, parents, and teachers with advanced AI tools designed to enhance productivity and improve treatment quality. Their latest development, Nia, is a human-centric AI co-pilot built on &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-3-1/&quot;&gt;Llama 3.1&lt;/a&gt;, specifically tailored for Applied Behavior Analysis (ABA) therapy.&lt;/p&gt;

&lt;p&gt;Neuromnia's journey with Llama 3.1 began in May 2024 during the research and testing phase of product development. The team quickly recognized that the 70B model offered the natural language processing capabilities necessary to support their mission. Initially, they utilized Llama to generate behavior intervention plans and skill acquisition recommendations based on client case histories.&lt;/p&gt;

&lt;p&gt;Co-Founder &amp;amp; Chief Product Officer Josh Farrow, a Board Certified Behavior Analyst (BCBA), played a crucial role in curating a dataset that would help reduce the administrative burden on clinicians. According to Co-Founder &amp;amp; CEO Jay Gupta, “After testing with Llama, we were impressed by its state-of-the-art performance in natural language processing tasks.” This collaboration between clinical expertise and advanced AI technology has enabled Neuromnia to build a comprehensive dataset aimed at improving care delivery.&lt;/p&gt;

&lt;p&gt;Open source has been instrumental for startups like Neuromnia in developing scalable solutions without incurring prohibitive costs. The availability of open-source large language models (LLMs) like Llama 3.1 allows teams to leverage community advancements and contributions, refining their capabilities to meet the specific needs of the ABA industry.&lt;/p&gt;

&lt;p&gt;Gupta emphasizes that open source empowers them to build and scale their solutions without falling into vendor lock-in traps. The support from the community has proven invaluable for guidance on LLM creation and deployment.&lt;/p&gt;

&lt;p&gt;Nia is specifically designed to tackle workforce shortages in the ABA field by automating critical elements of treatment planning, documentation, and modification. By suggesting key treatment components—such as goals and intervention strategies—Nia reduces the time clinicians spend on repetitive tasks, allowing them to manage larger caseloads more efficiently.&lt;/p&gt;

&lt;p&gt;The platform also aims to address high burnout and turnover rates among ABA professionals by enhancing productivity through AI-powered capabilities. Gupta notes that many clinics lack robust analytics tools; however, Neuromnia provides automated insights that optimize the entire treatment process from intake to discharge.&lt;/p&gt;

&lt;p&gt;With the release of Llama 3.1, Neuromnia has taken Nia’s capabilities to new heights. After resolving initial misconfigurations through trial and error, the team successfully integrated Llama into their platform. Techniques such as prompt engineering and retrieval-augmented generation (RAG) have been employed to enhance model output quality.&lt;/p&gt;

&lt;p&gt;The team has trained the LLM on clinician-created synthetic data to ensure accurate and context-appropriate recommendations within Nia’s software. Gupta explains that while out-of-the-box LLMs can struggle with complex tasks, they have significantly reduced error rates through a combination of fine-tuning and advanced techniques like semantic search.&lt;/p&gt;

&lt;p&gt;Looking ahead, Neuromnia remains committed to leveraging Llama for ongoing improvements in autism care solutions. They are dedicated to evaluating each new release of Llama to ensure their platform stays cutting-edge and capable of meeting growing demands.&lt;/p&gt;

&lt;p&gt;The launch of Neuromnia’s platform represents a significant step forward in autism care by combining advanced AI technology with clinical expertise. By automating key processes within ABA therapy, Neuromnia not only enhances clinician productivity but also improves access to quality care for families affected by autism. As they continue to innovate with Llama, Neuromnia is poised to make a lasting impact on the field of autism support.&lt;/p&gt;

&lt;p&gt;For more information about Neuromnia’s initiatives and updates, visit their &lt;a href=&quot;https://www.neuromnia.com/&quot;&gt;website&lt;/a&gt;. To read full post, visit &lt;a href=&quot;https://ai.meta.com/blog/neuromnia-autism-aba-therapy-built-with-llama/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Neuromnia has launched an innovative AI-driven platform leveraging Llama 3.1 to enhance Applied Behavior Analysis (ABA) therapy for individuals with autism. By automating treatment planning and documentation, Neuromnia aims to improve clinician productivity and access to care while addressing workforce shortages in the ABA industry.</summary></entry><entry><title type="html">Pixtral 12B</title><link href="http://localhost:4000/Pixtral" rel="alternate" type="text/html" title="Pixtral 12B" /><published>2024-10-09T00:00:00+03:00</published><updated>2024-10-09T00:00:00+03:00</updated><id>http://localhost:4000/Pixtral</id><content type="html" xml:base="http://localhost:4000/Pixtral">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \&amp;amp; Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Agrawal,+P&quot;&gt;Pravesh Agrawal&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Antoniak,+S&quot;&gt;Szymon Antoniak&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hanna,+E+B&quot;&gt;Emma Bou Hanna&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Bout,+B&quot;&gt;Baptiste Bout&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chaplot,+D&quot;&gt;Devendra Chaplot&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chudnovsky,+J&quot;&gt;Jessica Chudnovsky&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Costa,+D&quot;&gt;Diogo Costa&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=De+Monicault,+B&quot;&gt;Baudouin De Monicault&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Garg,+S&quot;&gt;Saurabh Garg&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Gervet,+T&quot;&gt;Theophile Gervet&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ghosh,+S&quot;&gt;Soham Ghosh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=H%C3%A9liou,+A&quot;&gt;Amélie Héliou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jacob,+P&quot;&gt;Paul Jacob&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jiang,+A+Q&quot;&gt;Albert Q. Jiang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Khandelwal,+K&quot;&gt;Kartik Khandelwal&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lacroix,+T&quot;&gt;Timothée Lacroix&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lample,+G&quot;&gt;Guillaume Lample&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Casas,+D+L&quot;&gt;Diego Las Casas&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lavril,+T&quot;&gt;Thibaut Lavril&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Scao,+T+L&quot;&gt;Teven Le Scao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lo,+A&quot;&gt;Andy Lo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Marshall,+W&quot;&gt;William Marshall&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Martin,+L&quot;&gt;Louis Martin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Mensch,+A&quot;&gt;Arthur Mensch&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muddireddy,+P&quot;&gt;Pavankumar Muddireddy&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Nemychnikova,+V&quot;&gt;Valera Nemychnikova&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Pellat,+M&quot;&gt;Marie Pellat&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Von+Platen,+P&quot;&gt;Patrick Von Platen&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Raghuraman,+N&quot;&gt;Nikhil Raghuraman&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Rozi%C3%A8re,+B&quot;&gt;Baptiste Rozière&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sablayrolles,+A&quot;&gt;Alexandre Sablayrolles&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Saulnier,+L&quot;&gt;Lucile Saulnier&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sauvestre,+R&quot;&gt;Romain Sauvestre&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shang,+W&quot;&gt;Wendy Shang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Soletskyi,+R&quot;&gt;Roman Soletskyi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Stewart,+L&quot;&gt;Lawrence Stewart&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Stock,+P&quot;&gt;Pierre Stock&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Studnia,+J&quot;&gt;Joachim Studnia&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Subramanian,+S&quot;&gt;Sandeep Subramanian&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Vaze,+S&quot;&gt;Sagar Vaze&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+T&quot;&gt;Thomas Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+S&quot;&gt;Sophia Yang&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.07073&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Introducing Long-Term Memory Support in LangGraph</title><link href="http://localhost:4000/LonTermMemory" rel="alternate" type="text/html" title="Introducing Long-Term Memory Support in LangGraph" /><published>2024-10-08T00:00:00+03:00</published><updated>2024-10-08T00:00:00+03:00</updated><id>http://localhost:4000/LonTermMemory</id><content type="html" xml:base="http://localhost:4000/LonTermMemory">&lt;p&gt;LangChain has announced the launch of long-term memory support in LangGraph, enabling AI agents to store and recall information across conversations. This feature enhances user interactions by allowing agents to learn from feedback and adapt to user preferences, ultimately improving the overall experience.&lt;/p&gt;

&lt;p&gt;The introduction of long-term memory support in LangGraph marks a significant advancement in the capabilities of AI agents. Traditionally, many AI applications have struggled with maintaining context between conversations, often likened to a &quot;goldfish&quot; that forgets everything after each interaction. This limitation not only hampers efficiency but also restricts the potential of AI applications to provide personalized experiences.&lt;/p&gt;

&lt;p&gt;Over the past year, LangChain has collaborated with various customers to integrate memory into their AI agents. Through these interactions, it became clear that there is no one-size-fits-all solution for AI memory. Different applications require specific logic tailored to their unique needs, which is why LangGraph's approach focuses on building a simple yet reliable persistent memory layer.&lt;/p&gt;

&lt;h3&gt;Key Features of Long-Term Memory Support&lt;/h3&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Cross-Thread Memory:&lt;/strong&gt; Extends memory capabilities across multiple conversation threads, allowing agents to remember information from previous interactions.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Persistent Document Store:&lt;/strong&gt; Utilizes a straightforward document storage system that supports basic operations like putting, getting, and searching for saved memories.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Flexible Namespacing:&lt;/strong&gt; Organizes memories using custom namespaces for better management across different users or contexts.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;JSON Document Storage:&lt;/strong&gt; Saves memories as JSON documents, facilitating easy manipulation and retrieval.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Content-Based Filtering:&lt;/strong&gt; Enables searching through memories based on their content across namespaces.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To assist developers in implementing long-term memory within their applications, LangChain has provided several resources. A new LangGraph template is available that showcases a chatbot agent capable of managing its own memory. This template serves as a practical demonstration of how to leverage long-term memory features effectively.&lt;/p&gt;

&lt;h3&gt;Resources Available&lt;/h3&gt;
&lt;ul&gt;
    &lt;li&gt;An end-to-end tutorial &lt;a href=&quot;https://youtu.be/-xkduCeudgY?ref=blog.langchain.dev&quot;&gt;video&lt;/a&gt; that guides users through the implementation process.&lt;/li&gt;
    &lt;li&gt;A &lt;a href=&quot;https://youtu.be/-xkduCeudgY?ref=blog.langchain.dev&quot;&gt;LangGraph Memory Agent&lt;/a&gt; example in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence/?ref=blog.langchain.dev&quot;&gt;Python&lt;/a&gt;.&lt;/li&gt;
    &lt;li&gt;A &lt;a href=&quot;https://github.com/langchain-ai/memory-agent-js?ref=blog.langchain.dev&quot;&gt;LangGraph.js Memory Agent&lt;/a&gt; example in &lt;a href=&quot;https://langchain-ai.github.io/langgraphjs/how-tos/cross-thread-persistence/?ref=blog.langchain.dev&quot;&gt;JavaScript&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The introduction of long-term memory support is designed to empower developers to create more sophisticated and user-friendly AI applications. By incorporating these new capabilities into LangGraph projects, developers can enhance user experiences significantly. The provided resources aim to bridge the gap between theoretical concepts and practical application, making it easier for developers to experiment with long-term memory integration.&lt;/p&gt;

&lt;p&gt;As LangChain continues to refine its offerings based on user feedback, the introduction of long-term memory support sets the stage for more advanced functionalities in future releases. The ability for AI agents to learn from past interactions and adapt accordingly will pave the way for more personalized and effective communication between users and their digital counterparts.&lt;/p&gt;

&lt;p&gt;The launch of long-term memory support in LangGraph represents an important milestone for AI development. By enabling agents to remember and learn from previous conversations, LangChain is enhancing the potential for creating more engaging and responsive applications. Developers are encouraged to explore these new features and share their experiences as they integrate long-term memory into their projects.&lt;/p&gt;

&lt;p&gt;This innovative approach not only improves the functionality of AI agents but also enriches the overall user experience, making interactions more meaningful and tailored to individual preferences. You can read full article and many more relevant posts to the topic in the official &lt;a href=&quot;https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/&quot;&gt;Langchain post&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">LangChain has announced the launch of long-term memory support in LangGraph, enabling AI agents to store and recall information across conversations. This feature enhances user interactions by allowing agents to learn from feedback and adapt to user preferences, ultimately improving the overall experience.</summary></entry></feed>