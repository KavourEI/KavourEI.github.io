<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-17T12:12:00+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">Codeium’s Windsurf Wave 3-A Leap Forward in AI-Powered Coding</title><link href="http://localhost:4000/WindsurfWave3" rel="alternate" type="text/html" title="Codeium’s Windsurf Wave 3-A Leap Forward in AI-Powered Coding" /><published>2025-02-13T00:00:00+02:00</published><updated>2025-02-13T00:00:00+02:00</updated><id>http://localhost:4000/WindsurfWave3</id><content type="html" xml:base="http://localhost:4000/WindsurfWave3">&lt;p&gt;The latest update from Codeium, Windsurf Wave 3, introduces significant improvements in AI-powered code completion and development workflows. Let's take a closer look though, on the updates and changes provided.&lt;/p&gt;

&lt;p&gt;Codeium has consistently pushed the boundaries of AI-driven coding assistance, and Windsurf Wave 3 is no exception. Since the release of the initial Windsurf version (I hope you've tryied it when I suggested you) there have been some major updates. With this latest updated version, developers can expect better code suggestions, improved latency, and a more intuitive user experience. These advancements make AI-assisted programming even more seamless, reducing friction in the development process.&lt;/p&gt;

&lt;p&gt;One of the standout features of Windsurf Wave 3 is its enhanced model accuracy. By leveraging deep learning techniques and training on vast datasets, Codeium's AI provides more contextually relevant code suggestions. This leads to reduced debugging time and helps developers write cleaner, more efficient code.&lt;/p&gt;

&lt;p&gt;Performance optimization has also been a key focus of this update. Faster response times mean that AI-generated code suggestions appear almost instantaneously, maintaining a smooth coding workflow. This speed improvement is especially beneficial for large-scale projects where efficiency is crucial.&lt;/p&gt;

&lt;p&gt;In addition to core improvements, Windsurf Wave 3 introduces better multi-language support. Whether developers work with Python, JavaScript, Java, or other popular programming languages, they can now enjoy a more consistent and accurate AI-powered coding experience. This broadens Codeium’s appeal to a more diverse group of developers.&lt;/p&gt;

&lt;p&gt;Another notable enhancement is the refinement of inline suggestions. The AI now better understands user intent, reducing instances of irrelevant or redundant code completions. This allows for a more intuitive interaction between the developer and the AI assistant.&lt;/p&gt;

&lt;p&gt;Codeium’s continued innovation in AI-driven development tools positions it as a strong competitor in the space. Windsurf Wave 3 reaffirms its commitment to providing cutting-edge AI solutions for developers, making coding more efficient, intelligent, and enjoyable.&lt;/p&gt;

&lt;p&gt;As AI continues to reshape software development, updates like Windsurf Wave 3 pave the way for more sophisticated and reliable coding assistance. With these improvements, Codeium ensures that developers can harness the full power of AI to streamline their work and focus on solving complex problems.&lt;/p&gt;

&lt;p&gt;You can find details about this version updates and many more detailes in the &lt;a href=&quot;https://codeium.com/blog/windsurf-wave-3&quot;&gt;official blog post&lt;/a&gt; where everything is explained thoroughly. Take a minute and eplore the features provided here because I believe you can benefit from it.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">The latest update from Codeium, Windsurf Wave 3, introduces significant improvements in AI-powered code completion and development workflows. Let's take a closer look though, on the updates and changes provided.</summary></entry><entry><title type="html">Meet the New Sonar-Perplexity AI’s Enhanced Search Experience</title><link href="http://localhost:4000/PerplexitySonar" rel="alternate" type="text/html" title="Meet the New Sonar-Perplexity AI’s Enhanced Search Experience" /><published>2025-02-11T00:00:00+02:00</published><updated>2025-02-11T00:00:00+02:00</updated><id>http://localhost:4000/PerplexitySonar</id><content type="html" xml:base="http://localhost:4000/PerplexitySonar">&lt;p&gt;Perplexity AI has unveiled the new Sonar, an advanced search technology designed to provide more accurate, relevant, and insightful responses. This update enhances how users interact with AI-driven search, improving precision and usability.&lt;/p&gt;

&lt;p&gt;Perplexity AI continues to redefine search by leveraging cutting-edge AI models, and the new Sonar represents a significant leap in this journey. By focusing on delivering highly relevant results with better contextual awareness, Sonar makes information retrieval faster and more intuitive for users. I don't know about you but I don't google any more, I use Perplexity instead.&lt;/p&gt;

&lt;p&gt;One of the key improvements in Sonar is its refined ranking system. By analyzing queries more intelligently, it prioritizes the most relevant and credible sources, reducing noise and misinformation. This ensures that users get high-quality answers without sifting through unnecessary data, and users do seem to enjoy it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://framerusercontent.com/images/YCALu2CFaq4OlGXciooTP0YucY.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another notable enhancement is the system’s improved ability to understand nuanced queries. Whether users are searching for technical information, general knowledge, or complex research topics, Sonar provides deeper insights by grasping context more effectively.&lt;/p&gt;

&lt;p&gt;Perplexity has also worked on making Sonar more interactive. With real-time response generation and dynamic follow-ups, users can refine their searches seamlessly. This feature mimics natural conversation, making AI-powered search feel more like an intelligent dialogue rather than a static lookup tool.&lt;/p&gt;

&lt;p&gt;In addition to accuracy, Sonar emphasizes transparency. It provides clear citations and source references, allowing users to verify information easily. This approach enhances trust in AI-generated responses, setting a new standard for responsible AI-driven search.&lt;/p&gt;

&lt;p&gt;By combining speed, accuracy, and contextual awareness, the new Sonar positions Perplexity AI as a leader in next-generation search technologies. This update ensures that users can access information more efficiently while maintaining the reliability and credibility they expect.&lt;/p&gt;

&lt;p&gt;As AI continues to evolve, innovations like Sonar demonstrate the potential of AI-driven search engines to enhance knowledge discovery. With its commitment to improving user experience, Perplexity AI is shaping the future of intelligent search, making it more intuitive, informative, and impactful.&lt;/p&gt;

&lt;p&gt;You can find further information and more details in the &lt;a href=&quot;https://www.perplexity.ai/hub/blog/meet-new-sonar&quot;&gt;official blog post&lt;/a&gt; and maybe try it our instead of using the usual way of googling staff up.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Perplexity AI has unveiled the new Sonar, an advanced search technology designed to provide more accurate, relevant, and insightful responses. This update enhances how users interact with AI-driven search, improving precision and usability.</summary></entry><entry><title type="html">Unveiling the Anthropic Economic Index-AI’s Real-World Impact on Labor Markets</title><link href="http://localhost:4000/AnthropicEconomicIndex" rel="alternate" type="text/html" title="Unveiling the Anthropic Economic Index-AI’s Real-World Impact on Labor Markets" /><published>2025-02-10T00:00:00+02:00</published><updated>2025-02-10T00:00:00+02:00</updated><id>http://localhost:4000/AnthropicEconomicIndex</id><content type="html" xml:base="http://localhost:4000/AnthropicEconomicIndex">&lt;p&gt;Anthropic has introduced the &lt;a href=&quot;https://www.anthropic.com/economic-index&quot;&gt;Anthropic Economic Index&lt;/a&gt;, a pioneering initiative aimed at analyzing AI's influence on labor markets and the broader economy. This inaugural report offers unprecedented insights into how AI is being integrated into various occupational tasks, based on extensive data from millions of anonymized interactions with Claude.ai.&lt;/p&gt;

&lt;p&gt;In February 2025, Anthropic launched the Anthropic Economic Index, an initiative designed to monitor and understand the evolving impact of artificial intelligence on labor markets and the economy. This effort is grounded in the analysis of millions of anonymized conversations with &lt;a href=&quot;http://claude.ai/&quot;&gt;Claude.ai&lt;/a&gt;, providing a unique perspective on AI's real-world applications across diverse professional sectors.&lt;/p&gt;

&lt;p&gt;The initial findings of the Index reveal that AI usage is predominantly concentrated in software development and technical writing tasks. Notably, over one-third of occupations (approximately 36%) incorporate AI in at least a quarter of their associated tasks, while about 4% of occupations utilize AI in three-quarters of their tasks. This indicates a significant, though varied, integration of AI into modern workflows.&lt;/p&gt;

&lt;p&gt;Furthermore, the data suggests that AI is more commonly employed for augmentation purposes—enhancing and collaborating with human capabilities—rather than for automation. Specifically, 57% of AI applications are aimed at augmenting human tasks, whereas 43% are focused on automating tasks. This trend underscores AI's role as a supportive tool that amplifies human productivity and creativity.&lt;/p&gt;

&lt;p&gt;AI adoption appears to be more prevalent in mid-to-high wage occupations, such as computer programmers and data scientists. Conversely, both the lowest and highest-paid roles exhibit lower levels of AI integration. This pattern likely reflects the current limitations of AI technologies and practical challenges associated with their implementation in certain job categories.&lt;/p&gt;

&lt;p&gt;To facilitate further research and policy development, Anthropic has &lt;a href=&quot;https://huggingface.co/datasets/Anthropic/EconomicIndex/&quot;&gt;open-sourced the dataset&lt;/a&gt; underpinning this analysis. By inviting economists, policy experts, and researchers to engage with the data, Anthropic aims to foster a collaborative approach to understanding and addressing the transformative effects of AI on employment and productivity.&lt;/p&gt;

&lt;p&gt;This comprehensive analysis builds upon existing economic research by focusing on occupational tasks rather than entire professions. Recognizing that many jobs share common tasks and skills, this approach provides a nuanced understanding of how AI is selectively adopted across different functions and industries.&lt;/p&gt;

&lt;p&gt;The research leverages &lt;a href=&quot;https://www.anthropic.com/research/clio&quot;&gt;Clio&lt;/a&gt;, an automated tool developed by Anthropic, to analyze conversations with Claude.ai while ensuring user privacy. By mapping these interactions to specific occupational tasks, classified according to the U.S. Department of Labor's Occupational Information Network (&lt;a href=&quot;https://www.onetonline.org/&quot;&gt;O*NET&lt;/a&gt;), the study offers a detailed examination of AI's role in the contemporary workforce.&lt;/p&gt;

&lt;p&gt;In summary, the Anthropic Economic Index serves as a vital resource for comprehending AI's current and potential impact on labor markets. By providing empirical data and fostering an open dialogue among stakeholders, this initiative aims to inform policy decisions and guide the responsible integration of AI into various economic sectors. Find our more about Antrhopic Economic Index &lt;a href=&quot;https://www.onetonline.org/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Anthropic has introduced the Anthropic Economic Index, a pioneering initiative aimed at analyzing AI's influence on labor markets and the broader economy. This inaugural report offers unprecedented insights into how AI is being integrated into various occupational tasks, based on extensive data from millions of anonymized interactions with Claude.ai.</summary></entry><entry><title type="html">GitHub Copilot - The Agent Awakens</title><link href="http://localhost:4000/GitAgent" rel="alternate" type="text/html" title="GitHub Copilot - The Agent Awakens" /><published>2025-02-06T00:00:00+02:00</published><updated>2025-02-06T00:00:00+02:00</updated><id>http://localhost:4000/GitAgent</id><content type="html" xml:base="http://localhost:4000/GitAgent">&lt;p&gt;GitHub has unveiled a groundbreaking &lt;a href=&quot;https://github.com/features/copilot/whats-new?utm_source=agent-awakens-announcement&amp;amp;utm_medium=blogtop&amp;amp;utm_campaign=agentic-ai&quot;&gt;update to Copilot&lt;/a&gt;: the new agent mode. This enhancement empowers Copilot to autonomously iterate on its code, identify errors, and rectify them, significantly boosting developer productivity.&lt;/p&gt;

&lt;iframe width=&quot;699&quot; height=&quot;393&quot; src=&quot;https://www.youtube.com/embed/of--3Fq1M3w&quot; title=&quot;Agent mode and new models in GitHub Copilot Chat: Visual Studio Code&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;GitHub announced the agent mode for Copilot, available in Visual Studio Code. This feature enables Copilot to suggest terminal commands and prompts developers to execute them. It also analyzes runtime errors with self-healing capabilities, allowing it to recognize and automatically fix issues in the code. This advancement transforms Copilot from a passive assistant into an active collaborator in the development process.&lt;/p&gt;

&lt;p&gt;Alongside agent mode, GitHub announced the general availability of Copilot Edits. This feature allows developers to make code changes more efficiently, streamlining the development process and reducing the time spent on manual edits. Copilot Edits leverages AI to suggest contextually relevant modifications, enhancing code quality and consistency.&lt;/p&gt;

&lt;p&gt;GitHub also provided a preview of their Software Engineering (SWE) agent. This agent is designed to assist developers by providing intelligent code suggestions, automating repetitive tasks, and improving overall code quality. The SWE agent represents a significant step forward in integrating AI into the software development lifecycle, offering developers a powerful tool to enhance their coding experience.&lt;/p&gt;

&lt;p&gt;With the introduction of agent mode, the general availability of Copilot Edits, and the preview of the SWE agent, GitHub is redefining the role of AI in software development. These innovations aim to make coding more efficient, intuitive, and collaborative, ushering in a new era of intelligent development tools. There are many more capabilities and resources in the &lt;a href=&quot;https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/&quot;&gt;official announcement&lt;/a&gt; for you to explore. So don't hacitate if you are interested and get your hands dirty!&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">GitHub has unveiled a groundbreaking update to Copilot: the new agent mode. This enhancement empowers Copilot to autonomously iterate on its code, identify errors, and rectify them, significantly boosting developer productivity.</summary></entry><entry><title type="html">Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2</title><link href="http://localhost:4000/Olymp" rel="alternate" type="text/html" title="Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2" /><published>2025-02-05T00:00:00+02:00</published><updated>2025-02-05T00:00:00+02:00</updated><id>http://localhost:4000/Olymp</id><content type="html" xml:base="http://localhost:4000/Olymp">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 &lt;a href=&quot;https://dpmd.ai/imo-silver&quot;&gt;this https URL&lt;/a&gt;. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chervonyi,+Y&quot; rel=&quot;nofollow&quot;&gt;Yuri Chervonyi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Trinh,+T+H&quot; rel=&quot;nofollow&quot;&gt;Trieu H. Trinh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ol%C5%A1%C3%A1k,+M&quot; rel=&quot;nofollow&quot;&gt;Miroslav Olšák&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+X&quot; rel=&quot;nofollow&quot;&gt;Xiaomeng Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Nguyen,+H&quot; rel=&quot;nofollow&quot;&gt;Hoang Nguyen&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Menegali,+M&quot; rel=&quot;nofollow&quot;&gt;Marcelo Menegali&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jung,+J&quot; rel=&quot;nofollow&quot;&gt;Junehyuk Jung&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Verma,+V&quot; rel=&quot;nofollow&quot;&gt;Vikas Verma&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Le,+Q+V&quot; rel=&quot;nofollow&quot;&gt;Quoc V. Le&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Luong,+T&quot; rel=&quot;nofollow&quot;&gt;Thang Luong&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2502.03544&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">s1-Simple test-time scaling</title><link href="http://localhost:4000/TestScaling" rel="alternate" type="text/html" title="s1-Simple test-time scaling" /><published>2025-01-31T00:00:00+02:00</published><updated>2025-01-31T00:00:00+02:00</updated><id>http://localhost:4000/TestScaling</id><content type="html" xml:base="http://localhost:4000/TestScaling">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending &quot;Wait&quot; multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at &lt;a href=&quot;https://github.com/simplescaling/s1&quot;&gt;this https URL&lt;/a&gt;. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muennighoff,+N&quot; rel=&quot;nofollow&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+Z&quot; rel=&quot;nofollow&quot;&gt;Zitong Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shi,+W&quot; rel=&quot;nofollow&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Li,+X+L&quot; rel=&quot;nofollow&quot;&gt;Xiang Lisa Li&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fei-Fei,+L&quot; rel=&quot;nofollow&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hajishirzi,+H&quot; rel=&quot;nofollow&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zettlemoyer,+L&quot; rel=&quot;nofollow&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liang,+P&quot; rel=&quot;nofollow&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Cand%C3%A8s,+E&quot; rel=&quot;nofollow&quot;&gt;Emmanuel Candès&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hashimoto,+T&quot; rel=&quot;nofollow&quot;&gt;Tatsunori Hashimoto&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2501.19393&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Mistral AI Unveils Mistral Small 3 - A High-Performance, Latency-Optimized Model</title><link href="http://localhost:4000/mistralsmall" rel="alternate" type="text/html" title="Mistral AI Unveils Mistral Small 3 - A High-Performance, Latency-Optimized Model" /><published>2025-01-30T00:00:00+02:00</published><updated>2025-01-30T00:00:00+02:00</updated><id>http://localhost:4000/mistralsmall</id><content type="html" xml:base="http://localhost:4000/mistralsmall">&lt;p&gt;Mistral AI has introduced Mistral Small 3, a 24-billion-parameter model optimized for low latency, delivering performance comparable to larger models while maintaining efficiency. Released under the Apache 2.0 license, it offers a versatile solution for various AI applications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mistral.ai/images/news/mistral-small-3/up-and-to-the-left.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On January 30, 2025, Mistral AI announced the release of Mistral Small 3, a latency-optimized model with 24 billion parameters. This model is designed to provide robust language understanding and instruction-following capabilities, catering to approximately 80% of generative AI tasks that demand quick and accurate responses. Notably, Mistral Small 3 achieves over 81% accuracy on the MMLU benchmark and processes 150 tokens per second, making it one of the most efficient models in its category.&lt;/p&gt;

&lt;p&gt;Mistral Small 3 stands out by delivering performance on par with larger models such as &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct&quot;&gt;Llama 3.3 70B&lt;/a&gt; and &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-llm/&quot;&gt;Qwen 32B&lt;/a&gt;, while operating more than three times faster on equivalent hardware. This efficiency is achieved through a streamlined architecture with fewer layers, reducing the time required for each forward pass. Both pretrained and instruction-tuned versions of the model are available under the Apache 2.0 license, providing a solid foundation for further development and customization.&lt;/p&gt;

&lt;p&gt;In evaluations conducted with an external third-party vendor, Mistral Small 3 was assessed alongside other models using over 1,000 proprietary coding and generalist prompts. Evaluators, who were unaware of which model generated each response, consistently preferred the outputs from Mistral Small 3. These assessments encompassed various domains, including code, mathematics, general knowledge, and instruction-following tasks, underscoring the model's versatility and effectiveness.&lt;/p&gt;

&lt;p&gt;Mistral Small 3 is well-suited for a range of applications:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Fast-Response Conversational Assistance:&lt;/strong&gt; Ideal for virtual assistants and scenarios where users expect immediate, accurate feedback.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Low-Latency Function Calling:&lt;/strong&gt; Capable of handling rapid function execution within automated workflows.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Fine-Tuning for Subject Matter Expertise:&lt;/strong&gt; Can be customized to specialize in specific domains, making it valuable for fields like legal advice, medical diagnostics, and technical support.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Local Inference:&lt;/strong&gt; When quantized, the model can run privately on hardware such as a single RTX 4090 or a MacBook with 32GB RAM, benefiting hobbyists and organizations managing sensitive information.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With the release of Mistral Small 3, Mistral AI continues its commitment to advancing open-source AI solutions. The model's combination of high performance, low latency, and versatility makes it a compelling choice for developers and organizations seeking efficient and customizable AI models for a variety of applications.&lt;/p&gt;

&lt;p&gt; To learn more, and find ways to get the most out of it, head to the &lt;a href=&quot;https://mistral.ai/news/mistral-small-3/&quot;&gt;official blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt; For those of you eager to get your hands dirty, it is said that there is the following collaborations with Hugging Face, Ollama, Kaggle, Together AI, and Fireworks AI to make the model available on their platforms:

&lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501&quot;&gt;Hugging Face&lt;/a&gt; (&lt;a href=&quot;https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501&quot;&gt;base model&lt;/a&gt;)&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://ollama.com/library/mistral-small&quot;&gt;Ollama&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/models/mistral-ai/mistral-small-24b&quot;&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.together.ai/models/mistral-small-3&quot;&gt;Together AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://fireworks.ai/models/fireworks/mistral-small-24b-instruct-2501&quot;&gt;Fireworks AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.ibm.com/products/watsonx-ai&quot;&gt;IBM watsonx&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;Coming soon on NVIDIA NIM, Amazon SageMaker, Groq, Databricks and Snowflake&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Mistral AI has introduced Mistral Small 3, a 24-billion-parameter model optimized for low latency, delivering performance comparable to larger models while maintaining efficiency. Released under the Apache 2.0 license, it offers a versatile solution for various AI applications.</summary></entry><entry><title type="html">DeepSeek-R1 Teams Up with NVIDIA NIM A Power Duo in AI</title><link href="http://localhost:4000/NVIDIADSR1" rel="alternate" type="text/html" title="DeepSeek-R1 Teams Up with NVIDIA NIM A Power Duo in AI" /><published>2025-01-30T00:00:00+02:00</published><updated>2025-01-30T00:00:00+02:00</updated><id>http://localhost:4000/NVIDIADSR1</id><content type="html" xml:base="http://localhost:4000/NVIDIADSR1">&lt;p&gt;DeepSeek-R1, a 671-billion-parameter model known for its exceptional reasoning capabilities, is now available as an NVIDIA NIM microservice. This collaboration promises to deliver high-speed, efficient AI solutions for complex tasks.&lt;/p&gt;

&lt;p&gt;DeepSeek-R1 stands out with its massive 671 billion parameters, significantly surpassing many existing open-source large language models. Its architecture features a mixture-of-experts design, with each layer comprising 256 experts. For every input token, the model engages eight experts in parallel, enabling advanced logical inference, reasoning, mathematics, coding, and language understanding. This design allows DeepSeek-R1 to handle an extensive input context of up to 128,000 tokens, making it adept at processing and generating lengthy and complex content.&lt;/p&gt;

&lt;p&gt;Integrating DeepSeek-R1 into NVIDIA's NIM microservice framework simplifies the deployment process for developers and enterprises. The NIM microservice supports industry-standard APIs, ensuring seamless integration into existing systems. By running the microservice on preferred accelerated computing infrastructure, organizations can maintain high standards of security and data privacy. Additionally, with &lt;a href=&quot;https://www.nvidia.com/en-us/ai/foundry/&quot;&gt;NVIDIA AI Foundry&lt;/a&gt; and &lt;a href=&quot;https://www.nvidia.com/en-us/ai-data-science/products/nemo/&quot;&gt;NeMo software&lt;/a&gt;, there's potential for creating customized &lt;a href=&quot;https://build.nvidia.com/deepseek-ai/deepseek-r1&quot;&gt;DeepSeek-R1 microservices&lt;/a&gt; tailored to specialized AI agents.&lt;/p&gt;

&lt;iframe width=&quot;733&quot; height=&quot;412&quot; src=&quot;https://www.youtube.com/embed/47DWCEzG1Cg&quot; title=&quot;DeepSeek-R1 in Action with NVIDIA NIM Microservices&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;When deployed on a single NVIDIA HGX H200 system equipped with eight H200 GPUs connected via NVLink and NVLink Switch, the DeepSeek-R1 NIM microservice achieves an impressive throughput of up to 3,872 tokens per second. This high performance is attributed to the NVIDIA Hopper architecture's FP8 Transformer Engine and the substantial NVLink bandwidth facilitating efficient communication among the model's experts. Looking ahead, the upcoming NVIDIA Blackwell architecture is expected to further enhance performance with its fifth-generation Tensor Cores, delivering up to 20 petaflops of peak FP4 compute performance and a 72-GPU NVLink domain optimized for inference.&lt;/p&gt;

&lt;p&gt;Developers eager to explore the capabilities of the DeepSeek-R1 NIM microservice can access a preview on &lt;a href=&quot;http://build.nvidia.com/&quot;&gt;NVIDIA's platform&lt;/a&gt;. The API is anticipated to be available soon as a downloadable NIM microservice, forming part of the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/products/ai-enterprise/&quot;&gt;NVIDIA AI Enterprise&lt;/a&gt; software suite. This development opens avenues for building specialized AI agents that leverage DeepSeek-R1's advanced reasoning capabilities, all within a secure and efficient deployment framework.&lt;/p&gt;

&lt;p&gt;The collaboration between DeepSeek-R1 and NVIDIA's NIM microservice framework marks a significant advancement in AI deployment. By combining a state-of-the-art reasoning model with a robust deployment infrastructure, this partnership offers developers and enterprises a powerful tool for implementing sophisticated AI solutions across various applications. Wanna find out more?! head to the &lt;a href=&quot;https://blogs.nvidia.com/blog/deepseek-r1-nim-microservice/&quot;&gt;official announcement&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">DeepSeek-R1, a 671-billion-parameter model known for its exceptional reasoning capabilities, is now available as an NVIDIA NIM microservice. This collaboration promises to deliver high-speed, efficient AI solutions for complex tasks.</summary></entry><entry><title type="html">Tülu 3 405B The New Heavyweight Champion in AI Models</title><link href="http://localhost:4000/tulu3" rel="alternate" type="text/html" title="Tülu 3 405B The New Heavyweight Champion in AI Models" /><published>2025-01-30T00:00:00+02:00</published><updated>2025-01-30T00:00:00+02:00</updated><id>http://localhost:4000/tulu3</id><content type="html" xml:base="http://localhost:4000/tulu3">&lt;p&gt;Hold onto your hats, all of you people! The Allen Institute for AI has just unleashed &lt;a href=&quot;https://allenai.org/blog/tulu-3-technical&quot;&gt;Tülu 3 405B&lt;/a&gt;, a colossal 405-billion-parameter model that's not just flexing its muscles but also outpacing the competition. This update is setting new benchmarks, leaving rivals like DeepSeek V3 and GPT-4o &lt;i&gt;in the dust&lt;/i&gt;, all while being as open-source as a community cookbook.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.datocms-assets.com/64837/1738270804-image-23.png?fit=max&amp;amp;fm=webp&amp;amp;h=810&amp;amp;w=1550&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On January 30, 2025, the Allen Institute for AI (Ai2) proudly introduced Tülu 3 405B, marking the first time fully open post-training recipes have been applied to such a massive open-weight model. Building upon the success of their previous Tülu 3 release, this model demonstrates that size does matter, especially when combined with innovative training techniques. Tülu 3 405B doesn't just compete; it often surpasses models like DeepSeek V3 and GPT-4o, and leaves prior open-weight models, including Llama 3.1 405B Instruct and Nous Hermes 3 405B, eating its dust on many standard benchmarks.&lt;/p&gt;

&lt;p&gt;So, how did Ai2 cook up this powerhouse? They scaled their proven Tülu 3 post-training recipe to the Llama-405B base model, following a five-step program:&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;&lt;strong&gt;Data Delicacy:&lt;/strong&gt; Carefully curating and synthesizing data targeting core skills.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Supervised Finetuning (SFT):&lt;/strong&gt; Applying a finely selected mix of prompts and their completions.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Direct Preference Optimization (DPO):&lt;/strong&gt; Tweaking the model based on both off- and on-policy preference data.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Reinforcement Learning with Verifiable Rewards (RLVR):&lt;/strong&gt; A fresh, RL-based method to boost specific skills with rewards you can take to the bank.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Evaluation Extravaganza:&lt;/strong&gt; Implementing a standardized suite for development, decontamination, and final evaluation stages.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the training arena, Ai2 employed their novel RLVR approach, focusing on tasks with clear-cut answers like math problems and precise instructions. To handle the 405B scale, they deployed the model using vLLM with 16-way tensor parallelism, utilizing 240 GPUs for training. Each RLVR iteration involved approximately 550 seconds for inference, 25 seconds for weight transfer, and 1500 seconds for training. Interestingly, they discovered that feeding the model exclusively MATH data, rather than a mix, yielded better results at this scale—a testament to the model's appetite for complex, specialized tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.datocms-assets.com/64837/1738199366-rlvr-diagram.png?dpr=2&amp;amp;fit=max&amp;amp;fm=webp&amp;amp;h=810&amp;amp;w=1550&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tülu 3 405B isn't just big; it's a top performer. It achieves competitive or superior results compared to both DeepSeek V3 and GPT-4o, while also surpassing prior open-weight post-trained models of the same size on many standard benchmarks. Notably, the Reinforcement Learning from Verifiable Rewards (RLVR) framework improved MATH performance more significantly at this larger scale, echoing findings in the DeepSeek-R1 report. Overall, Tülu 3 405B shows a consistent edge over DeepSeek V3, especially when safety benchmarks are in play.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.datocms-assets.com/64837/1738199543-405b_rl.png?dpr=2&amp;amp;fit=max&amp;amp;fm=webp&amp;amp;h=810&amp;amp;w=1550&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With Tülu 3 405B, Ai2 has not only scaled up their model parameters but also the impact of open-source AI. This release underscores the scalability and effectiveness of their post-training recipe at an unprecedented scale, setting a new standard in the AI community. As Tülu 3 405B continues to flex its capabilities, it's clear that the future of AI is not just about bigger models, but smarter, more open ones too. To get started and find out more detailes about the afforementioned updates you can visit official &lt;a href=&quot;https://allenai.org/blog/tulu-3-405B&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Hold onto your hats, all of you people! The Allen Institute for AI has just unleashed Tülu 3 405B, a colossal 405-billion-parameter model that's not just flexing its muscles but also outpacing the competition. This update is setting new benchmarks, leaving rivals like DeepSeek V3 and GPT-4o in the dust, all while being as open-source as a community cookbook.</summary></entry><entry><title type="html">Learn-by-interact A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments</title><link href="http://localhost:4000/DataCleric" rel="alternate" type="text/html" title="Learn-by-interact A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments" /><published>2025-01-18T00:00:00+02:00</published><updated>2025-01-18T00:00:00+02:00</updated><id>http://localhost:4000/DataCleric</id><content type="html" xml:base="http://localhost:4000/DataCleric">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis. The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the corresponding environments they interact with. We propose Learn-by-interact, a data-centric framework to adapt LLM agents to any given environments without human annotations. Learn-by-interact synthesizes trajectories of agent-environment interactions based on documentations, and constructs instructions by summarizing or abstracting the interaction histories, a process called backward construction. We assess the quality of our synthetic data by using them in both training-based scenarios and training-free in-context learning (ICL), where we craft innovative retrieval approaches optimized for agents. Extensive experiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across realistic coding, web, and desktop environments show the effectiveness of Learn-by-interact in various downstream agentic tasks -- baseline results are improved by up to 12.2\% for ICL with Claude-3.5 and 19.5\% for training with Codestral-22B. We further demonstrate the critical role of backward construction, which provides up to 14.0\% improvement for training. Our ablation studies demonstrate the efficiency provided by our synthesized data in ICL and the superiority of our retrieval pipeline over alternative approaches like conventional retrieval-augmented generation (RAG). We expect that Learn-by-interact will serve as a foundation for agent data synthesis as LLMs are increasingly deployed at real-world environments. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Su,+H&quot; rel=&quot;nofollow&quot;&gt;Hongjin Su&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sun,+R&quot; rel=&quot;nofollow&quot;&gt;Ruoxi Sun&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yoon,+J&quot; rel=&quot;nofollow&quot;&gt;Jinsung Yoon&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yin,+P&quot; rel=&quot;nofollow&quot;&gt;Pengcheng Yin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+T&quot; rel=&quot;nofollow&quot;&gt;Tao Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ar%C4%B1k,+S+%C3%96&quot; rel=&quot;nofollow&quot;&gt;Sercan Ö. Arık&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2501.10893&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry></feed>