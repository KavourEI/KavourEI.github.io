<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-04T16:28:10+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">SynthID:Revolutionizing AI Text Watermarking</title><link href="http://localhost:4000/SynthID" rel="alternate" type="text/html" title="SynthID:Revolutionizing AI Text Watermarking" /><published>2024-10-23T00:00:00+03:00</published><updated>2024-10-23T00:00:00+03:00</updated><id>http://localhost:4000/SynthID</id><content type="html" xml:base="http://localhost:4000/SynthID">&lt;p&gt;SynthID, developed by Google DeepMind, is an innovative watermarking technology designed to identify AI-generated text while maintaining the quality and integrity of the content. Open-sourced for broader accessibility, SynthID aims to enhance transparency and trust in AI-generated materials across various platforms.&lt;/p&gt;

&lt;p&gt;As artificial intelligence continues to permeate various aspects of our lives, the challenge of distinguishing between human-generated and AI-generated content has become increasingly pressing. The rise of misinformation, plagiarism, and copyright issues necessitates robust tools to verify content authenticity. In response to these challenges, Google DeepMind has launched SynthID, a watermarking technology that aims to identify AI-generated text effectively.&lt;/p&gt;

&lt;p&gt;SynthID is a watermarking tool developed by Google DeepMind in collaboration with Hugging Face. Initially launched for images and videos, it has now been expanded to include AI-generated text. This tool embeds an imperceptible digital watermark directly into the text generated by specific language models (LLMs), allowing for easy identification without compromising the quality or fluency of the output.&lt;/p&gt;

&lt;p&gt;The core functionality of SynthID lies in its ability to adjust the probability scores of tokens—units of text such as characters or words—during the generation process. By subtly modifying these scores, SynthID embeds a unique watermark within the text. This adjustment is executed in a way that does not alter the overall quality or coherence of the generated content.&lt;/p&gt;

&lt;p&gt;Large language models generate text by predicting the next token based on preceding words. Each potential token is assigned a probability score reflecting its likelihood of being chosen. SynthID modifies these scores for specific tokens to create a distinctive pattern that serves as a watermark. This technique ensures that even as the model generates coherent sentences, the embedded watermark remains intact and detectable.&lt;/p&gt;

&lt;p&gt;Recently, Google DeepMind announced that SynthID would be available as open-source software through its &lt;a href=&quot;https://ai.google.dev/responsible/docs/safeguards/synthid&quot;&gt;Responsible Generative AI Toolkit&lt;/a&gt;. This decision aims to broaden the technology's compatibility with various tools and platforms, allowing other developers to integrate it into their own models. This initiative will enable more developers to build AI responsibly by providing them with essential tools for identifying AI-generated content.&lt;/p&gt;

&lt;p&gt;A significant aspect of SynthID's development involved extensive testing to ensure that the watermarking process did not degrade the quality of AI-generated text. In a study analyzing approximately 20 million chatbot responses, researchers found no noticeable difference in quality or usefulness between watermarked and unwatermarked outputs. However, limitations exist; for instance, SynthID's effectiveness diminishes when dealing with factual prompts or when generated text undergoes significant rewriting or translation.&lt;/p&gt;

&lt;p&gt;Achieving reliable watermarking for AI-generated text presents unique challenges. The technique works best with longer responses where there are multiple opportunities to adjust token probabilities without compromising factual accuracy or coherence. However, in scenarios where outputs are deterministic—like answering straightforward factual questions—the watermarking may not be as effective.&lt;/p&gt;

&lt;p&gt;The ability to identify AI-generated content is crucial for promoting trust in information sources. While SynthID is not a comprehensive solution to issues like misinformation or misattribution, it represents a significant step toward developing reliable identification tools for AI outputs. As generative AI becomes more prevalent, ensuring transparency in content creation will be vital for maintaining public trust.&lt;/p&gt;

&lt;p&gt;SynthID stands at the forefront of efforts to address the challenges posed by AI-generated content. By providing an open-source solution for watermarking text, Google DeepMind not only enhances transparency but also empowers developers across various sectors to create responsible AI applications. As this technology evolves, it holds promise for improving how we interact with and perceive AI-generated materials.&lt;/p&gt;

&lt;p&gt; To read full article, and find out all the needy-greedy details about SynthID, you can visit Google's Deep Mind &lt;a href=&quot;https://deepmind.google/technologies/synthid/&quot;&gt;official blog post&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">SynthID, developed by Google DeepMind, is an innovative watermarking technology designed to identify AI-generated text while maintaining the quality and integrity of the content. Open-sourced for broader accessibility, SynthID aims to enhance transparency and trust in AI-generated materials across various platforms.</summary></entry><entry><title type="html">Introducing Multimodal Embed 3:Enhancing AI Search Capabilities</title><link href="http://localhost:4000/Cohere" rel="alternate" type="text/html" title="Introducing Multimodal Embed 3:Enhancing AI Search Capabilities" /><published>2024-10-22T00:00:00+03:00</published><updated>2024-10-22T00:00:00+03:00</updated><id>http://localhost:4000/Cohere</id><content type="html" xml:base="http://localhost:4000/Cohere">&lt;p&gt;Cohere has launched Multimodal Embed 3, a cutting-edge multimodal AI search model that significantly enhances the ability to search and analyze image data. This model aims to unlock real business value by improving the integration of text and image data, paving the way for more effective AI-driven applications.&lt;/p&gt;

&lt;p&gt;On October 22, 2024, Cohere introduced Multimodal Embed 3, a state-of-the-art AI model designed to revolutionize how businesses interact with image data. As organizations increasingly rely on visual content, the need for advanced search capabilities that can effectively combine text and image data has become paramount. Multimodal Embed 3 addresses this challenge by providing a robust framework for integrating and analyzing diverse data types, ultimately enhancing the search experience and driving business value.&lt;/p&gt;

&lt;p&gt;Multimodal Embed 3 boasts several innovative features that set it apart from previous models. The model is designed to seamlessly integrate text and image embeddings, allowing for more nuanced understanding and retrieval of information across different modalities. By leveraging advanced neural network architectures, it enhances the accuracy of search results while maintaining high performance across various tasks. Furthermore, the model is optimized for scalability, enabling businesses to handle large volumes of image data efficiently. This capability is crucial for organizations looking to implement AI-driven solutions that require real-time processing and analysis of multimedia content.&lt;/p&gt;

&lt;p&gt;The introduction of Multimodal Embed 3 opens up new possibilities for businesses across various sectors. For instance, in e-commerce, retailers can enhance product discovery by allowing customers to search using images instead of text alone. This capability not only improves user experience but also increases conversion rates by making it easier for customers to find relevant products. In marketing and advertising, brands can analyze visual content alongside textual data to gain deeper insights into consumer behavior and preferences, enabling more targeted campaigns.&lt;/p&gt;

&lt;p&gt;Cohere’s team has focused on refining the underlying technology of Multimodal Embed 3 to ensure it delivers superior performance compared to its predecessors. The model employs state-of-the-art techniques in machine learning and artificial intelligence, including attention mechanisms that allow it to focus on relevant features within both text and images. This attention to detail enhances its ability to generate accurate embeddings that represent complex relationships between different data types.&lt;/p&gt;

&lt;p&gt;As Cohere continues to innovate in the field of multimodal AI, future iterations of the Embed model are expected to incorporate even more advanced functionalities. The company aims to explore additional applications beyond traditional search capabilities, such as real-time image recognition and contextual understanding of visual content in dynamic environments. By continuously improving their models, Cohere is committed to providing businesses with the tools they need to harness the full potential of their data.&lt;/p&gt;

&lt;p&gt;The launch of Multimodal Embed 3 represents a significant advancement in AI search technology, particularly in how businesses can leverage image data alongside text. With its powerful integration capabilities and high-performance architecture, this model is poised to transform various industries by enhancing user experiences and driving actionable insights from multimedia content. As organizations increasingly adopt AI-driven solutions, Cohere’s innovations will play a crucial role in shaping the future of multimodal interactions.&lt;/p&gt;

&lt;p&gt;For more information about Multimodal Embed 3 and its potential applications in your business, visit the &lt;a href=&quot;https://cohere.com/blog/multimodal-embed-3&quot;&gt;official blog post&lt;/a&gt;. In case you want to explore their great work visit &lt;a href=&quot;https://cohere.com/&quot;&gt;Cohere's official website&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Cohere has launched Multimodal Embed 3, a cutting-edge multimodal AI search model that significantly enhances the ability to search and analyze image data. This model aims to unlock real business value by improving the integration of text and image data, paving the way for more effective AI-driven applications.</summary></entry><entry><title type="html">Introducing Internal Knowledge Search and Spaces by Perplexity</title><link href="http://localhost:4000/perplexity" rel="alternate" type="text/html" title="Introducing Internal Knowledge Search and Spaces by Perplexity" /><published>2024-10-17T00:00:00+03:00</published><updated>2024-10-17T00:00:00+03:00</updated><id>http://localhost:4000/perplexity</id><content type="html" xml:base="http://localhost:4000/perplexity">&lt;p&gt;Perplexity AI has launched Internal Knowledge Search and Spaces, innovative features designed to enhance information retrieval and collaboration within teams. These tools aim to streamline access to knowledge and improve productivity by allowing users to create customized environments for their specific needs.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/LqKZAHeCkEg?si=Lbi6TDCAYDz_0fvn&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In an era where information overload is a common challenge, Perplexity AI has introduced two groundbreaking features: &lt;strong&gt;Internal Knowledge Search&lt;/strong&gt; and &lt;strong&gt;Spaces&lt;/strong&gt;. These tools are designed to empower users by enhancing their ability to find relevant information quickly and collaborate effectively within their teams. By focusing on personalized knowledge management, Perplexity AI aims to transform how individuals and organizations interact with data.&lt;/p&gt;

&lt;p&gt;Effective knowledge management is crucial for organizations looking to maintain a competitive edge. As teams grow and projects become more complex, the ability to access and share information seamlessly becomes paramount. Internal Knowledge Search addresses this need by providing a powerful search capability that allows users to sift through vast amounts of data effortlessly. This feature not only enhances individual productivity but also fosters a culture of collaboration as team members can easily share insights and findings.&lt;/p&gt;

&lt;p&gt;The Internal Knowledge Search feature enables users to conduct deep searches across internal documents, notes, and other resources, making it easier to locate specific information quickly. This capability is enhanced by advanced filtering options that allow users to refine their searches based on various criteria. Meanwhile, Spaces offers a customizable environment where teams can organize their projects, share resources, and collaborate in real-time. Users can create dedicated spaces for different projects or topics, ensuring that relevant information is always at hand. Together, these features create a cohesive ecosystem that enhances both individual and team productivity.&lt;/p&gt;

&lt;p&gt;The integration of Internal Knowledge Search into the Perplexity platform is designed with user experience in mind. Users can initiate searches using natural language queries, making the process intuitive and accessible even for those who may not be tech-savvy. The results are displayed in a user-friendly format, allowing individuals to quickly identify the most pertinent information. In addition, the Spaces feature enables teams to set up their workspaces tailored to their specific workflows, facilitating smoother collaboration and communication.&lt;/p&gt;

&lt;p&gt;Organizations across various sectors can benefit from these new features. For instance, research teams can utilize Internal Knowledge Search to locate relevant studies or data points swiftly, while marketing teams can leverage Spaces to brainstorm ideas and track campaign progress in a centralized location. By streamlining access to information and enhancing collaborative efforts, these tools empower teams to work more efficiently and effectively.&lt;/p&gt;

&lt;p&gt;As Perplexity AI continues to innovate, the introduction of Internal Knowledge Search and Spaces represents a significant step toward improving how individuals manage knowledge and collaborate within teams. The company is committed to refining these features based on user feedback, ensuring they meet the evolving needs of its user base. With these tools at their disposal, organizations can look forward to a future where accessing and sharing knowledge becomes second nature.&lt;/p&gt;

&lt;p&gt;The launch of Internal Knowledge Search and Spaces by Perplexity AI marks an important milestone in the realm of knowledge management and collaboration tools. By focusing on user-friendly interfaces and powerful search capabilities, these features are set to transform how teams interact with information. As organizations strive for greater efficiency in their operations, the ability to harness internal knowledge effectively will be crucial for success.&lt;/p&gt;

&lt;p&gt;To to explore these new features further, visit Perplexity AI's official &amp;lt; a href='https://www.perplexity.ai/hub/blog/introducing-internal-knowledge-search-and-spaces?fob=fX3q7wR73cDzq5Dk'&amp;gt;blog post&amp;lt;/a&amp;gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Perplexity AI has launched Internal Knowledge Search and Spaces, innovative features designed to enhance information retrieval and collaboration within teams. These tools aim to streamline access to knowledge and improve productivity by allowing users to create customized environments for their specific needs.</summary></entry><entry><title type="html">Introducing the prompt() Function:Enhancing SQL with LLMs</title><link href="http://localhost:4000/LLMwSQL" rel="alternate" type="text/html" title="Introducing the prompt() Function:Enhancing SQL with LLMs" /><published>2024-10-17T00:00:00+03:00</published><updated>2024-10-17T00:00:00+03:00</updated><id>http://localhost:4000/LLMwSQL</id><content type="html" xml:base="http://localhost:4000/LLMwSQL">&lt;p&gt;MotherDuck has launched the &lt;code&gt;prompt()&lt;/code&gt; function, a groundbreaking feature that integrates large language models (LLMs) directly into SQL, allowing users to generate, summarize, and extract structured data efficiently. This new capability democratizes access to advanced natural language processing techniques, making it easier for developers to leverage AI in their data workflows.&lt;/p&gt;

&lt;p&gt;In recent years, the costs associated with running large language models (LLMs) have significantly decreased, making advanced natural language processing techniques more accessible. The emergence of small language models (SLMs) like OpenAI’s gpt-4o-mini has further contributed to this trend, enabling cost-effective integration of AI capabilities into various applications. MotherDuck is excited to announce the introduction of the &lt;code&gt;prompt()&lt;/code&gt; function, which simplifies the use of LLMs and SLMs within SQL queries, allowing users to generate and summarize text as well as extract structured data without needing separate infrastructure.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;prompt()&lt;/code&gt; function currently supports OpenAI's gpt-4o-mini and gpt-4o models, providing flexibility in terms of cost-effectiveness and performance. In its preview release, users can apply gpt-4o-mini-based prompts to all rows in a table, unlocking use cases such as bulk text summarization and structured data extraction. The function allows for single-row and constant inputs with gpt-4o, enabling high-quality responses ideal for retrieval-augmented generation (RAG) scenarios. Users can specify the model for inference using the optional (model:=) parameter, which enhances customization based on specific needs. Additionally, the prompt function supports returning structured output through parameters like &lt;stron g=&quot;&quot;&gt;struct&amp;lt;/strong&amp;gt; and &lt;strong&gt;struct_descr&lt;/strong&gt;, facilitating integration into analytical workflows.&amp;lt;/p&amp;gt;

&lt;p&gt;The &lt;code&gt;prompt()&lt;/code&gt; function is designed for various practical applications. One prominent use case is text summarization; for instance, users can retrieve concise summaries from a text column in a dataset by executing a simple SQL query that applies the prompt function across multiple rows. In testing, processing 100 rows took approximately 2.8 seconds, showcasing significant speed improvements compared to traditional methods that would take hours without concurrency. Another compelling application is converting unstructured data into structured formats. By utilizing the struct and struct_descr parameters, users can extract specific information—such as topics, sentiments, and technologies mentioned in comments—transforming raw text into easily analyzable structured outputs.&lt;/p&gt;

&lt;p&gt;The performance of the &lt;code&gt;prompt()&lt;/code&gt; function is noteworthy; it allows for concurrent processing of multiple requests to enhance efficiency significantly. For example, when extracting structured information from a dataset like Hacker News, users can achieve high accuracy while maintaining reasonable compute costs. However, it is essential to consider practical limitations; for certain tasks like extracting email addresses from text, traditional SQL methods such as regex may be more efficient and reliable than using LLMs.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;prompt()&lt;/code&gt; function is currently available in Preview for MotherDuck users on both Free Trial and Standard Plans. To start exploring this functionality, users are encouraged to consult the &lt;a href=&quot;https://motherduck.com/docs/sql-reference/motherduck-sql-reference/ai-functions/prompt/&quot;&gt;documentation&lt;/a&gt; provided by MotherDuck. Notably, while running the &lt;code&gt;prompt()&lt;/code&gt; function over large datasets can incur higher compute costs compared to other analytical queries, MotherDuck has set quotas to manage usage effectively—Free Trial users receive 40 compute unit hours per day while Standard Plan users have similar limits that can be adjusted upon request.&lt;/p&gt;

&lt;p&gt;The introduction of the &lt;code&gt;prompt()&lt;/code&gt; function represents a significant advancement in integrating LLMs with SQL databases. By enabling users to harness the power of AI within their existing workflows seamlessly, MotherDuck is paving the way for more efficient data processing and analysis. As developers explore this new functionality, they are invited to share their experiences and feedback through community channels to foster collaboration and innovation.&lt;/p&gt;

&lt;p&gt;For further details on how to utilize the prompt() function and its capabilities, visit MotherDuck's &lt;a href=&quot;https://motherduck.com/&quot;&gt;official website&lt;/a&gt; or check &lt;a href=&quot;https://motherduck.com/blog/sql-llm-prompt-function-gpt-models&quot;&gt;official announcement&lt;/a&gt;.&lt;/p&gt;
&lt;/stron&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">MotherDuck has launched the prompt() function, a groundbreaking feature that integrates large language models (LLMs) directly into SQL, allowing users to generate, summarize, and extract structured data efficiently. This new capability democratizes access to advanced natural language processing techniques, making it easier for developers to leverage AI in their data workflows.</summary></entry><entry><title type="html">PyTorch 2.5 Speeding things up</title><link href="http://localhost:4000/PyTorchUpdate" rel="alternate" type="text/html" title="PyTorch 2.5 Speeding things up" /><published>2024-10-17T00:00:00+03:00</published><updated>2024-10-17T00:00:00+03:00</updated><id>http://localhost:4000/PyTorchUpdate</id><content type="html" xml:base="http://localhost:4000/PyTorchUpdate">&lt;p&gt;The release of PyTorch 2.5 introduces significant enhancements, including a new CuDNN backend for SDPA and optimizations in the TorchInductor CPU backend. These updates aim to improve performance and streamline user experience across various machine learning tasks.&lt;/p&gt;

&lt;h3&gt;Key Features and Insights&lt;/h3&gt;

&lt;h3&gt;Beta Features&lt;/h3&gt;

&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;CuDNN Backend for SDPA:&lt;/strong&gt; This new backend offers speed improvements by default on NVIDIA H100 GPUs, achieving up to 75% faster performance compared to FlashAttentionV2.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Regional Compilation:&lt;/strong&gt; The torch.compile feature allows for regional compilation of repeated nn.Module instances without recompilation, reducing cold startup times while maintaining performance.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;TorchInductor CPU Backend Optimization:&lt;/strong&gt; Enhancements include CPP backend code generation and support for various data types, delivering consistent performance improvements across multiple benchmark suites.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Prototype Features&lt;/h3&gt;

&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;FlexAttention:&lt;/strong&gt; A flexible API for implementing diverse attention mechanisms with improved memory efficiency.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Compiled Autograd:&lt;/strong&gt; Captures the entire backward pass, allowing for deferred tracing that is resilient to forward pass disruptions.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Flight Recorder:&lt;/strong&gt; A debugging tool that helps identify issues in stuck jobs by capturing runtime information.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Max-autotune Support:&lt;/strong&gt; Profiles multiple implementations at compile time to select the best-performing one for GEMM operations.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Enhanced Intel GPU Support:&lt;/strong&gt; Improved support for Intel GPUs to accelerate machine learning workflows on both Data Center and Client GPUs.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;FP16 Support:&lt;/strong&gt; Float16 is now supported on CPU paths for both eager mode and TorchInductor, enhancing performance in neural network tasks.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Autoload Device Extension:&lt;/strong&gt; Streamlines integration of out-of-tree device extensions by automating loading processes.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;TorchInductor on Windows:&lt;/strong&gt; The Inductor CPU backend is now compatible with Windows environments, supporting multiple compilers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This release comprises 4095 commits from 504 contributors, reflecting the ongoing commitment of the PyTorch community to enhance its capabilities.&lt;/p&gt;

&lt;p&gt;To read full article and explore all new updates provided by this update check the &lt;a href=&quot;https://pytorch.org/blog/pytorch2-5/&quot;&gt;official blog post&lt;/a&gt; by PyTorch&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">The release of PyTorch 2.5 introduces significant enhancements, including a new CuDNN backend for SDPA and optimizations in the TorchInductor CPU backend. These updates aim to improve performance and streamline user experience across various machine learning tasks.</summary></entry><entry><title type="html">Agent-as-a-Judge:Evaluate Agents with Agents</title><link href="http://localhost:4000/AgentJudge" rel="alternate" type="text/html" title="Agent-as-a-Judge:Evaluate Agents with Agents" /><published>2024-10-16T00:00:00+03:00</published><updated>2024-10-16T00:00:00+03:00</updated><id>http://localhost:4000/AgentJudge</id><content type="html" xml:base="http://localhost:4000/AgentJudge">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhuge,+M&quot;&gt;Mingchen Zhuge&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhao,+C&quot;&gt;Changsheng Zhao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ashley,+D&quot;&gt;Dylan Ashley&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wang,+W&quot;&gt;Wenyi Wang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Khizbullin,+D&quot;&gt;Dmitrii Khizbullin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xiong,+Y&quot;&gt;Yunyang Xiong&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liu,+Z&quot;&gt;Zechun Liu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chang,+E&quot;&gt;Ernie Chang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Krishnamoorthi,+R&quot;&gt;Raghuraman Krishnamoorthi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tian,+Y&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shi,+Y&quot;&gt;Yangyang Shi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chandra,+V&quot;&gt;Vikas Chandra&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Schmidhuber,+J&quot;&gt;Jürgen Schmidhuber&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.10934&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models</title><link href="http://localhost:4000/TimeConsistencyModels" rel="alternate" type="text/html" title="Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models" /><published>2024-10-16T00:00:00+03:00</published><updated>2024-10-16T00:00:00+03:00</updated><id>http://localhost:4000/TimeConsistencyModels</id><content type="html" xml:base="http://localhost:4000/TimeConsistencyModels">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lu,+C&quot; rel=&quot;nofollow&quot;&gt;Cheng Lu&lt;/a&gt;
, 
&lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Song,+Y&quot; rel=&quot;nofollow&quot;&gt;Yang Song&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.11081&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Thinking LLMs:General Instruction Following with Thought Generation</title><link href="http://localhost:4000/ThinkingLLM" rel="alternate" type="text/html" title="Thinking LLMs:General Instruction Following with Thought Generation" /><published>2024-10-14T00:00:00+03:00</published><updated>2024-10-14T00:00:00+03:00</updated><id>http://localhost:4000/ThinkingLLM</id><content type="html" xml:base="http://localhost:4000/ThinkingLLM">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning &amp;amp; problem-solving tasks. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wu,+T&quot;&gt;Tianhao Wu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lan,+J&quot;&gt;Janice Lan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yuan,+W&quot;&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jiao,+J&quot;&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Weston,+J&quot;&gt;Jason Weston&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Sukhbaatar,+S&quot;&gt;Sainbayar Sukhbaatar&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2410.10630&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Adobe Launches Firefly Video Model</title><link href="http://localhost:4000/AdobeFirefly" rel="alternate" type="text/html" title="Adobe Launches Firefly Video Model" /><published>2024-10-14T00:00:00+03:00</published><updated>2024-10-14T00:00:00+03:00</updated><id>http://localhost:4000/AdobeFirefly</id><content type="html" xml:base="http://localhost:4000/AdobeFirefly">&lt;p&gt;Adobe has unveiled the Firefly Video Model, expanding its suite of generative AI tools to include video capabilities designed for commercial safety. With enhanced features for video editing and integration into popular Adobe applications, this model aims to revolutionize content creation for professionals.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/puEgugluadk?si=P4KsYVEHdZMkaaMC&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;On October 14, 2024, during Adobe MAX, the world’s largest creativity conference, Adobe announced the launch of the Firefly Video Model (beta). This model marks a significant expansion of Adobe's Firefly family of generative AI models, which has already gained traction in image generation since its initial release. With over 13 billion images generated since March 2023, the Firefly suite is rapidly becoming a go-to resource for creative professionals.&lt;/p&gt;

&lt;p&gt;The Firefly Video Model is the first publicly available video model explicitly designed to be safe for commercial use. This innovative tool allows creators to generate and edit video content efficiently while ensuring compliance with licensing and copyright considerations. As part of its beta launch, Adobe is gathering feedback from a select group of creative professionals to refine the model further.&lt;/p&gt;

&lt;p&gt;The Firefly Video Model includes several standout features that enhance its functionality. The &lt;strong&gt;&lt;a href=&quot;https://blog.adobe.com/en/publish/2024/10/14/generative-extend-in-premiere-pro&quot;&gt;Generative Extend&lt;/a&gt;&lt;/strong&gt; feature allows users to seamlessly extend video clips, filling gaps in footage and smoothing transitions within Adobe Premiere Pro. Additionally, users can generate videos directly from text prompts with controls for camera angles and motion through the &lt;strong&gt;Text to Video&lt;/strong&gt; feature. The &lt;strong&gt;Image to Video&lt;/strong&gt; capability enables creators to transform still images or illustrations into dynamic video clips, adding a new dimension to their visual storytelling. Furthermore, the latest Firefly Image 3 model offers image generation that is up to four times faster than previous iterations, while the &lt;strong&gt;Generative Workspace in &lt;a href=&quot;https://blog.adobe.com/en/publish/2024/10/14/photoshop-delivers-powerful-innovation-for-image-editing-ideation-3d-design-more&quot;&gt;Photoshop&lt;/a&gt;&lt;/strong&gt; fosters creativity and collaboration by enabling designers to brainstorm and iterate concepts quickly.&lt;/p&gt;

&lt;p&gt;The Firefly Video Model integrates seamlessly with various Adobe applications including Photoshop, Illustrator, and Premiere Pro. This integration enhances workflows by allowing users to leverage generative AI capabilities across different platforms. For instance, the Generative Shape Fill and Text to Pattern features in Illustrator empower designers to create unique artwork rapidly.&lt;/p&gt;

&lt;p&gt;Adobe also introduced new offerings aimed at enterprises through Firefly Services. These include:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://blog.adobe.com/en/publish/2024/10/14/oliver-supercharges-its-ai-content-creation-with-help-from-adobe-firefly-services&quot;&gt;Dubbing and Lip Sync&lt;/a&gt;:&lt;/strong&gt; This feature uses generative AI to translate spoken dialogue into multiple languages while maintaining the original voice's sound and lip movements.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Bulk Create:&lt;/strong&gt; Aimed at enhancing efficiency, this tool allows creative professionals to edit large volumes of images quickly, streamlining tasks such as resizing and background removal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Leading brands like PepsiCo/Gatorade, IBM, Mattel, and Deloitte have already utilized Adobe Firefly to optimize their workflows and scale content creation. By automating repetitive tasks, these companies can focus more on exploring their creative visions rather than getting bogged down by technical details.&lt;/p&gt;

&lt;p&gt;Adobe emphasizes responsible innovation through its &lt;a href=&quot;https://www.adobe.com/ai/overview/ethics.html&quot;&gt;AI Ethics principles—accountability&lt;/a&gt;, responsibility, and transparency. The company has trained its Firefly models on licensed content from Adobe Stock and public domain sources. Furthermore, Adobe promotes Content Credentials as a standard for transparency in digital content creation, allowing users to understand how content was generated or modified.&lt;/p&gt;

&lt;p&gt;The Firefly Video Model is currently available in a limited public beta at firefly.adobe.com (there is a waiting list &lt;a href=&quot;https://www.adobe.com/products/firefly/features/ai-video-generator.html&quot;&gt;here&lt;/a&gt;). During this phase, generations are free of charge. Adobe plans to provide additional information regarding pricing and offers once the model moves beyond beta testing.&lt;/p&gt;

&lt;p&gt;The launch of the Firefly Video Model represents a significant leap forward in generative AI tools for creative professionals. By integrating advanced video capabilities into its existing suite of applications, Adobe is empowering users to create high-quality content more efficiently than ever before. As feedback from the beta phase rolls in, Adobe aims to refine these tools further, solidifying its position as a leader in creative technology.&lt;/p&gt;

&lt;p&gt;For more information about the Firefly Video Model and other innovations from &amp;lt;a href=https://cts.businesswire.com/ct/CT?id=smartlink&amp;amp;url=http%3A%2F%2Fwww.adobe.com&amp;amp;esheet=53380676&amp;amp;newsitemid=20230413005364&amp;amp;lan=en-US&amp;amp;anchor=www.adobe.com&amp;amp;index=10&amp;amp;md5=415c333dd2658050a4e133a58c1983ec'&amp;gt;Adobe&amp;lt;/a&amp;gt;, visit their &lt;a href=&quot;https://news.adobe.com/news/2024/10/101424-adobe-launches-firefly-video-model&quot;&gt;official website&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Adobe has unveiled the Firefly Video Model, expanding its suite of generative AI tools to include video capabilities designed for commercial safety. With enhanced features for video editing and integration into popular Adobe applications, this model aims to revolutionize content creation for professionals.</summary></entry><entry><title type="html">Zamba2-7B:Setting New Standards for Small Language Models</title><link href="http://localhost:4000/Zamba" rel="alternate" type="text/html" title="Zamba2-7B:Setting New Standards for Small Language Models" /><published>2024-10-14T00:00:00+03:00</published><updated>2024-10-14T00:00:00+03:00</updated><id>http://localhost:4000/Zamba</id><content type="html" xml:base="http://localhost:4000/Zamba">&lt;p&gt;Zyphra has introduced Zamba2-7B, a state-of-the-art small language model that outperforms leading competitors in both quality and performance. Designed for on-device and consumer GPU applications, Zamba2-7B offers exceptional efficiency and capabilities for natural language tasks.&lt;/p&gt;

&lt;p&gt;The rapid evolution of natural language processing (NLP) has led to the development of increasingly sophisticated models. Zyphra's latest offering, Zamba2-7B, is a small language model that stands out in a crowded field by delivering superior performance while maintaining a compact size. This model is particularly suited for applications requiring efficient processing on consumer-grade hardware.&lt;/p&gt;

&lt;p&gt;Zamba2-7B excels in standard language modeling evaluation sets, demonstrating remarkable latency and generation speed. Among small language models with fewer than 9 billion parameters, Zamba2-7B leads the pack in both quality and performance metrics. Its design allows it to outperform notable models from Mistral, Google’s Gemma, and Meta’s Llama3 series.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.prod.website-files.com/669abfd62b0d2313e8024f30/670b31c473f57e3c96e35298_ttft_mmlu-p-800.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt; The key features that you will come across while using it and reasons why you should try this model out are:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Exceptional Pretraining Quality:&lt;/strong&gt; The model benefits from high-quality pretraining and annealing datasets, which significantly enhance its performance on a per-training-token basis.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Hybrid Architecture:&lt;/strong&gt; Utilizing an advanced hybrid SSM-attention architecture, Zamba2-7B features Mamba layers interleaved with shared attention layers to optimize parameter efficiency.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;LoRA Projection Matrices:&lt;/strong&gt; These matrices allow for increased expressivity within each block while keeping additional parameter overhead minimal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The core architecture of Zamba2-7B builds upon the original Zamba model but introduces enhancements that improve its functionality. The backbone consists of Mamba layers interspersed with shared attention layers—two in Zamba2 compared to one in its predecessor. This innovative design minimizes the parameter cost while maximizing performance.&lt;/p&gt;

&lt;p&gt;A notable feature of the architecture is the concatenation of original model embeddings with the attention block. This approach helps maintain information across depth, resulting in better overall performance during generation tasks.&lt;/p&gt;

&lt;p&gt;Zamba2-7B was trained on 128 H100 GPUs over approximately 50 days using Zyphra's internal training framework built atop Megatron-LM. This extensive training process demonstrates that even at the 7 billion parameter scale, significant advancements can be achieved with a small team and moderate budget.&lt;/p&gt;

&lt;p&gt;The model achieves state-of-the-art inference efficiency across various metrics, including latency, throughput, and memory usage. These efficiencies make it an attractive option for enterprises looking to deploy powerful AI solutions without extensive computational resources.&lt;/p&gt;

&lt;p&gt;Zyphra is committed to democratizing access to advanced AI systems by releasing Zamba2-7B under an open-source license. This decision allows researchers, developers, and companies to leverage its capabilities freely. The broader AI community is invited to explore Zamba's unique architecture and contribute to ongoing advancements in efficient foundation models.&lt;/p&gt;

&lt;p&gt;A Hugging Face integration is available for easy access to &lt;a href=&quot;https://huggingface.co/Zyphra/Zamba2-7B&quot;&gt;Zamba2-7B&lt;/a&gt;, alongside a &lt;a href=&quot;https://github.com/Zyphra/Zamba2&quot;&gt;pure-PyTorch&lt;/a&gt; implementation that facilitates further customization and experimentation by developers.&lt;/p&gt;

&lt;p&gt;Zyphra's team aims to continue pushing the boundaries of what small language models can achieve. By exploring novel architectures and enhancing their understanding of powerful models, they are dedicated to advancing the field of AI research and application.&lt;/p&gt;

&lt;p&gt;The launch of Zamba2-7B marks a significant milestone in the development of small language models. With its exceptional performance metrics and efficient architecture, it sets a new standard for what can be achieved in natural language processing on consumer hardware. As Zyphra opens up this technology to the community, the potential for innovation in AI applications continues to grow.&lt;/p&gt;

&lt;p&gt;For more information about Zamba2-7B and its capabilities, visit Zyphra's &lt;a href=&quot;https://www.zyphra.com&quot;&gt;official website&lt;/a&gt;, or the official &lt;a href=&quot;https://www.zyphra.com/post/zamba2-7b&quot;&gt;blog post&lt;/a&gt; presenting the afforementioned results.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Zyphra has introduced Zamba2-7B, a state-of-the-art small language model that outperforms leading competitors in both quality and performance. Designed for on-device and consumer GPU applications, Zamba2-7B offers exceptional efficiency and capabilities for natural language tasks.</summary></entry></feed>