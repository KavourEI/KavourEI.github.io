<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-02-10T02:21:40+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">GitHub Copilot - The Agent Awakens</title><link href="http://localhost:4000/GitAgent" rel="alternate" type="text/html" title="GitHub Copilot - The Agent Awakens" /><published>2025-02-06T00:00:00+02:00</published><updated>2025-02-06T00:00:00+02:00</updated><id>http://localhost:4000/GitAgent</id><content type="html" xml:base="http://localhost:4000/GitAgent">&lt;p&gt;GitHub has unveiled a groundbreaking &lt;a href=&quot;https://github.com/features/copilot/whats-new?utm_source=agent-awakens-announcement&amp;amp;utm_medium=blogtop&amp;amp;utm_campaign=agentic-ai&quot;&gt;update to Copilot&lt;/a&gt;: the new agent mode. This enhancement empowers Copilot to autonomously iterate on its code, identify errors, and rectify them, significantly boosting developer productivity.&lt;/p&gt;

&lt;iframe width=&quot;699&quot; height=&quot;393&quot; src=&quot;https://www.youtube.com/embed/of--3Fq1M3w&quot; title=&quot;Agent mode and new models in GitHub Copilot Chat: Visual Studio Code&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;GitHub announced the agent mode for Copilot, available in Visual Studio Code. This feature enables Copilot to suggest terminal commands and prompts developers to execute them. It also analyzes runtime errors with self-healing capabilities, allowing it to recognize and automatically fix issues in the code. This advancement transforms Copilot from a passive assistant into an active collaborator in the development process.&lt;/p&gt;

&lt;p&gt;Alongside agent mode, GitHub announced the general availability of Copilot Edits. This feature allows developers to make code changes more efficiently, streamlining the development process and reducing the time spent on manual edits. Copilot Edits leverages AI to suggest contextually relevant modifications, enhancing code quality and consistency.&lt;/p&gt;

&lt;p&gt;GitHub also provided a preview of their Software Engineering (SWE) agent. This agent is designed to assist developers by providing intelligent code suggestions, automating repetitive tasks, and improving overall code quality. The SWE agent represents a significant step forward in integrating AI into the software development lifecycle, offering developers a powerful tool to enhance their coding experience.&lt;/p&gt;

&lt;p&gt;With the introduction of agent mode, the general availability of Copilot Edits, and the preview of the SWE agent, GitHub is redefining the role of AI in software development. These innovations aim to make coding more efficient, intuitive, and collaborative, ushering in a new era of intelligent development tools. There are many more capabilities and resources in the &lt;a href=&quot;https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/&quot;&gt;official announcement&lt;/a&gt; for you to explore. So don't hacitate if you are interested and get your hands dirty!&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">GitHub has unveiled a groundbreaking update to Copilot: the new agent mode. This enhancement empowers Copilot to autonomously iterate on its code, identify errors, and rectify them, significantly boosting developer productivity.</summary></entry><entry><title type="html">Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2</title><link href="http://localhost:4000/Olymp" rel="alternate" type="text/html" title="Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2" /><published>2025-02-05T00:00:00+02:00</published><updated>2025-02-05T00:00:00+02:00</updated><id>http://localhost:4000/Olymp</id><content type="html" xml:base="http://localhost:4000/Olymp">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 &lt;a href=&quot;https://dpmd.ai/imo-silver&quot;&gt;this https URL&lt;/a&gt;. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Chervonyi,+Y&quot; rel=&quot;nofollow&quot;&gt;Yuri Chervonyi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Trinh,+T+H&quot; rel=&quot;nofollow&quot;&gt;Trieu H. Trinh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ol%C5%A1%C3%A1k,+M&quot; rel=&quot;nofollow&quot;&gt;Miroslav Olšák&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+X&quot; rel=&quot;nofollow&quot;&gt;Xiaomeng Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Nguyen,+H&quot; rel=&quot;nofollow&quot;&gt;Hoang Nguyen&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Menegali,+M&quot; rel=&quot;nofollow&quot;&gt;Marcelo Menegali&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jung,+J&quot; rel=&quot;nofollow&quot;&gt;Junehyuk Jung&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Verma,+V&quot; rel=&quot;nofollow&quot;&gt;Vikas Verma&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Le,+Q+V&quot; rel=&quot;nofollow&quot;&gt;Quoc V. Le&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Luong,+T&quot; rel=&quot;nofollow&quot;&gt;Thang Luong&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2502.03544&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">s1-Simple test-time scaling</title><link href="http://localhost:4000/TestScaling" rel="alternate" type="text/html" title="s1-Simple test-time scaling" /><published>2025-01-31T00:00:00+02:00</published><updated>2025-01-31T00:00:00+02:00</updated><id>http://localhost:4000/TestScaling</id><content type="html" xml:base="http://localhost:4000/TestScaling">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending &quot;Wait&quot; multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at &lt;a href=&quot;https://github.com/simplescaling/s1&quot;&gt;this https URL&lt;/a&gt;. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muennighoff,+N&quot; rel=&quot;nofollow&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+Z&quot; rel=&quot;nofollow&quot;&gt;Zitong Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shi,+W&quot; rel=&quot;nofollow&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Li,+X+L&quot; rel=&quot;nofollow&quot;&gt;Xiang Lisa Li&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fei-Fei,+L&quot; rel=&quot;nofollow&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hajishirzi,+H&quot; rel=&quot;nofollow&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zettlemoyer,+L&quot; rel=&quot;nofollow&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Liang,+P&quot; rel=&quot;nofollow&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Cand%C3%A8s,+E&quot; rel=&quot;nofollow&quot;&gt;Emmanuel Candès&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hashimoto,+T&quot; rel=&quot;nofollow&quot;&gt;Tatsunori Hashimoto&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2501.19393&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Tülu 3 405B The New Heavyweight Champion in AI Models</title><link href="http://localhost:4000/tulu3" rel="alternate" type="text/html" title="Tülu 3 405B The New Heavyweight Champion in AI Models" /><published>2025-01-30T00:00:00+02:00</published><updated>2025-01-30T00:00:00+02:00</updated><id>http://localhost:4000/tulu3</id><content type="html" xml:base="http://localhost:4000/tulu3">&lt;p&gt;Hold onto your hats, all of you people! The Allen Institute for AI has just unleashed &lt;a href=&quot;https://allenai.org/blog/tulu-3-technical&quot;&gt;Tülu 3 405B&lt;/a&gt;, a colossal 405-billion-parameter model that's not just flexing its muscles but also outpacing the competition. This update is setting new benchmarks, leaving rivals like DeepSeek V3 and GPT-4o &lt;i&gt;in the dust&lt;/i&gt;, all while being as open-source as a community cookbook.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.datocms-assets.com/64837/1738270804-image-23.png?fit=max&amp;amp;fm=webp&amp;amp;h=810&amp;amp;w=1550&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On January 30, 2025, the Allen Institute for AI (Ai2) proudly introduced Tülu 3 405B, marking the first time fully open post-training recipes have been applied to such a massive open-weight model. Building upon the success of their previous Tülu 3 release, this model demonstrates that size does matter, especially when combined with innovative training techniques. Tülu 3 405B doesn't just compete; it often surpasses models like DeepSeek V3 and GPT-4o, and leaves prior open-weight models, including Llama 3.1 405B Instruct and Nous Hermes 3 405B, eating its dust on many standard benchmarks.&lt;/p&gt;

&lt;p&gt;So, how did Ai2 cook up this powerhouse? They scaled their proven Tülu 3 post-training recipe to the Llama-405B base model, following a five-step program:&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;&lt;strong&gt;Data Delicacy:&lt;/strong&gt; Carefully curating and synthesizing data targeting core skills.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Supervised Finetuning (SFT):&lt;/strong&gt; Applying a finely selected mix of prompts and their completions.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Direct Preference Optimization (DPO):&lt;/strong&gt; Tweaking the model based on both off- and on-policy preference data.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Reinforcement Learning with Verifiable Rewards (RLVR):&lt;/strong&gt; A fresh, RL-based method to boost specific skills with rewards you can take to the bank.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Evaluation Extravaganza:&lt;/strong&gt; Implementing a standardized suite for development, decontamination, and final evaluation stages.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the training arena, Ai2 employed their novel RLVR approach, focusing on tasks with clear-cut answers like math problems and precise instructions. To handle the 405B scale, they deployed the model using vLLM with 16-way tensor parallelism, utilizing 240 GPUs for training. Each RLVR iteration involved approximately 550 seconds for inference, 25 seconds for weight transfer, and 1500 seconds for training. Interestingly, they discovered that feeding the model exclusively MATH data, rather than a mix, yielded better results at this scale—a testament to the model's appetite for complex, specialized tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.datocms-assets.com/64837/1738199366-rlvr-diagram.png?dpr=2&amp;amp;fit=max&amp;amp;fm=webp&amp;amp;h=810&amp;amp;w=1550&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tülu 3 405B isn't just big; it's a top performer. It achieves competitive or superior results compared to both DeepSeek V3 and GPT-4o, while also surpassing prior open-weight post-trained models of the same size on many standard benchmarks. Notably, the Reinforcement Learning from Verifiable Rewards (RLVR) framework improved MATH performance more significantly at this larger scale, echoing findings in the DeepSeek-R1 report. Overall, Tülu 3 405B shows a consistent edge over DeepSeek V3, especially when safety benchmarks are in play.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.datocms-assets.com/64837/1738199543-405b_rl.png?dpr=2&amp;amp;fit=max&amp;amp;fm=webp&amp;amp;h=810&amp;amp;w=1550&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With Tülu 3 405B, Ai2 has not only scaled up their model parameters but also the impact of open-source AI. This release underscores the scalability and effectiveness of their post-training recipe at an unprecedented scale, setting a new standard in the AI community. As Tülu 3 405B continues to flex its capabilities, it's clear that the future of AI is not just about bigger models, but smarter, more open ones too. To get started and find out more detailes about the afforementioned updates you can visit official &lt;a href=&quot;https://allenai.org/blog/tulu-3-405B&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Hold onto your hats, all of you people! The Allen Institute for AI has just unleashed Tülu 3 405B, a colossal 405-billion-parameter model that's not just flexing its muscles but also outpacing the competition. This update is setting new benchmarks, leaving rivals like DeepSeek V3 and GPT-4o in the dust, all while being as open-source as a community cookbook.</summary></entry><entry><title type="html">DeepSeek-R1 Teams Up with NVIDIA NIM A Power Duo in AI</title><link href="http://localhost:4000/NVIDIADSR1" rel="alternate" type="text/html" title="DeepSeek-R1 Teams Up with NVIDIA NIM A Power Duo in AI" /><published>2025-01-30T00:00:00+02:00</published><updated>2025-01-30T00:00:00+02:00</updated><id>http://localhost:4000/NVIDIADSR1</id><content type="html" xml:base="http://localhost:4000/NVIDIADSR1">&lt;p&gt;DeepSeek-R1, a 671-billion-parameter model known for its exceptional reasoning capabilities, is now available as an NVIDIA NIM microservice. This collaboration promises to deliver high-speed, efficient AI solutions for complex tasks.&lt;/p&gt;

&lt;p&gt;DeepSeek-R1 stands out with its massive 671 billion parameters, significantly surpassing many existing open-source large language models. Its architecture features a mixture-of-experts design, with each layer comprising 256 experts. For every input token, the model engages eight experts in parallel, enabling advanced logical inference, reasoning, mathematics, coding, and language understanding. This design allows DeepSeek-R1 to handle an extensive input context of up to 128,000 tokens, making it adept at processing and generating lengthy and complex content.&lt;/p&gt;

&lt;p&gt;Integrating DeepSeek-R1 into NVIDIA's NIM microservice framework simplifies the deployment process for developers and enterprises. The NIM microservice supports industry-standard APIs, ensuring seamless integration into existing systems. By running the microservice on preferred accelerated computing infrastructure, organizations can maintain high standards of security and data privacy. Additionally, with &lt;a href=&quot;https://www.nvidia.com/en-us/ai/foundry/&quot;&gt;NVIDIA AI Foundry&lt;/a&gt; and &lt;a href=&quot;https://www.nvidia.com/en-us/ai-data-science/products/nemo/&quot;&gt;NeMo software&lt;/a&gt;, there's potential for creating customized &lt;a href=&quot;https://build.nvidia.com/deepseek-ai/deepseek-r1&quot;&gt;DeepSeek-R1 microservices&lt;/a&gt; tailored to specialized AI agents.&lt;/p&gt;

&lt;iframe width=&quot;733&quot; height=&quot;412&quot; src=&quot;https://www.youtube.com/embed/47DWCEzG1Cg&quot; title=&quot;DeepSeek-R1 in Action with NVIDIA NIM Microservices&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;When deployed on a single NVIDIA HGX H200 system equipped with eight H200 GPUs connected via NVLink and NVLink Switch, the DeepSeek-R1 NIM microservice achieves an impressive throughput of up to 3,872 tokens per second. This high performance is attributed to the NVIDIA Hopper architecture's FP8 Transformer Engine and the substantial NVLink bandwidth facilitating efficient communication among the model's experts. Looking ahead, the upcoming NVIDIA Blackwell architecture is expected to further enhance performance with its fifth-generation Tensor Cores, delivering up to 20 petaflops of peak FP4 compute performance and a 72-GPU NVLink domain optimized for inference.&lt;/p&gt;

&lt;p&gt;Developers eager to explore the capabilities of the DeepSeek-R1 NIM microservice can access a preview on &lt;a href=&quot;http://build.nvidia.com/&quot;&gt;NVIDIA's platform&lt;/a&gt;. The API is anticipated to be available soon as a downloadable NIM microservice, forming part of the &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/products/ai-enterprise/&quot;&gt;NVIDIA AI Enterprise&lt;/a&gt; software suite. This development opens avenues for building specialized AI agents that leverage DeepSeek-R1's advanced reasoning capabilities, all within a secure and efficient deployment framework.&lt;/p&gt;

&lt;p&gt;The collaboration between DeepSeek-R1 and NVIDIA's NIM microservice framework marks a significant advancement in AI deployment. By combining a state-of-the-art reasoning model with a robust deployment infrastructure, this partnership offers developers and enterprises a powerful tool for implementing sophisticated AI solutions across various applications. Wanna find out more?! head to the &lt;a href=&quot;https://blogs.nvidia.com/blog/deepseek-r1-nim-microservice/&quot;&gt;official announcement&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">DeepSeek-R1, a 671-billion-parameter model known for its exceptional reasoning capabilities, is now available as an NVIDIA NIM microservice. This collaboration promises to deliver high-speed, efficient AI solutions for complex tasks.</summary></entry><entry><title type="html">Mistral AI Unveils Mistral Small 3 - A High-Performance, Latency-Optimized Model</title><link href="http://localhost:4000/mistralsmall" rel="alternate" type="text/html" title="Mistral AI Unveils Mistral Small 3 - A High-Performance, Latency-Optimized Model" /><published>2025-01-30T00:00:00+02:00</published><updated>2025-01-30T00:00:00+02:00</updated><id>http://localhost:4000/mistralsmall</id><content type="html" xml:base="http://localhost:4000/mistralsmall">&lt;p&gt;Mistral AI has introduced Mistral Small 3, a 24-billion-parameter model optimized for low latency, delivering performance comparable to larger models while maintaining efficiency. Released under the Apache 2.0 license, it offers a versatile solution for various AI applications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mistral.ai/images/news/mistral-small-3/up-and-to-the-left.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On January 30, 2025, Mistral AI announced the release of Mistral Small 3, a latency-optimized model with 24 billion parameters. This model is designed to provide robust language understanding and instruction-following capabilities, catering to approximately 80% of generative AI tasks that demand quick and accurate responses. Notably, Mistral Small 3 achieves over 81% accuracy on the MMLU benchmark and processes 150 tokens per second, making it one of the most efficient models in its category.&lt;/p&gt;

&lt;p&gt;Mistral Small 3 stands out by delivering performance on par with larger models such as &lt;a href=&quot;https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct&quot;&gt;Llama 3.3 70B&lt;/a&gt; and &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-llm/&quot;&gt;Qwen 32B&lt;/a&gt;, while operating more than three times faster on equivalent hardware. This efficiency is achieved through a streamlined architecture with fewer layers, reducing the time required for each forward pass. Both pretrained and instruction-tuned versions of the model are available under the Apache 2.0 license, providing a solid foundation for further development and customization.&lt;/p&gt;

&lt;p&gt;In evaluations conducted with an external third-party vendor, Mistral Small 3 was assessed alongside other models using over 1,000 proprietary coding and generalist prompts. Evaluators, who were unaware of which model generated each response, consistently preferred the outputs from Mistral Small 3. These assessments encompassed various domains, including code, mathematics, general knowledge, and instruction-following tasks, underscoring the model's versatility and effectiveness.&lt;/p&gt;

&lt;p&gt;Mistral Small 3 is well-suited for a range of applications:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Fast-Response Conversational Assistance:&lt;/strong&gt; Ideal for virtual assistants and scenarios where users expect immediate, accurate feedback.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Low-Latency Function Calling:&lt;/strong&gt; Capable of handling rapid function execution within automated workflows.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Fine-Tuning for Subject Matter Expertise:&lt;/strong&gt; Can be customized to specialize in specific domains, making it valuable for fields like legal advice, medical diagnostics, and technical support.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Local Inference:&lt;/strong&gt; When quantized, the model can run privately on hardware such as a single RTX 4090 or a MacBook with 32GB RAM, benefiting hobbyists and organizations managing sensitive information.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With the release of Mistral Small 3, Mistral AI continues its commitment to advancing open-source AI solutions. The model's combination of high performance, low latency, and versatility makes it a compelling choice for developers and organizations seeking efficient and customizable AI models for a variety of applications.&lt;/p&gt;

&lt;p&gt; To learn more, and find ways to get the most out of it, head to the &lt;a href=&quot;https://mistral.ai/news/mistral-small-3/&quot;&gt;official blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt; For those of you eager to get your hands dirty, it is said that there is the following collaborations with Hugging Face, Ollama, Kaggle, Together AI, and Fireworks AI to make the model available on their platforms:

&lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501&quot;&gt;Hugging Face&lt;/a&gt; (&lt;a href=&quot;https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501&quot;&gt;base model&lt;/a&gt;)&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://ollama.com/library/mistral-small&quot;&gt;Ollama&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/models/mistral-ai/mistral-small-24b&quot;&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.together.ai/models/mistral-small-3&quot;&gt;Together AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://fireworks.ai/models/fireworks/mistral-small-24b-instruct-2501&quot;&gt;Fireworks AI&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.ibm.com/products/watsonx-ai&quot;&gt;IBM watsonx&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;Coming soon on NVIDIA NIM, Amazon SageMaker, Groq, Databricks and Snowflake&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Mistral AI has introduced Mistral Small 3, a 24-billion-parameter model optimized for low latency, delivering performance comparable to larger models while maintaining efficiency. Released under the Apache 2.0 license, it offers a versatile solution for various AI applications.</summary></entry><entry><title type="html">Exploring QwQ-32B:A New Frontier in AI Reasoning</title><link href="http://localhost:4000/QwQ" rel="alternate" type="text/html" title="Exploring QwQ-32B:A New Frontier in AI Reasoning" /><published>2024-11-28T00:00:00+02:00</published><updated>2024-11-28T00:00:00+02:00</updated><id>http://localhost:4000/QwQ</id><content type="html" xml:base="http://localhost:4000/QwQ">&lt;p&gt; The QwQ-32B model by the Qwen Team represents a significant advancement in AI reasoning capabilities, showcasing impressive performance in mathematical problem-solving and programming while highlighting its limitations and potential for future development.&lt;/p&gt;

&lt;p&gt; With new models emerging every now and then, boundaries are pushed so that machines can understand and achieve more and more. One such model is &lt;strong&gt;QwQ-32B&lt;/strong&gt;, developed by the Qwen Team, which focuses on enhancing AI reasoning capabilities. But let's take a closer look on features, performance benchmarks, limitations of QwQ-32B, and provide some insights into its potential applications.&lt;/p&gt;

&lt;p&gt; QwQ-32B is an experimental research model that embodies a philosophical approach to learning and understanding. It operates with a mindset of curiosity, questioning its own assumptions and exploring various paths to arrive at answers. This introspective nature allows it to tackle complex problems across different domains, including mathematics, programming, and general knowledge.&lt;/p&gt;

&lt;h3&gt;Key Features of QwQ-32B&lt;/h3&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Deep Reasoning Capabilities:&lt;/strong&gt; The model excels in mathematical problem-solving and programming tasks, demonstrating an ability to engage in recursive reasoning and self-reflection.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Benchmarks Performance:&lt;/strong&gt; QwQ-32B has achieved notable scores on various benchmarks such as GPQA (65.2%), AIME (50.0%), MATH-500 (90.6%), and LiveCodeBench (50.0%), showcasing its strength in analytical reasoning.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Philosophical Approach:&lt;/strong&gt; The model's design encourages a questioning mindset, allowing it to explore problems from multiple angles before arriving at conclusions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The performance of QwQ-32B has been evaluated across several challenging benchmarks:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;GPQA:&lt;/strong&gt; A graduate-level benchmark that assesses scientific problem-solving abilities through grade school level questions.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;AIME:&lt;/strong&gt; Tests mathematical problem-solving skills across various topics including arithmetic, algebra, and geometry.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;MATH-500:&lt;/strong&gt; A comprehensive dataset designed to evaluate mathematical comprehension across diverse topics.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;LiveCodeBench:&lt;/strong&gt; Evaluates code generation and programming problem-solving abilities in real-world scenarios.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The results indicate that QwQ-32B excels particularly in mathematics, achieving an impressive 90.6% on the MATH-500 benchmark, which highlights its exceptional understanding of mathematical concepts.&lt;/p&gt;

&lt;p&gt;Despite its remarkable capabilities, QwQ-32B has several limitations that users should be aware of when considering whether to use this model or not:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Language Mixing:&lt;/strong&gt; The model may unexpectedly switch between languages or mix them, which can affect clarity in responses.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Recursive Reasoning Loops:&lt;/strong&gt; Occasionally, it may enter circular reasoning patterns leading to lengthy responses without conclusive answers.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Safety Considerations:&lt;/strong&gt; Enhanced safety measures are necessary to ensure reliable performance, particularly when deployed in sensitive applications.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Narrow Focus Areas:&lt;/strong&gt; While it excels in math and coding, there is room for improvement in common sense reasoning and nuanced language understanding.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Demos and Use Cases&lt;/h3&gt;
&lt;p&gt;The capabilities of QwQ-32B can be observed through various demo cases that illustrate its thought process. For instance, when tasked with solving a mathematical equation by adding parentheses to achieve a specific outcome, the model engages in a step-by-step analysis:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&quot;Let’s tackle this problem step by step... I need to think about where to place the parentheses to alter the order of operations to achieve the desired result.&quot;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This introspective approach exemplifies how QwQ-32B engages with problems thoughtfully rather than providing hasty conclusions. Its ability to analyze multiple possibilities before arriving at an answer showcases its potential for deeper understanding.&lt;/p&gt;

&lt;p&gt;The introduction of QwQ-32B marks an exciting chapter in AI reasoning development. As researchers continue to refine its capabilities and address existing limitations, the potential applications for this model are vast. From educational tools that assist students with complex subjects to programming aids that enhance coding efficiency, QwQ models could significantly impact various fields.&lt;/p&gt;

&lt;p&gt;The QwQ-32B model represents a significant advancement in AI reasoning capabilities, demonstrating impressive performance across mathematical and programming benchmarks while embodying a philosophical approach to learning. Despite its limitations, the model's potential for growth and application is substantial. As we continue to explore the depths of AI reasoning through models like QwQ-32B, we move closer to realizing the full potential of artificial intelligence as a tool for knowledge acquisition and problem-solving.&lt;/p&gt;

&lt;p&gt; In case you want to further explore this new model, its capabilities or take a look at the available, presented example cases you can read official blog post, &lt;a href=&quot;https://qwenlm.github.io/blog/qwq-32b-preview/&quot;&gt;here&lt;/a&gt;.&lt;p&gt;
&lt;/p&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">The QwQ-32B model by the Qwen Team represents a significant advancement in AI reasoning capabilities, showcasing impressive performance in mathematical problem-solving and programming while highlighting its limitations and potential for future development.</summary></entry><entry><title type="html">Transforming Reading with GenFM on ElevenReader</title><link href="http://localhost:4000/GenFM" rel="alternate" type="text/html" title="Transforming Reading with GenFM on ElevenReader" /><published>2024-11-27T00:00:00+02:00</published><updated>2024-11-27T00:00:00+02:00</updated><id>http://localhost:4000/GenFM</id><content type="html" xml:base="http://localhost:4000/GenFM">&lt;p&gt; ElevenLabs has enhanced its ElevenReader app with GenFM, enabling users to generate personalized podcasts from various text formats, thereby revolutionizing the way people consume written content.&lt;/p&gt;

&lt;p&gt;The digital age has transformed how we access and consume information, and with the advent of artificial intelligence, this evolution is accelerating. ElevenLabs has taken a significant step forward by launching the &lt;strong&gt;GenFM feature&lt;/strong&gt; within its &lt;a href=&quot;https://elevenlabs.io/text-reader&quot;&gt;ElevenReader app&lt;/a&gt;. This innovative addition allows users to create smart personal podcasts from PDFs, articles, ebooks, and more, making it easier than ever to absorb information on the go. This article explores the features of GenFM, its implications for content consumption, and how it enhances the overall user experience within the ElevenReader app.&lt;/p&gt;

&lt;p&gt;The ElevenReader app is a versatile tool that transforms written text into high-quality audio using advanced AI voice technology. Available for free on both iOS and Android platforms, it supports 32 languages and provides natural-sounding narration. Users can listen to any text they choose, whether it's a document, article, or ebook, making reading more accessible and convenient.&lt;/p&gt;

&lt;p&gt;With the introduction of GenFM, ElevenReader takes a giant leap forward in personalizing the reading experience. This feature allows users to generate podcasts that adapt to the most relevant ideas and insights from their reading materials. By leveraging AI co-hosts tailored to match the content's theme, GenFM creates engaging audio experiences that resonate with listeners.&lt;/p&gt;

&lt;h3&gt;Key Features of GenFM&lt;/h3&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Smart Podcast Generation:&lt;/strong&gt; Users can convert their PDFs, articles, and documents into personalized podcasts in seconds, making it easier to digest information while multitasking.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;AI Co-hosts:&lt;/strong&gt; The app features AI co-hosts that are matched with the content being read, enhancing the listening experience through dynamic narration.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Multilingual Support:&lt;/strong&gt; With support for 32 languages, GenFM ensures that users from diverse backgrounds can access content in their preferred language.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;User-Friendly Interface:&lt;/strong&gt; The intuitive design of the ElevenReader app allows users to easily navigate through their text selections and generate podcasts effortlessly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The addition of GenFM represents a significant shift in how individuals engage with written material. By transforming text into audio format, users can now listen to their favorite articles or books during commutes, workouts, or other activities where reading may not be feasible. This flexibility not only enhances productivity but also caters to different learning styles—making it particularly beneficial for auditory learners.&lt;/p&gt;

&lt;iframe width=&quot;675&quot; height=&quot;379&quot; src=&quot;https://www.youtube.com/embed/x6ub-9HhxGU&quot; title=&quot;GenFM, Now Playing on ElevenReader: Smart Podcasts Produced by Generative AI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Looking ahead, ElevenLabs plans to expand the capabilities of GenFM by introducing this feature for Android users. As more people adopt this technology, it will likely lead to an increase in demand for audio content across various sectors—be it education, journalism, or entertainment.&lt;/p&gt;

&lt;p&gt;The launch of GenFM within the ElevenReader app signifies a transformative moment in digital content consumption. By allowing users to generate personalized podcasts from their reading materials quickly and easily, ElevenLabs is redefining how we interact with information. As technology continues to evolve, tools like ElevenReader will play a crucial role in making knowledge more accessible and engaging for everyone.To read all about it and explore this great initiative, do not hesitate and take a look at the &lt;a href=&quot;https://elevenlabs.io/blog/genfm-on-elevenreader&quot;&gt;official blog&lt;/a&gt; post of ElevenLabs.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">ElevenLabs has enhanced its ElevenReader app with GenFM, enabling users to generate personalized podcasts from various text formats, thereby revolutionizing the way people consume written content.</summary></entry><entry><title type="html">Star Attention:Efficient LLM Inference over Long Sequences</title><link href="http://localhost:4000/StarAttention" rel="alternate" type="text/html" title="Star Attention:Efficient LLM Inference over Long Sequences" /><published>2024-11-26T00:00:00+02:00</published><updated>2024-11-26T00:00:00+02:00</updated><id>http://localhost:4000/StarAttention</id><content type="html" xml:base="http://localhost:4000/StarAttention">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy. &lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Acharya,+S&quot; rel=&quot;nofollow&quot;&gt;Shantanu Acharya&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Jia,+F&quot; rel=&quot;nofollow&quot;&gt;Fei Jia&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ginsburg,+B&quot; rel=&quot;nofollow&quot;&gt;Boris Ginsburg&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2411.17116&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Advancing Open Language Models by OlMo 2</title><link href="http://localhost:4000/Olm2" rel="alternate" type="text/html" title="Advancing Open Language Models by OlMo 2" /><published>2024-11-26T00:00:00+02:00</published><updated>2024-11-26T00:00:00+02:00</updated><id>http://localhost:4000/Olm2</id><content type="html" xml:base="http://localhost:4000/Olm2">&lt;p&gt; The release of OLMo 2 marks a showcase on improved performance and stability through innovative training techniques and comprehensive evaluation frameworks.&lt;/p&gt;

&lt;p&gt; Following the success of the first OLMo model released in February 2024, the Allen Institute for AI has intrduced &lt;strong&gt;OLMo 2&lt;/strong&gt;, a new family of models that promise to narrow the performance gap between open and proprietary systems. This article delves into the features and innovations behind OLMo 2, highlighting its potential impact on the landscape of language modeling.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.datocms-assets.com/64837/1732648723-olmo2-2.png?dpr=1.5&amp;amp;fit=max&amp;amp;fm=webp&amp;amp;h=810&amp;amp;w=1550&quot; /&gt;&lt;/p&gt;

&lt;p&gt;OLMo 2 includes two primary models: a &lt;strong&gt;7 billion parameter&lt;/strong&gt; model and a &lt;strong&gt;13 billion parameter&lt;/strong&gt; model, both trained on up to &lt;strong&gt;5 trillion tokens&lt;/strong&gt;. These models are designed to compete with leading open-weight models like Llama 3.1 on various English academic benchmarks. The development of OLMo 2 reflects a commitment to enhancing the capabilities of fully open models, providing researchers and developers with robust tools for natural language processing tasks.&lt;/p&gt;

&lt;p&gt;A key aspect of OLMo 2's success lies in its innovative training techniques aimed at improving stability and performance:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Training Stability:&lt;/strong&gt; The team focused on enhancing long model training runs by addressing common issues such as loss spikes that can lead to lower final performance.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Staged Training:&lt;/strong&gt; This approach involves interventions during late pretraining to address knowledge gaps identified during earlier phases.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Post-training Recipes:&lt;/strong&gt; State-of-the-art post-training methodologies were applied to create instruction-tuned variants of OLMo 2, enhancing its capability to follow user instructions effectively.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The development team established the Open Language Modeling Evaluation System (&lt;a href=&quot;https://github.com/allenai/olmes&quot;&gt;OLMES&lt;/a&gt;), which consists of a suite of &lt;strong&gt;20 evaluation benchmarks&lt;/strong&gt;. This framework assesses core capabilities such as knowledge recall, commonsense reasoning, and mathematical problem-solving. By implementing OLMES, the team could track improvements throughout the development stages and ensure that OLMo 2 meets high-performance standards across various tasks.&lt;/p&gt;

&lt;p&gt;The results from OLMo 2’s evaluations indicate that it outperforms many existing open-weight models. Notably, the OLMo 2 7B model surpassed the performance of Llama-3.1 8B on several benchmarks, while the OLMo 2 13B model outperformed Qwen 2.5 7B despite having lower total training FLOPs. These findings underscore OLMo 2's position at the Pareto frontier of training efficiency versus average performance.&lt;/p&gt;

&lt;p&gt;The pretraining process for OLMo 2 was conducted in two stages:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Stage One:&lt;/strong&gt; Utilized a massive dataset called OLMo-Mix-1124, consisting of approximately &lt;strong&gt;3.9 trillion tokens&lt;/strong&gt;. The models were trained for one epoch on this dataset.&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Stage Two:&lt;/strong&gt; Focused on high-quality web data and domain-specific content, resulting in an additional &lt;strong&gt;843 billion tokens&lt;/strong&gt;. This stage aimed to refine model capabilities further through targeted training.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The release also includes instruction-tuned variants known as OLMo 2-Instruct models. These variants were developed using advanced post-training techniques from &lt;a href=&quot;https://allenai.org/tulu&quot;&gt;Tülu 3&lt;/a&gt;, incorporating supervised finetuning and reinforcement learning methodologies. The results show that these models are competitive with other leading instruction-following models in the open-weight category.&lt;/p&gt;

&lt;p&gt;The introduction of OLMo 2 represents a significant step forward in the open language model ecosystem. With its robust architecture, innovative training techniques, and comprehensive evaluation framework, OLMo 2 is positioned to empower researchers and developers in their pursuit of advanced natural language processing applications. As the community continues to engage with these developments, we can expect further enhancements in model performance and usability.&lt;/p&gt;

&lt;p&gt;The advancements embodied in OLMo 2 highlight the potential for fully open language models to compete with proprietary systems effectively. By prioritizing transparency and collaboration within the AI community, Allen Institute for AI is paving the way for future innovations that will benefit researchers and practitioners alike. As we move forward, the lessons learned from developing OLMo 2 will undoubtedly influence subsequent generations of language models. If you feel like reading more and exploring this ai chapter, head &lt;a href=&quot;https://allenai.org/blog/olmo2&quot;&gt;here&lt;/a&gt; at the official blog post written by Allenai.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">The release of OLMo 2 marks a showcase on improved performance and stability through innovative training techniques and comprehensive evaluation frameworks.</summary></entry></feed>