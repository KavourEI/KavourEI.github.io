<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-11T14:36:21+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">Multiple datasources - Route selections</title><link href="http://localhost:4000/rag3" rel="alternate" type="text/html" title="Multiple datasources - Route selections" /><published>2024-09-09T00:00:00+03:00</published><updated>2024-09-09T00:00:00+03:00</updated><id>http://localhost:4000/rag3</id><content type="html" xml:base="http://localhost:4000/rag3">&lt;p&gt; I hope you enjoy every step so far. Until this point of our Langchain/RAG journey, we have managed to build a simple local application and a querry transformation assistant. But what happens when we have multiple data sources? How to define where our application will retrieve the required information from? The definition of this process, finding the correct road, or better let's say finding the correct route, is called Routing.&lt;/p&gt;

&lt;p&gt; There are many ways to give directions in real life, this applies here as well! As we can see in the following image, there are two main categories of routing. 

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Logical Routing&lt;/strong&gt;: Let LLM choose a DB based on the question.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Semantic Routing&lt;/strong&gt;: Embed question and choose prompt based on similarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:1400/1*iBm4xuEwvnp9KFyH9x05cw.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt; Before going though the architecture of such a RAG model for either Logical or Semantic routing, we are going to take a look at the differences accompanied with some simplier examples to understand which is best for your case case to go with.&lt;/p&gt;

&lt;h3&gt; Logical Routing &lt;/h3&gt;

&lt;p&gt; Routes are defined based on pre-determined rules, conditions, or algorithms. These rules are purely technical, and they don't take into account the meaning or context of the data. Logical Routing decisions are made based on factors like IP addresses, routing tables, or specific conditions that match certain criteria. It focuses on efficiently directing traffic or requests based on a logical or structural framework.&lt;/p&gt;

&lt;p&gt;To put logical routing straight, imagine you’re in a city, and there are different routes based on traffic signals or road rules. These routes don't care about where you're coming from or your personal preferences, but simply about making sure traffic moves according to the road signs.

&lt;ul&gt;
&lt;li&gt; Situation: You’re driving from Point A to Point B.&lt;/li&gt;
&lt;li&gt; Logical Rule: If a road is closed, take the next available road.&lt;/li&gt;
&lt;li&gt; Routing Decision: You don't think about why the road is closed; you just follow the detour and proceed based on traffic rules. It's like having a GPS system giving you orders, while driving, in a city that you have never been.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;h3&gt; Semantic Routing &lt;/h3&gt;

&lt;p&gt;  Routes are chosen based on the meaning or context of the data being transmitted. The logical routing decisions consider the content, purpose, or relationships within the data. It uses metadata or content analysis to decide the best route for a request, often ensuring that the most relevant resources handle the data. It prioritizes meaning and relevance to ensure that the request or data is handled in the best possible way based on its content.&lt;/p&gt;

&lt;p&gt; Imagine you're in a library, and someone asks where to find a book. Instead of directing them based on just shelf numbers (logical), you direct them based on their interest in the content of the book. You ask, &quot;What topic are you looking for?&quot; and route them accordingly to the right section.

&lt;ul&gt;
&lt;li&gt; Situation: You want to ask a question, but there are different people who can answer it.&lt;/li&gt;
&lt;li&gt; Semantic Rule: If your question is about science, you’re directed to a scientist; if it's about cooking, you’re directed to a chef.&lt;/li&gt;
&lt;li&gt; Routing Decision: The decision isn’t based on random criteria but on the meaning of your question and who’s best equipped to answer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;
Based on Data/Context, Logical Routing ignores the meaning of the data and routes based on fixed rules while semantic routing analyzes the meaning and routes accordingly. Overall and while looking at the purpose, logical routing focuses on efficiency and technical structure and on the other hand, semantic routing focuses on relevance and meaningful processing.
&lt;/p&gt;

&lt;p&gt; Now that we got the main idea about what is the meaning of both, let us take a closer look to the code and how to build a system like that.&lt;/p&gt;

&lt;p&gt; Initially, we need to call the modules that are going to help us accomplish our goal. I am not going to go through &lt;code&gt;.env&lt;/code&gt; file again and won't be mentioning this file again in the future. Hopefully, you got the idea behind it, why it is important and the reasons I insist on using one. If you are not so sure on what file I am referring to, please take a look at the first two notebooks of this series and follow the steps to create one. Remember &lt;code&gt;.env&lt;/code&gt; file will keep your Usernames, Passwords and Endpoint IDs safe without the need to erase them or hide them when sharing any file. So the modules we are going to use within this notebook are the following.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import AzureOpenAIEmbeddings
# from langchain.chat_models import AzureChatOpenAI
from langchain_openai import AzureChatOpenAI
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from typing import Literal
from langchain.utils.math import cosine_similarity
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda

from dotenv import load_dotenv, dotenv_values
load_dotenv()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we are using LLM models we need to define embeddings so that we can communicate with the computers and we &lt;i&gt;&quot;talk&quot;&lt;/i&gt; the same language.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;embeddings = AzureOpenAIEmbeddings(
    deployment = os.getenv('OPENAI_DEPLOYMENT_NAME'),
    model = os.getenv('OPENAI_MODEL_NAME_EMB'),
    azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),
    openai_api_type = os.getenv('OPENAI_API_TYPE'),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Apart from that, &lt;strong&gt;vectorstore&lt;/strong&gt; is essential as we need a place to store the vectors that will result from our embeddings call. Hecne, we are going to define a function call &lt;i&gt;vectorstor_gen&lt;/i&gt; in order to store the information acquired (vectors generatated) from the files one owns and what to search from.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def vectorstore_gen(file, dir):
    loader = PyPDFLoader(file)
    documents = loader.load()

    # Split text into chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)
    text_chunks = text_splitter.split_documents(documents)

    vectorstore = Chroma.from_documents(documents=text_chunks,
                                        embedding=embeddings,
                                        persist_directory=dir)
    vectorstore.persist()
    return vectorstore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now image you have two documents or batter let's say two different files containing documents, and you want to create two vectorstore for both of them so that our llm will be able to choose betwen then and answer to your questions. Remember that we have created a folder data in our project and there we store all the data that we want to use for our RAG projects. In this example I am going to use dummy pdf names (&lt;i&gt;A.pdf&lt;/i&gt; and &lt;i&gt;B.pdf&lt;/i&gt;) and imaging that those two files contain information/documentation about different LLM models. Feel free to replace those with two irrelevant pdf files (when it comes to topic)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# Create a vectorestore to answer questions about topic A
vectorstore_A = vectorstore_gen('data/A.pdf', 'data/vectorstore_A')
# Create a vectorstore to answer questions about topic B
vectorstore_B = vectorstore_gen('data/B.pdf', 'data/vectorstore_B')

retriever_A = vectorstore_A.as_retriever(search_kwargs={'k':5})
retriever_B = vectorstore_B.as_retriever(search_kwargs={'k':5})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now it is time to create the classes we are going to use to our routing models. Initially we are going to start with Logical Routing. Initially, we are going to build a class to route to the most relevant datasource accourding to the User's question. We are going to use an extention of &lt;code&gt;BaseModel&lt;/code&gt;. The &lt;code&gt;BaseModel&lt;/code&gt; class provides automatic validation parsing and serialization of data. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
class QueryRouter(BaseModel):
    &quot;&quot;&quot;Route a user query to the appropriate datasource that will help answer the query accurately&quot;&quot;&quot;
    datasource: Literal['A', 'B', 'general'] = Field(..., description=&quot;Given a user question choose which datasource would be most relevant for answering their question&quot;)
    question: str = Field(..., description=&quot;User question to be routed to the appropriate datasource&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  Next we need to build and and call an LLM model that will help us build our RAG model, within which we are going to include routing stratey.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
llm = AzureChatOpenAI(
    deployment_name = os.getenv('LLM_35_Deployment'),
    model_name = os.getenv('LLM_35_Model'),
    azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),
    temperature = 0
)

structured_llm = llm.with_structured_output(QueryRouter, method=&quot;function_calling&quot;, include_raw=True)
structured_llm.invoke(&quot;Which datasource should be used for a question about general knowledge?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it is time to build the routing prompt that will be included in the RAG pipeline.  &lt;strong&gt;Not that&lt;/strong&gt; if the pdf files that you include are not about a topic of NLP and/or LLMs pleas read the system prompt and change it accordingly&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
router_prompt = ChatPromptTemplate.from_messages(
    [
        (&quot;system&quot;,
         &quot;You are an expert router that can direct user queries to the appropriate datasource. Route the following user question about a topic in NLP and LLMs to the appropriate datasource.\nIf it is a general question not related to the provided datasources, route it to the general datasource.\n&quot;),
        (&quot;user&quot;, &quot;{question}&quot;)
    ]
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Finally we are ready to create the first router pipeline for our RAG model and include everythin we have been creating so far. &lt;strong&gt;Not that&lt;/strong&gt; change the upcoming &lt;code&gt;question&lt;/code&gt; accordingly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
router = (
        {'question': RunnablePassthrough()}
        | router_prompt
        | structured_llm
)
question = &quot;How does the A work?&quot;
result = router.invoke(question)
result
result['parsed'].datasource.lower()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; The final step is to define the a function that will choose which database should be used to answer our question, either A or B. In case you are running alongsite this project, the code chunks, and you have replaced  A and B for example, (A could be vectore similiarity and B could be KAN Model) then you could repalce A and B in the following code chunk as well in order to obtain relevant answer/result.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
qa_prompt = hub.pull('rlm/rag-prompt')

def choose_route(result):

    llm_route = AzureChatOpenAI(
        deployment_name = os.getenv('LLM_35_Deployment'),
        model_name = os.getenv('LLM_35_Model'),
        azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),
        temperature = 0

    )
    if &quot;B&quot; in result['parsed'].datasource.lower():
        print(f&quot;&amp;gt; Asking about B ...\nQuestion: {result['parsed'].question}\nAnswer:&quot;)
        B_chain = (
            {'context': retriever_B, 'question': RunnablePassthrough()}
            | qa_prompt
            | llm_route
            | StrOutputParser()
        )
        return B_chain.invoke(result['parsed'].question)
    elif &quot;A&quot; in result['parsed'].datasource.lower():
        print(f&quot;&amp;gt; Asking about A ...\nQuestion: {result['parsed'].question}\nAnswer:&quot;)
        A_chain = (
            {'context': retriever_lora, 'question': RunnablePassthrough()}
            | qa_prompt
            | llm_route
            | StrOutputParser()
        )
        return A_chain.invoke(result['parsed'].question)
    else:
        print(f&quot;&amp;gt; Asking about a general question ...\nQuestion: {result.question}\nAnswer:&quot;)
        general_chain = llm_route | StrOutputParser()
        return general_chain.invoke(result.question)

full_chain = router | RunnableLambda(choose_route)
full_chain.invoke(&quot;What is A?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; That concludes our example for logical routing. Next we are going to change a bit our previous coding chunks and produce an example about semantic routing. In this example we are going to create two templates about different topics. We are going to assume that the users ask questions about physics or mathematics. Initially we need to create two templates that are going to give a character to our LLM according to the question provided.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
physical_template = &quot;&quot;&quot;
You are a very smart physics professor. \
You are great at answering questions about physics in a concise and easy to understand manner. \
When you don't know the answer to a question you admit that you don't know.

Here is a question:
{question}
&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
math_template = &quot;&quot;&quot;You are a very good mathematician. You are great at answering math questions. \
You are so good because you are able to break down hard problems into their component parts, \
answer the component parts, and then put them together to answer the broader question.

Here is a question:
{question}&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to create a list containing the character templates. We are going to create embeddings about those routes in order to calculate the similarity with the question provided by the user. Remember that semantic routing checks the similarity of the meaning after all. The choice of the database that the RAG will provide the answer from is not defined by rules as the previous case.&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
routes = [physical_template, math_template]
route_embeddings = embeddings.embed_documents(routes)
len(route_embeddings)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can proceed to the definition of a function that according to the topic of the question that was provided, LLM will answer the question with a routing strategy to either physics or mathematics experties.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
def router(input):
    # Generate embeddings for the user query
    query_embedding = embeddings.embed_query(input['question'])
    # Getting similarity scores between the user query and the routes. This contains the similarity scores between the user query and each of the two routes.
    similarity = cosine_similarity([query_embedding], route_embeddings)[0]
    # Find the route that gives the maximum similarity score
    route_id = similarity.argmax()
    if route_id == 0:
        print(f&quot;&amp;gt; Asking a physics question ...\nQuestion: {input['question']}\nAnswer:&quot;)
    else:
        print(f&quot;&amp;gt; Asking a math question ...\nQuestion: {input['question']}\nAnswer:&quot;)

    return PromptTemplate.from_template(routes[route_id])

semantic_router_chain = (
    {'question': RunnablePassthrough()}
    | RunnableLambda(router)
    | llm
    | StrOutputParser
)

semantic_router_chain.invoke(&quot;What is the formula for the area of a circle?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; I hope you enjoyed this session as much as I did and a learned a thing or two. I wish that you kept a code chunk and will use it later on, on your own projects. Stay tuned for the next topic that we are going to take a look at this LangChain series&lt;/p&gt;

&lt;p&gt;&lt;i&gt; Be safe, code safer!&lt;/i&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="project" /><summary type="html">I hope you enjoy every step so far. Until this point of our Langchain/RAG journey, we have managed to build a simple local application and a querry transformation assistant. But what happens when we have multiple data sources? How to define where our application will retrieve the required information from? The definition of this process, finding the correct road, or better let's say finding the correct route, is called Routing.</summary></entry><entry><title type="html">Google’s Illuminate - Transforming Academic Papers into AI-Generated Podcasts</title><link href="http://localhost:4000/GoogleIllumnate" rel="alternate" type="text/html" title="Google’s Illuminate - Transforming Academic Papers into AI-Generated Podcasts" /><published>2024-09-09T00:00:00+03:00</published><updated>2024-09-09T00:00:00+03:00</updated><id>http://localhost:4000/GoogleIllumnate</id><content type="html" xml:base="http://localhost:4000/GoogleIllumnate">&lt;p&gt;Google Labs has a long tradition of inviting users to explore innovative technologies, with notable successes like Gmail, which began as an exclusive beta. Now, Google is unveiling Illuminate, a groundbreaking project that transforms academic papers into AI-generated audio discussions, styled like an NPR podcast. The concept is straightforward but powerful: Google's large language model, Gemini, creates a concise summary of a research paper and follows it up with a Q&amp;amp;A session. These are voiced by two AI-generated personas—a male interviewer and a female expert—who guide listeners through a brief, engaging conversation about the paper’s key points.

What makes Illuminate especially valuable is its potential to make academic content more accessible and convenient. With this service, you can listen to insightful summaries of research papers while you're on the go—whether you're working out, commuting, or simply multitasking. The platform could also easily be adapted for other types of audio narration, extending its utility to a wide range of subjects and industries. Currently in private beta, Illuminate allows you to join the waitlist if you’re interested in early access and testing out this exciting new tool. You can check out samples on the Google Illuminate website to see its potential firsthand.&lt;/p&gt;

&lt;p&gt;Google Labs has always been at the forefront of innovation, frequently inviting users to explore groundbreaking tech. Remember Gmail? It started as a private beta before revolutionizing how we manage email. Today, Google is back with another fascinating project, &lt;strong&gt;Illuminate&lt;/strong&gt;. This new initiative takes a fresh approach to academic research, turning complex papers into digestible, AI-generated audio discussions styled after popular NPR podcasts.&lt;/p&gt;

&lt;p&gt;So, what exactly is Illuminate, and why is it worth your attention?&lt;/p&gt;

&lt;h3&gt;The Power Behind Illuminate&lt;/h3&gt;

&lt;p&gt; At the heart of Illuminate lies Google’s large language model, Gemini, which does the heavy lifting. Here’s how it works: Gemini generates a concise summary of an academic paper and pairs it with a Q&amp;amp;A session. Two AI-generated voices—a male interviewer and a female expert—then present the content in the form of an engaging, short interview. Imagine being guided through the highlights of a research paper as if you were listening to a lively radio discussion.&lt;/p&gt;

&lt;h3&gt; Why Is This So Useful?&lt;/h3&gt;

&lt;p&gt; For anyone who regularly dives into research papers, you know the grind. The sheer volume of academic literature can be overwhelming, and finding the time to sit down and read every paper in full is often a challenge. This is where Illuminate shines. It offers a new way to stay informed by converting these dense academic texts into easily consumable audio discussions. Picture yourself catching up on the latest research while driving to work, hitting the gym, or even while doing chores around the house. It’s multitasking made smarter.&lt;/p&gt;

&lt;p&gt; Illuminate isn’t just limited to academic papers either. The platform’s potential extends far beyond that, with the ability to adapt to other forms of narration. Whether for educational content, business reports, or any document-heavy field, this kind of AI-generated podcasting could become a versatile tool for delivering information in a more accessible and engaging format.&lt;/p&gt;

&lt;h3&gt; A Glimpse Into the Future&lt;/h3&gt;

&lt;p&gt;Illuminate is still in its early stages, currently available as a private beta. Google is letting select users test the waters while they continue refining the service. If you’re eager to see what the future of academic content consumption could look like, you can join the waitlist for access.&lt;/p&gt;

&lt;p&gt;Curious to experience it for yourself? You can check out some sample audio discussions on the &lt;a href=&quot;https://illuminate.google.com/home?pli=1&quot;&gt;Google Illuminate&lt;/a&gt; website. Whether you’re an academic researcher, a student, or just someone who loves learning on the go, Illuminate has the potential to change how we interact with complex information.&lt;/p&gt;

&lt;p&gt;In the end, this is more than just a novelty. It’s a glimpse into how AI can reshape content delivery, making it more convenient, engaging, and adaptable to our daily routines. The future of learning might just sound a lot like your favorite podcast.&lt;/p&gt;

&lt;iframe width=&quot;1175&quot; height=&quot;661&quot; src=&quot;https://www.youtube.com/embed/mxlPGgfMJJs&quot; title=&quot;Using long-context to make knowledge accessible&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Stay tuned, because Illuminate could be the next big step in AI-driven information consumption.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Google Labs has a long tradition of inviting users to explore innovative technologies, with notable successes like Gmail, which began as an exclusive beta. Now, Google is unveiling Illuminate, a groundbreaking project that transforms academic papers into AI-generated audio discussions, styled like an NPR podcast. The concept is straightforward but powerful: Google's large language model, Gemini, creates a concise summary of a research paper and follows it up with a Q&amp;amp;A session. These are voiced by two AI-generated personas—a male interviewer and a female expert—who guide listeners through a brief, engaging conversation about the paper’s key points.</summary></entry><entry><title type="html">Exploring the Replit Agent - AI Power Coding for Developers</title><link href="http://localhost:4000/ReplitAgent" rel="alternate" type="text/html" title="Exploring the Replit Agent - AI Power Coding for Developers" /><published>2024-09-09T00:00:00+03:00</published><updated>2024-09-09T00:00:00+03:00</updated><id>http://localhost:4000/ReplitAgent</id><content type="html" xml:base="http://localhost:4000/ReplitAgent">&lt;p&gt;Replit has long been at the forefront of integrating AI into software development, and their latest offering, the &lt;strong&gt;Replit Agent&lt;/strong&gt;, is no exception. Currently available through a limited early access program, this AI-powered assistant is designed to help users create software projects from scratch using simple, natural language prompts. Whether you’re a seasoned developer or new to coding, Replit Agent aims to make building applications more intuitive and accessible.&lt;/p&gt;

&lt;h3&gt; What is the Replit Agent? &lt;/h3&gt;

&lt;p&gt;The Replit Agent is an experimental AI tool designed to assist developers in building software projects. It understands natural language commands, allowing users to describe what they want to build, and then the agent takes over—planning, writing, and deploying code. Whether you’re working on web-based applications or prototyping new ideas, the agent is meant to accelerate the development process by generating code, suggesting solutions, and helping manage tasks.&lt;/p&gt;

&lt;h3&gt; How It Works &lt;/h3&gt;

&lt;p&gt;To get started with the Replit Agent, subscribers to Replit Core or Replit Teams can dive right in. The agent is included in these plans at no extra cost during the early access phase, though pricing details for general release will be announced later in 2024. Here’s Replit CEO Amjad Masad with an overview of how the agent works:

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://www.youtube.com/embed/IYiVPrxY8-Y&quot; title=&quot;Meet the Replit Agent&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

Here’s how you can start using it:

On the Web:
&lt;ol&gt;
&lt;li&gt; Log into your Replit account with a Core or Teams subscription. &lt;/li&gt;
&lt;li&gt; Visit the home page or select Create Repl in the left navigation. &lt;/li&gt;
&lt;li&gt; Input a prompt of what you would like the agent to build. 
    &lt;ul&gt;
    &lt;li&gt; A good prompt is descriptive and detailed. Imagine you are describing a task for a teammate at work to complete. What information would they have to know to get the job done?&lt;/li&gt;
    &lt;li&gt; We recommend letting the agent select which technologies to use rather than specifying specific languages or frameworks. &lt;/li&gt;
    &lt;li&gt; The agent is currently best at 0 -&amp;gt; 1 prototyping for web-based applications.&lt;/li&gt;
    &lt;/ul&gt;
    &amp;lt;img src=”https://docimg.replit.com/images/replitai/agent_01.png”&amp;gt;
&lt;li&gt; Review and iterate on the plan the agent generated. Feel free to edit or delete steps that the agent recommends. &lt;/li&gt;
    &lt;img src=&quot;https://docimg.replit.com/images/replitai/agent_02.png&quot; /&gt;
&lt;li&gt; Follow along with the agent’s progress. &lt;/li&gt;
    &lt;img src=&quot;https://docimg.replit.com/images/replitai/agent_03.png&quot; /&gt;
&lt;li&gt; Work with the agent to provide API keys, feedback, or direction as it progresses in building your application. &lt;/li&gt;
    &lt;img src=&quot;https://docimg.replit.com/images/replitai/agent_04.png&quot; /&gt;
&lt;li&gt; Test your application and ask follow up questions as needed. &lt;/li&gt;
&lt;li&gt; Deploy your application to production! Learn more about Replit Deployments. &lt;/li&gt;
&amp;lt;/ol&amp;gt;

On Mobile:
&lt;ol&gt;
&lt;li&gt;Download the Replit app (version 2.90.2 or higher) via this link or QR code.&lt;/li&gt;
    &lt;img src=&quot;https://docimg.replit.com/images/replitai/agent_05.png&quot; /&gt;
&lt;li&gt;Ensure you're logged in with the email associated with your Core or Teams account.&lt;/li&gt;
&lt;li&gt;Head to the &quot;Create&quot; tab and select &quot;Start with AI,&quot; then describe the project you want to build.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently, the Replit Agent excels at prototyping web-based applications, but there’s potential for it to grow into more diverse use cases. Keep in mind that while it’s still in the experimental phase, you might encounter the occasional error or unexpected behavior. Your feedback will be crucial in improving the product as it evolves.&lt;/p&gt;

&lt;h3&gt; The Future of AI in Software Development &lt;/h3&gt;

&lt;p&gt; The vision behind Replit Agent is bigger than just an AI assistant for coding—it’s a step toward closer collaboration between humans and machines in the software development process. Replit envisions a future where AI agents not only fill in gaps but also enhance developers’ capabilities, offering creative solutions, and speeding up workflows. As you experiment with the Replit Agent, you’ll be helping to shape what this future looks like.&lt;/p&gt;

&lt;h3&gt; Getting Early Access&lt;/h3&gt;

&lt;p&gt;Replit Agent is currently in early access, available to Core and Teams subscribers. If you're subscribed to one of these plans, you can start using the agent today without any additional cost. While usage limits are in place during this phase, these quotas are refreshed regularly, giving you ongoing opportunities to test and experiment with the agent’s capabilities.&lt;/p&gt;

&lt;h3&gt; Final Thoughts &lt;/h3&gt;

&lt;p&gt;The Replit Agent represents an exciting new chapter in AI-assisted development, bringing us closer to a world where machines and humans collaborate seamlessly on software projects. Whether you're building a prototype or simply looking to experiment with AI in your workflow, Replit Agent offers a promising glimpse into the future of development. Don’t miss out—if you’re eligible, dive into early access today and start shaping the future of AI-driven coding. &lt;/p&gt;

&lt;p&gt; To read more about this brand new Replit product head &lt;a href=&quot;https://docs.replit.com/replitai/agent?ref=maginative.com&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;&lt;/ol&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Replit has long been at the forefront of integrating AI into software development, and their latest offering, the Replit Agent, is no exception. Currently available through a limited early access program, this AI-powered assistant is designed to help users create software projects from scratch using simple, natural language prompts. Whether you’re a seasoned developer or new to coding, Replit Agent aims to make building applications more intuitive and accessible.</summary></entry><entry><title type="html">Theory, Analysis, and Best Practices for Sigmoid Self-Attention</title><link href="http://localhost:4000/SigmoidSelfAttntion" rel="alternate" type="text/html" title="Theory, Analysis, and Best Practices for Sigmoid Self-Attention" /><published>2024-09-06T00:00:00+03:00</published><updated>2024-09-06T00:00:00+03:00</updated><id>http://localhost:4000/SigmoidSelfAttntion</id><content type="html" xml:base="http://localhost:4000/SigmoidSelfAttntion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ramapuram,+J&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Danieli,+F&quot;&gt;Federico Danieli&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dhekane,+E&quot;&gt;Eeshan Dhekane&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Weers,+F&quot;&gt;Floris Weers&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Busbridge,+D&quot;&gt;Dan Busbridge&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ablin,+P&quot;&gt;Pierre Ablin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Likhomanenko,+T&quot;&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Digani,+J&quot;&gt;Jagrit Digani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Gu,+Z&quot;&gt;Zijin Gu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shidani,+A&quot;&gt;Amitis Shidani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Webb,+R&quot;&gt;Russ Webb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.04431&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers</title><link href="http://localhost:4000/LLMNovelResearchIdeas" rel="alternate" type="text/html" title="Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers" /><published>2024-09-06T00:00:00+03:00</published><updated>2024-09-06T00:00:00+03:00</updated><id>http://localhost:4000/LLMNovelResearchIdeas</id><content type="html" xml:base="http://localhost:4000/LLMNovelResearchIdeas">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p &amp;lt; 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Si,+C&quot;&gt;Chenglei Si&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+D&quot;&gt;Diyi Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hashimoto,+T&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;&amp;lt;/a&amp;gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://www.arxiv.org/abs/2409.04109&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">OLMoE-Open Mixture of Experts Language Models</title><link href="http://localhost:4000/OLMoE" rel="alternate" type="text/html" title="OLMoE-Open Mixture of Experts Language Models" /><published>2024-09-03T00:00:00+03:00</published><updated>2024-09-03T00:00:00+03:00</updated><id>http://localhost:4000/OLMoE</id><content type="html" xml:base="http://localhost:4000/OLMoE">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muennighoff,+N&quot;&gt;Niklas Muennighoff&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Soldaini,+L&quot;&gt;Luca Soldaini&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Groeneveld,+D&quot;&gt;Dirk Groeneveld&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lo,+K&quot;&gt;Kyle Lo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Morrison,+J&quot;&gt;Jacob Morrison&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Min,+S&quot;&gt;Sewon Min&lt;/a&gt;,  &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shi,+W&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Walsh,+P&quot;&gt;Pete Walsh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Tafjord,+O&quot;&gt;Oyvind Tafjord&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Lambert,+N&quot;&gt;Nathan Lambert&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Gu,+Y&quot;&gt;Yuling Gu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Arora,+S&quot;&gt;Shane Arora&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Bhagia,+A&quot;&gt;Akshita Bhagia&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Schwenk,+D&quot;&gt;Dustin Schwenk&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wadden,+D&quot;&gt;David Wadden&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wettig,+A&quot;&gt;Alexander Wettig&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hui,+B&quot;&gt;Binyuan Hui&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dettmers,+T&quot;&gt;Tim Dettmers&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Kiela,+D&quot;&gt;Douwe Kiela&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Farhadi,+A&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Smith,+N+A&quot;&gt;Noah A. Smith&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Koh,+P+W&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Singh,+A&quot;&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hajishirzi,+H&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.02060&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">In Defense of RAG in the Era of Long-Context Language Models</title><link href="http://localhost:4000/DefenceOfRAG" rel="alternate" type="text/html" title="In Defense of RAG in the Era of Long-Context Language Models" /><published>2024-09-03T00:00:00+03:00</published><updated>2024-09-03T00:00:00+03:00</updated><id>http://localhost:4000/DefenceOfRAG</id><content type="html" xml:base="http://localhost:4000/DefenceOfRAG">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Akkiraju,+R&quot;&gt;Rama Akkiraju&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+T&quot;&gt;Tan Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Xu,+A&quot;&gt;Anbang Xu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.01666&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">FLUX that Plays Music</title><link href="http://localhost:4000/FLUXthatPlaysMusic" rel="alternate" type="text/html" title="FLUX that Plays Music" /><published>2024-09-01T00:00:00+03:00</published><updated>2024-09-01T00:00:00+03:00</updated><id>http://localhost:4000/FLUXthatPlaysMusic</id><content type="html" xml:base="http://localhost:4000/FLUXthatPlaysMusic">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; This paper explores a simple extension of diffusion-based rectified flow Transformers for text-to-music generation, termed as FluxMusic. Generally, along with design in advanced Flux\footnote{this https URL} model, we transfers it into a latent VAE space of mel-spectrum. It involves first applying a sequence of independent attention to the double text-music stream, followed by a stacked single music stream for denoised patch prediction. We employ multiple pre-trained text encoders to sufficiently capture caption semantic information as well as inference flexibility. In between, coarse textual information, in conjunction with time step embeddings, is utilized in a modulation mechanism, while fine-grained textual details are concatenated with the music patch sequence as inputs. Through an in-depth study, we demonstrate that rectified flow training with an optimized architecture significantly outperforms established diffusion methods for the text-to-music task, as evidenced by various automatic metrics and human preference evaluations. Our experimental data, code, and model weights are made publicly available at: &lt;a href=&quot;https://github.com/feizc/FluxMusic&quot;&gt;this URL&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fei,+Z&quot;&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fan,+M&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yu,+C&quot;&gt;Changqian Yu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Huang,+J&quot;&gt;Junshi Huang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.00587&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Introducing LLaVA V1.5 7B on GroqCloud</title><link href="http://localhost:4000/GroqLlava" rel="alternate" type="text/html" title="Introducing LLaVA V1.5 7B on GroqCloud" /><published>2024-08-27T00:00:00+03:00</published><updated>2024-08-27T00:00:00+03:00</updated><id>http://localhost:4000/GroqLlava</id><content type="html" xml:base="http://localhost:4000/GroqLlava">&lt;p&gt;Introducing LLaVA v1.5 7B: The Next Level of Multimodal AI on GroqCloud&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://console.groq.com/?_gl=1*1v016td*_ga*OTczMzMxMDEyLjE3MjU4NjM4OTU.*_ga_4TD0X2GEZG*MTcyNTg2MjYzNS4xLjEuMTcyNTg2Mzg4Ni42MC4wLjA.&quot;&gt;GroqCloud&lt;/a&gt; has launched &lt;a href=&quot;https://huggingface.co/liuhaotian/llava-v1.5-7b&quot;&gt;LLaVA v1.5 7B&lt;/a&gt;, a state-of-the-art multimodal AI model that combines language, vision, and auditory capabilities.&lt;/p&gt;

&lt;p&gt;LLaVA stands for &lt;i&gt;Large Language and Vision Assistant&lt;/i&gt;, a powerful multimodal model that combines the strengths of language and vision. Based on OpenAI’s CLIP and a fine-tuned version of Meta’s Llama 2 7B model, LLaVA uses visual instruction tuning to support image-based natural instruction following and visual reasoning capabilities. This allows LLaVA to perform a range of tasks, including:

&lt;ul&gt;
&lt;li&gt; Visual question answering: answering questions based on image content&lt;/li&gt;
&lt;li&gt; Caption generation: generating text descriptions of images&lt;/li&gt;
&lt;li&gt; Optical Character Recognition: identifying text in image&lt;/li&gt;
&lt;li&gt; Multimodal dialogue: engaging in conversations that involve both text and images&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When trained in September 2023, LLaVA v1.5 achieved state-of-the-art performance on a total of 7 benchmarks, including 5 academic VQA benchmarks. This demonstrates the model’s exceptional capabilities in understanding and generating text based on visual inputs.&lt;/p&gt;

&lt;p&gt;Use Cases and Industry Benefits LLaVA v1.5 7B can transform industries like retail, finance, education, and manufacturing. Retailers can monitor inventory using image recognition, customer service chatbots can handle text and image queries, and factory lines can automate defect detection. In education, it can assist students by analyzing diagrams and generating explanations.&lt;/p&gt;

&lt;p&gt;Real-World Applications From visual question answering in retail to image captioning for accessibility, LLaVA v1.5 opens up endless possibilities. It can aid quality control on factory lines, automate finance audits by analyzing documents, or enhance the learning experience with detailed image explanations.&lt;/p&gt;

&lt;p&gt;Get Started with GroqCloud LLaVA v1.5 7B is now available on the &lt;a href=&quot;https://console.groq.com/?_gl=1*1pu1tte*_ga*OTczMzMxMDEyLjE3MjU4NjM4OTU.*_ga_4TD0X2GEZG*MTcyNTg2MjYzNS4xLjEuMTcyNTg2Mzg4Ni42MC4wLjA.&quot;&gt;GroqCloud Developer Console&lt;/a&gt;, allowing developers to experiment with its multimodal capabilities. This release marks GroqCloud’s expansion into supporting three modalities—image, audio, and text—offering immense potential for building innovative, real-world applications.&lt;/p&gt;

&lt;p&gt;The Future of Multimodal AI With LLaVA v1.5 7B, developers can push the boundaries of what’s possible by seamlessly integrating visual, auditory, and textual inputs, unlocking a future where AI can understand and generate complex multimodal interactions. Start building with LLaVA today on GroqCloud and lead the way in the AI revolution.&lt;/p&gt;

&lt;p&gt;Check full official article from &lt;a href=&quot;https://groq.com/introducing-llava-v1-5-7b-on-groqcloud-unlocking-the-power-of-multimodal-ai/&quot;&gt;Groq blog&lt;/a&gt;.&lt;/p&gt;
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Introducing LLaVA v1.5 7B: The Next Level of Multimodal AI on GroqCloud</summary></entry><entry><title type="html">Introducing Cerebras Inference - AI at Instant Speed</title><link href="http://localhost:4000/cerebras" rel="alternate" type="text/html" title="Introducing Cerebras Inference - AI at Instant Speed" /><published>2024-08-27T00:00:00+03:00</published><updated>2024-08-27T00:00:00+03:00</updated><id>http://localhost:4000/cerebras</id><content type="html" xml:base="http://localhost:4000/cerebras">&lt;p&gt; Cerebras has unveiled its new AI inference solution, claiming it to be the fastest in the world, outpacing NVIDIA GPU-based clouds by 20 times and delivering industry-leading cost efficiency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cerebras.ai/wp-content/uploads/2024/08/Screenshot-2024-08-26-at-11.39.02%E2%80%AFPM.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt; Powered by the third-generation Wafer Scale Engine (WSE-3), this solution can process 1,800 tokens per second for Llama3.1 8B models and 450 tokens per second for Llama3.1 70B models, all while maintaining high accuracy with native 16-bit weights. The system's exceptional memory bandwidth and unique chip design eliminate traditional bottlenecks, enabling real-time AI responses. With open API access and competitive pricing, Cerebras Inference aims to revolutionize the development and deployment of large language models (LLMs) across various industries.&lt;/p&gt;

&lt;p&gt; This breakthrough allows for more sophisticated AI workflows, such as enhanced real-time intelligence and complex tasks like code generation, which previously required extensive processing power and time. As Cerebras expands support to even larger models, its platform is set to open new possibilities in AI innovation.&lt;/p&gt;

&lt;p&gt; To read more and benefit from those changes you can find out more &lt;a href=&quot;https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Cerebras has unveiled its new AI inference solution, claiming it to be the fastest in the world, outpacing NVIDIA GPU-based clouds by 20 times and delivering industry-leading cost efficiency.</summary></entry></feed>