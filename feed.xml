<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-26T13:28:44+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">Empowering YouTube creators with generative AI</title><link href="http://localhost:4000/EmpYoutube" rel="alternate" type="text/html" title="Empowering YouTube creators with generative AI" /><published>2024-09-18T00:00:00+03:00</published><updated>2024-09-18T00:00:00+03:00</updated><id>http://localhost:4000/EmpYoutube</id><content type="html" xml:base="http://localhost:4000/EmpYoutube">&lt;p&gt;YouTube is aiming to change that through the introduction of advanced generative AI tools that will help millions of creators realize their creative visions. By integrating cutting-edge AI models into its platform, YouTube aims to make video generation more accessible and intuitive, particularly through its YouTube Shorts feature.&lt;/p&gt;

&lt;video class=&quot;gdm-video-embed__player&quot; muted=&quot;&quot; playsinline=&quot;&quot; loop=&quot;&quot; data-autoplay=&quot;true&quot; autoplay=&quot;&quot;&gt;
    &lt;source src=&quot;https://deepmind.google/api/blob/website/media/Veo_CreationMediaPicker.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;YouTube is introducing &lt;a href=&quot;https://blog.youtube/news-and-events/made-on-youtube-2024&quot;&gt;Dream Screen&lt;/a&gt;, a new feature that allows creators to generate dynamic video content for Shorts using two advanced AI models—&lt;a href=&quot;https://deepmind.google/technologies/veo/&quot;&gt;Veo&lt;/a&gt; and &lt;a href=&quot;https://deepmind.google/technologies/imagen-3/&quot;&gt;Imagen 3&lt;/a&gt;. These models, built upon Google's decade-long innovation in &lt;a href=&quot;https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/&quot;&gt;Transformer architecture&lt;/a&gt; and diffusion models, have been refined for large-scale use. Dream Screen allows creators to input text prompts, generating four distinct images from which they can choose their preferred visual style. From there, Veo transforms the selected image into a high-quality, six-second video clip. These AI-generated video clips will initially be available for use as backgrounds, with standalone six-second video generation rolling out in 2025.&lt;/p&gt;

&lt;p&gt;By launching these generative AI tools, YouTube is enhancing creative possibilities for its vast user base. The company hopes these innovations will inspire creators to bring their ideas to life in vivid and transformative ways. In addition to ensuring accessibility, YouTube is committed to transparency by labeling AI-generated content with watermarks and notifications. These advancements mark a significant step forward in democratizing creativity, empowering creators globally to push the boundaries of their imagination. To read more click &lt;a href=&quot;https://deepmind.google/discover/blog/empowering-youtube-creators-with-generative-ai/?utm_source=x&amp;amp;utm_medium=social&amp;amp;utm_campaign=&amp;amp;utm_content=&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">YouTube is aiming to change that through the introduction of advanced generative AI tools that will help millions of creators realize their creative visions. By integrating cutting-edge AI models into its platform, YouTube aims to make video generation more accessible and intuitive, particularly through its YouTube Shorts feature.</summary></entry><entry><title type="html">Open Source Generative AI Platform by Together AI</title><link href="http://localhost:4000/Llamacoder" rel="alternate" type="text/html" title="Open Source Generative AI Platform by Together AI" /><published>2024-09-18T00:00:00+03:00</published><updated>2024-09-18T00:00:00+03:00</updated><id>http://localhost:4000/Llamacoder</id><content type="html" xml:base="http://localhost:4000/Llamacoder">&lt;p&gt; &lt;a href=&quot;https://www.together.ai/&quot;&gt;Together AI&lt;/a&gt;, a leading AI acceleration cloud, is transforming the way developers and businesses design, develop, and manage generative AI applications. By focusing on open-source models like &lt;a href=&quot;https://www.llama.com/&quot;&gt;Llama&lt;/a&gt;, Together AI is enabling developers to seamlessly navigate the entire AI lifecycle with tools that are both accessible and powerful. With the launch of innovative applications like &lt;a href=&quot;https://llamacoder.together.ai/&quot;&gt;LlamaCoder&lt;/a&gt;, the company continues to push the boundaries of what open-source generative AI can achieve.&lt;/p&gt;

&lt;p&gt;To inspire developers working with Llama models, Together AI created LlamaCoder, an open-source web application that generates full applications from simple text prompts using the Llama 3.1 405B model. Since its release, LlamaCoder has gained rapid popularity, with more than 200,000 apps generated and over 2,000 GitHub stars. This success highlights the potential of Llama 3.1 405B, the first open-source model to excel at coding-based use cases. Developers have used LlamaCoder to create a range of applications, including quiz apps, pomodoro timers, and budgeting tools, demonstrating the model’s versatility and power.&lt;/p&gt;

&lt;p&gt;In addition to LlamaCoder, Together AI has also developed other example apps using Llama 3.1, such as &lt;a href=&quot;https://llamatutor.together.ai/&quot;&gt;LlamaTutor&lt;/a&gt; for learning and &lt;a href=&quot;https://www.turboseek.io/&quot;&gt;TurboSeek&lt;/a&gt;, an AI-powered search engine. These applications showcase the extensive capabilities of the Llama models, which rival closed-source models while maintaining robust safety features for responsible development.&lt;/p&gt;

&lt;video class=&quot;gdm-video-embed__player&quot; muted=&quot;&quot; playsinline=&quot;&quot; loop=&quot;&quot; data-autoplay=&quot;true&quot; autoplay=&quot;&quot;&gt;
    &lt;source src=&quot;https://video.fath7-1.fna.fbcdn.net/o1/v/t2/f2/m69/AQPsmAks9NYq2Exz3Q9ipg8z8OSFrGOZ4-feeJNtDRttY4W0Ln48hJy6bDsH7pJ2ayMUG-8N_hYAA2V1tmtzpIOs.mp4?efg=eyJ2ZW5jb2RlX3RhZyI6Im9lcF9oZCJ9&amp;amp;_nc_ht=video.fath7-1.fna.fbcdn.net&amp;amp;_nc_cat=100&amp;amp;strext=1&amp;amp;vs=1352f9b7ee1846d2&amp;amp;_nc_vs=HBkcFQIYOnBhc3N0aHJvdWdoX2V2ZXJzdG9yZS9HTWRhYWh0aDlPU0hHNWdCQUxpZFNwYWV2UlFkYm1kakFBQUYVAALIAQBLB4gScHJvZ3Jlc3NpdmVfcmVjaXBlATENc3Vic2FtcGxlX2ZwcwAQdm1hZl9lbmFibGVfbnN1YgAgbWVhc3VyZV9vcmlnaW5hbF9yZXNvbHV0aW9uX3NzaW0AKGNvbXB1dGVfc3NpbV9vbmx5X2F0X29yaWdpbmFsX3Jlc29sdXRpb24AHXVzZV9sYW5jem9zX2Zvcl92cW1fdXBzY2FsaW5nABFkaXNhYmxlX3Bvc3RfcHZxcwAVACUAHIwXQAAAAAAAAAAREQAAACbAw4v_pOaIBRUCKAJDMxgLdnRzX3ByZXZpZXccF0BAk5WBBiTdGBlkYXNoX2gyNjQtYmFzaWMtZ2VuMl83MjBwEgAYGHZpZGVvcy52dHMuY2FsbGJhY2sucHJvZDgSVklERU9fVklFV19SRVFVRVNUGwqIFW9lbV90YXJnZXRfZW5jb2RlX3RhZwZvZXBfaGQTb2VtX3JlcXVlc3RfdGltZV9tcwEwDG9lbV9jZmdfcnVsZQd1bm11dGVkE29lbV9yb2lfcmVhY2hfY291bnQDOTk3EW9lbV9pc19leHBlcmltZW50AAxvZW1fdmlkZW9faWQPODY5NjA1MDU1MTM0NDIwEm9lbV92aWRlb19hc3NldF9pZA80MTIyMDc2MDg1NTk4ODgVb2VtX3ZpZGVvX3Jlc291cmNlX2lkEDE0MjY3MjQzODEzNTYyNTYcb2VtX3NvdXJjZV92aWRlb19lbmNvZGluZ19pZA84Mzc1NDQwNTE5MTQ3NDkOdnRzX3JlcXVlc3RfaWQAJQIcACW-ARsHiAFzBDkyMDQCY2QKMjAyNC0wOS0xNwNyY2IDOTAwA2FwcAVWaWRlbwJjdBFDTVNfTUVESUFfTUFOQUdFUhNvcmlnaW5hbF9kdXJhdGlvbl9zCTMzLjEzMzMzMwJ0cxVwcm9ncmVzc2l2ZV9lbmNvZGluZ3MA&amp;amp;ccb=9-4&amp;amp;oh=00_AYDGX13YwRdhTcLpLp10vqWSwagUlMQ06M72ImpDH8FhZg&amp;amp;oe=66F71DBF&amp;amp;_nc_sid=1d576d&amp;amp;_nc_rid=860686001678256&amp;amp;_nc_store_type=1&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;With more than 150,000 developers and companies using the Together AI platform, the applications for Llama models are expanding across industries—from gaming to customer service and AI-driven benchmarks. The platform’s advanced inference engine, powered by technologies like &lt;a href=&quot;https://arxiv.org/html/2407.08608v1&quot;&gt;FlashAttention-3 kernels&lt;/a&gt; and RedPajama-based speculators, ensures unmatched performance and cost-efficiency for generative AI applications. Together AI’s commitment to open-source innovation is driving the rapid adoption of its platform, allowing developers and enterprises to maintain control over their data and models while fostering faster technological advancements. Find out more &lt;a href=&quot;https://ai.meta.com/blog/together-ai-llamacoder/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Together AI, a leading AI acceleration cloud, is transforming the way developers and businesses design, develop, and manage generative AI applications. By focusing on open-source models like Llama, Together AI is enabling developers to seamlessly navigate the entire AI lifecycle with tools that are both accessible and powerful. With the launch of innovative applications like LlamaCoder, the company continues to push the boundaries of what open-source generative AI can achieve.</summary></entry><entry><title type="html">Runway’s Gen-3 Alpha Video-to-Video</title><link href="http://localhost:4000/Vid2VidRunaway" rel="alternate" type="text/html" title="Runway’s Gen-3 Alpha Video-to-Video" /><published>2024-09-14T00:00:00+03:00</published><updated>2024-09-14T00:00:00+03:00</updated><id>http://localhost:4000/Vid2VidRunaway</id><content type="html" xml:base="http://localhost:4000/Vid2VidRunaway">&lt;p&gt;Runway has officially launched its Gen-3 Alpha Video-to-Video feature, which is now available on the web for all paid plans. This tool allows users to modify existing video content using simple text prompts.&lt;/p&gt;

&lt;p&gt;In the rapidly evolving world of AI-powered creativity, Runway has once again pushed the boundaries with the launch of its Gen-3 Alpha Video-to-Video feature. This innovative tool enables users to transform existing videos through simple text prompts, unlocking a new realm of creative possibilities. Whether it’s changing the weather, altering a scene’s style, or manipulating objects within a frame, Runway’s latest offering brings advanced video editing capabilities to anyone with a paid subscription. Let’s explore the key features, capabilities, and how to start using this game-changing tool.&lt;/p&gt;

&lt;p&gt;Runway’s Gen-3 Alpha Video-to-Video tool introduces several groundbreaking features that allow for easy, intuitive video transformation:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt; &lt;strong&gt;Text-to-Video Transformation&lt;/strong&gt;
&lt;p&gt;One of the most notable aspects of this tool is its ability to modify video content based on user-provided text descriptions. For instance, with a prompt like &quot;change the background to a beach sunset,&quot; the AI alters the video scene accordingly while preserving the original subjects and actions. This makes it easier for creators to bring new ideas to life without needing advanced video editing skills.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt; &lt;strong&gt;Style Transfer&lt;/strong&gt;
&lt;p&gt;With the Video-to-Video tool, users can apply different visual styles to their footage, allowing for experimentation with a wide range of aesthetics. From retro film grain to futuristic sci-fi looks, this feature enhances the creative flexibility of video projects, offering a quick way to change the entire atmosphere of a video.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt; &lt;strong&gt;Weather and Lighting Adjustments&lt;/strong&gt;
&lt;p&gt;Runway’s new tool goes beyond basic editing by enabling users to adjust environmental conditions in their videos. Creators can introduce elements like rain, snow, or even change the time of day within a scene. This opens up exciting new storytelling possibilities, allowing users to craft dynamic and engaging content with just a few clicks.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt; &lt;strong&gt;Object Manipulation&lt;/strong&gt;
&lt;p&gt;For content creators looking to enhance their videos with special effects or product placements, the Video-to-Video feature enables precise manipulation of objects within a frame. This includes adding, removing, or altering objects—ideal for fixing continuity issues, introducing new elements, or customizing the visual narrative of a project.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Gen-3 Alpha Video to Video is now available on web for all paid plans. Video to Video represents a new control mechanism for precise movement, expressiveness and intent within generations. To use Video to Video, simply upload your input video, prompt in any aesthetic direction… &lt;a href=&quot;https://t.co/ZjRwVPyqem&quot;&gt;pic.twitter.com/ZjRwVPyqem&lt;/a&gt;&lt;/p&gt;&amp;mdash; Runway (@runwayml) &lt;a href=&quot;https://twitter.com/runwayml/status/1834711758335779300?ref_src=twsrc%5Etfw&quot;&gt;September 13, 2024&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;Getting started with Runway’s Gen-3 Alpha Video-to-Video feature is straightforward. Here’s a step-by-step guide:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; Upload Input Video: Begin by uploading the video you want to modify to Runway’s platform.&lt;/li&gt;
&lt;li&gt; Prompt Aesthetic Direction: Once uploaded, you can enter a text prompt to direct the AI in modifying the video. Alternatively, users can select from preset styles that match their creative vision.&lt;/li&gt;
&lt;li&gt; AI Processing: The AI analyzes the video frame by frame, identifying key elements such as subjects, lighting, and backgrounds. It then processes the footage based on the instructions, generating new frames that seamlessly integrate the changes with the original content.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The entire process is streamlined, making it accessible for both seasoned professionals and newcomers alike.&lt;/p&gt;

&lt;h3&gt; Availability and Pricing &lt;/h3&gt;

&lt;p&gt;Runway has made its Gen-3 Alpha Video-to-Video feature available on the web for all paid plans. Here are the details regarding costs and supported video durations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; Cost: Pricing is based on video length, with videos 5 seconds or shorter costing 50 credits, and videos longer than 5 seconds costing 100 credits.&lt;/li&gt;
&lt;li&gt; Supported Durations: Users can transform videos up to 10 seconds in length.&lt;/li&gt;
&lt;li&gt; Platform: The feature is accessible via &lt;a href=&quot;https://www.testingcatalog.com/tag/runway/&quot;&gt;Runway&lt;/a&gt;’s web platform, making it convenient for users with paid plans to start transforming their videos immediately.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Runway’s Gen-3 Alpha Video-to-Video feature brings cutting-edge AI-driven video editing capabilities to a broader audience. Whether it’s changing a video’s style, manipulating objects, or adjusting environmental elements, this tool empowers creators to make impressive changes with minimal effort. With its simple interface and powerful AI, Runway’s latest offering promises to be a game-changer in video content creation, making professional-level edits accessible to everyone. To read more visit the official &amp;lt;a href=https://www.testingcatalog.com/runway-released-gen-3-alpha-video-to-video-feature-for-paid-plans/'&amp;gt;blog post&amp;lt;/a&amp;gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Runway has officially launched its Gen-3 Alpha Video-to-Video feature, which is now available on the web for all paid plans. This tool allows users to modify existing video content using simple text prompts.</summary></entry><entry><title type="html">Breaking reCAPTCHAv2</title><link href="http://localhost:4000/reCaptcha" rel="alternate" type="text/html" title="Breaking reCAPTCHAv2" /><published>2024-09-13T00:00:00+03:00</published><updated>2024-09-13T00:00:00+03:00</updated><id>http://localhost:4000/reCaptcha</id><content type="html" xml:base="http://localhost:4000/reCaptcha">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Our work examines the efficacy of employing advanced machine learning methods to solve captchas from Google's reCAPTCHAv2 system. We evaluate the effectiveness of automated systems in solving captchas by utilizing advanced YOLO models for image segmentation and classification. Our main result is that we can solve 100% of the captchas, while previous work only solved 68-71%. Furthermore, our findings suggest that there is no significant difference in the number of challenges humans and bots must solve to pass the captchas in reCAPTCHAv2. This implies that current AI technologies can exploit advanced image-based captchas. We also look under the hood of reCAPTCHAv2, and find evidence that reCAPTCHAv2 is heavily based on cookie and browser history data when evaluating whether a user is human or not. The code is provided alongside this paper.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Plesner,+A&quot;&gt;Andreas Plesner&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Vontobel,+T&quot;&gt;Tobias Vontobel&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Wattenhofer,+R&quot;&gt;Roger Wattenhofer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.08831&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Advancing AI Reasoning - A Look at OpenAI’s o1 Model</title><link href="http://localhost:4000/AdvReasoning" rel="alternate" type="text/html" title="Advancing AI Reasoning - A Look at OpenAI’s o1 Model" /><published>2024-09-12T00:00:00+03:00</published><updated>2024-09-12T00:00:00+03:00</updated><id>http://localhost:4000/AdvReasoning</id><content type="html" xml:base="http://localhost:4000/AdvReasoning">&lt;p&gt; Artificial intelligence notes progress almost daily in architecture as well as in problem-solving and reasoning, but OpenAI’s latest model, o1, marks a significant leap forward. Designed to excel in complex reasoning tasks, the o1 model has achieved remarkable results in competitive programming, academic benchmarks, and real-world applications. This article explores the cutting-edge capabilities of o1, from its performance in math and science challenges to its proficiency in coding and human-like reasoning.&lt;/p&gt;

&lt;h3&gt;The Power of o1: Breaking Benchmarks and Rivaling Experts&lt;/h3&gt;

&lt;p&gt;OpenAI's o1 model outperforms its predecessors and rivals human experts in various fields, particularly in reasoning-heavy tasks. In math, for instance, o1 solved 74% of problems from the prestigious AIME exam, placing it among the top 500 high school students in the U.S. With additional techniques like consensus sampling and re-ranking, its accuracy reached an impressive 93%. This is a significant improvement over earlier models like GPT-4o, which only solved 12% of the same problems.&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;https://images.ctfassets.net/kftzwdyauwt9/7rMY55vLbGTlTiP9GdSOrf/0944e1cde904e896bc5bc6f3da7f16b6/compute-dark.png?w=3840&amp;amp;q=80&amp;amp;fm=webp&quot; /&gt;
&lt;figcaption&gt;o1 performance smoothly improves with both train-time and test-time compute&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In science, o1 excelled on the GPQA diamond benchmark, designed to test advanced knowledge in chemistry, physics, and biology. When compared to PhD experts, o1 surpassed their performance, becoming the first AI model to do so on this difficult test. However, OpenAI clarifies that o1’s results &lt;strong&gt;do not imply&lt;/strong&gt; it is superior to PhDs across the board, but it does outshine them in certain types of problem-solving and that it is expected to solve some problems that a PhD would be expected to solve.&lt;/p&gt;

&lt;p&gt;The model’s coding abilities are equally impressive. In the 2024 International Olympiad in Informatics (IOI), o1 ranked in the 49th percentile among human competitors. When relaxed constraints allowed for 10,000 submissions per problem, the model scored above the gold medal threshold, showcasing its problem-solving agility in algorithmic challenges. Additionally, in simulated programming contests on Codeforces, o1 achieved an Elo rating of 1807, outperforming 93% of human participants, a remarkable feat in competitive programming.&lt;/p&gt;

&lt;figure&gt;
&lt;img src=&quot;https://cdn.openai.com/reasoning-evals/v3/headline-desktop-dark.png?w=3840&amp;amp;q=90&amp;amp;fm=webp&quot; /&gt;
&lt;figcaption&gt;o1 greatly improves over GPT-4o on challenging reasoning benchmarks. Solid bars show pass@1 accuracy and the shaded region shows the performance of majority vote (consensus) with 64 samples.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
&lt;img src=&quot;https://cdn.openai.com/reasoning-evals/v3/breakdown-dark.png?w=3840&amp;amp;q=90&amp;amp;fm=webp&quot; /&gt;
&lt;figcaption&gt;o1 improves over GPT-4o on a wide range of benchmarks, including 54/57 MMLU subcategories. Seven are shown for illustration.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3&gt;Reinforcement Learning and Chain of Thought: The Key to o1's Success&lt;/h3&gt;

&lt;p&gt;Central to o1's reasoning advancements is its ability to leverage a &lt;i&gt;&quot;chain of thought&quot;&lt;/i&gt; process, similar to how humans think through complex problems. This process allows the model to break down difficult questions, refine its approaches, and correct mistakes over time. Reinforcement learning plays a crucial role in this, teaching the model how to think productively, which leads to consistent improvements in both training and test-time performance.&lt;/p&gt;

&lt;p&gt;This &quot;chain of thought&quot; approach significantly improves the model's reasoning capabilities across tasks such as math, data analysis, and programming. For example, in open-ended evaluations, human trainers consistently preferred o1's responses over GPT-4o in areas that required deep reasoning. However, o1 is not perfect in all domains, as it struggled with some natural language tasks, suggesting it may not yet be universally applicable.&lt;/p&gt;

&lt;h3&gt;Safety and Alignment: A Responsible Approach to AI Development&lt;/h3&gt;

&lt;p&gt;Safety is a critical consideration in AI development, and OpenAI has integrated safety principles into o1’s chain of thought reasoning. By teaching the model safety rules and embedding these principles into its reasoning process, o1 has shown increased robustness in safety evaluations. It performed exceptionally well in tests designed to assess its resistance to harmful outputs, surpassing previous models like GPT-4o.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;Metric&lt;/th&gt;
    &lt;th&gt;GPT-4o&lt;/th&gt;
    &lt;th&gt;o1-preview&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;% Safe completions on harmful prompts
Standard&lt;/td&gt;
    &lt;td&gt;0.990&lt;/td&gt;
    &lt;td&gt;0.995&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;% Safe completions on harmful prompts
Challenging: jailbreaks &amp;amp; edge cases&lt;/td&gt;
    &lt;td&gt;0.714&lt;/td&gt;
    &lt;td&gt;0.934&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;↳ Harassment (severe)&lt;/td&gt;
    &lt;td&gt;0.845&lt;/td&gt;
    &lt;td&gt;0.900&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;↳ Exploitative sexual content&lt;/td&gt;
    &lt;td&gt;0.483&lt;/td&gt;
    &lt;td&gt;0.949&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;↳ Sexual content involving minors&lt;/td&gt;
    &lt;td&gt;0.707&lt;/td&gt;
    &lt;td&gt;0.931&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;↳ Advice about non-violent wrongdoing&lt;/td&gt;
    &lt;td&gt;0.688&lt;/td&gt;
    &lt;td&gt;0.961&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;↳  Advice about violent wrongdoing&lt;/td&gt;
    &lt;td&gt;0.778&lt;/td&gt;
    &lt;td&gt;0.963&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;% Safe completions for top 200 with highest Moderation API scores per category in WildChat (&lt;a href=&quot;https://arxiv.org/abs/2405.01470&quot;&gt;Zhao, et al. 2024&lt;/a&gt;)&lt;/td&gt;
    &lt;td&gt;0.945&lt;/td&gt;
    &lt;td&gt;0.971&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Goodness@0.1 StrongREJECT jailbreak eval (&lt;a href=&quot;https://arxiv.org/abs/2402.10260&quot;&gt;Souly et al. 2024&lt;/a&gt;)&lt;/td&gt;
    &lt;td&gt;0.220&lt;/td&gt;
    &lt;td&gt;0.840&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Human sourced jailbreak eval&lt;/td&gt;
    &lt;td&gt;0.770&lt;/td&gt;
    &lt;td&gt;0.960&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;% Compliance on internal benign edge cases
“not over-refusal”&lt;/td&gt;
    &lt;td&gt;0.910&lt;/td&gt;
    &lt;td&gt;0.930&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;% Compliance on benign edge cases in XSTest
“not over-refusal” (&lt;a href=&quot;https://arxiv.org/abs/2308.01263&quot;&gt;Röttger, et al. 2023&lt;/a&gt;)&lt;/td&gt;
    &lt;td&gt;0.924&lt;/td&gt;
    &lt;td&gt;0.976&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;OpenAI has also introduced the concept of a &quot;hidden chain of thought,&quot; allowing for internal monitoring of the model’s thought processes without revealing them to users. This strategy provides a balance between transparency and safety, enabling developers to observe the model’s reasoning while preventing potential misuse or manipulation.&lt;/p&gt;

&lt;p&gt;OpenAI’s o1 model sets a new standard for AI reasoning and problem-solving. By outperforming human experts in math, science, and coding benchmarks, and introducing innovative techniques like chain of thought reasoning, o1 demonstrates its potential to revolutionize a variety of fields. While still under development, this early release marks a significant step toward more aligned and capable AI systems, offering exciting possibilities for future applications in science, coding, and beyond.&lt;/p&gt;

&lt;p&gt;As OpenAI continues to iterate on the o1 model, the advancements in reasoning and alignment are expected to unlock even more groundbreaking use cases. To read full article go to oficial &lt;a href=&quot;https://openai.com/index/learning-to-reason-with-llms/&quot;&gt;blog post.&amp;lt;/a&amp;lt;/p&amp;gt;
&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Artificial intelligence notes progress almost daily in architecture as well as in problem-solving and reasoning, but OpenAI’s latest model, o1, marks a significant leap forward. Designed to excel in complex reasoning tasks, the o1 model has achieved remarkable results in competitive programming, academic benchmarks, and real-world applications. This article explores the cutting-edge capabilities of o1, from its performance in math and science challenges to its proficiency in coding and human-like reasoning.</summary></entry><entry><title type="html">DataGemma - Grounding AI in Real-World Data to Combat Hallucinations</title><link href="http://localhost:4000/GoogleDataGemma" rel="alternate" type="text/html" title="DataGemma - Grounding AI in Real-World Data to Combat Hallucinations" /><published>2024-09-12T00:00:00+03:00</published><updated>2024-09-12T00:00:00+03:00</updated><id>http://localhost:4000/GoogleDataGemma</id><content type="html" xml:base="http://localhost:4000/GoogleDataGemma">&lt;p&gt;Large Language Models (LLMs) have revolutionized the AI landscape by providing powerful tools for generating human-like text, answering complex questions, and assisting with tasks like summarization and code generation. However, these models sometimes produce inaccurate information with confidence, a phenomenon known as &quot;hallucination.&quot; Addressing this issue is critical for enhancing AI reliability. Enter &lt;a href=&quot;https://ai.google.dev/gemma&quot;&gt;DataGemma&lt;/a&gt;, the first open model designed to reduce hallucinations by grounding LLMs in real-world statistical data from Google’s vast &lt;a href=&quot;https://datacommons.org/&quot;&gt;Data Commons&lt;/a&gt;. This article explores how DataGemma leverages the power of trusted data sources to improve the factual accuracy and reasoning of LLMs.&lt;/p&gt;

&lt;h3&gt;The Challenges of Hallucination in AI&lt;/h3&gt;

&lt;p&gt;As AI models grow more advanced, they demonstrate remarkable capabilities in various domains. They can sift through extensive text databases, generate creative ideas, and even draft software code. Yet, despite their strengths, they are prone to hallucinations—generating outputs that are either partially or entirely incorrect. This challenge is particularly problematic when AI models are used in fields requiring high accuracy, such as research, policymaking, and data analysis. For AI to become a more dependable tool, it must consistently provide accurate information grounded in verifiable facts.&lt;/p&gt;

&lt;h3&gt;Introducing DataGemma and Data Commons&lt;/h3&gt;

&lt;p&gt;DataGemma is Google’s innovative solution to the hallucination problem. It works by connecting LLMs to the Data Commons, a public knowledge graph filled with over 240 billion data points across numerous statistical variables. This data is sourced from reputable organizations like the United Nations (UN), World Health Organization (WHO), and the Centers for Disease Control and Prevention (CDC). The wealth of reliable information within Data Commons spans topics like health, economics, demographics, and environmental trends.&lt;/p&gt;

&lt;p&gt;By integrating Data Commons, DataGemma ensures that LLMs can access real-world, trustworthy data during their response generation process. This connection allows AI systems to verify statistical claims, reducing the likelihood of hallucinations and improving the overall factual accuracy of the responses generated by models.&lt;/p&gt;

&lt;h3&gt;Grounding LLMs with DataGemma: RIG and RAG Approaches&lt;/h3&gt;

&lt;p&gt;DataGemma employs two primary techniques to mitigate hallucinations: RIG (Retrieval-Interleaved Generation) and RAG (Retrieval-Augmented Generation).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;lt;a href=https://colab.research.google.com/github/datacommonsorg/llm-tools/blob/master/notebooks/datagemma_rig.ipynb'&amp;gt;RIG&amp;lt;/a&amp;gt; (Retrieval-Interleaved Generation) – This method allows models to proactively query trusted sources, such as Data Commons, during the response generation process. If the model encounters a statistical query or data-related prompt, it retrieves accurate information from Data Commons before finalizing its response. This proactive retrieval helps the model fact-check its output, greatly minimizing the chances of hallucinating.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/datacommonsorg/llm-tools/blob/master/notebooks/datagemma_rag.ipynb&quot;&gt;RAG&lt;/a&gt; (Retrieval-Augmented Generation) – RAG enables LLMs to go beyond their initial training data, pulling in additional contextual information from external sources. In the case of DataGemma, the model utilizes a long context window to retrieve relevant information from Data Commons before generating a response. By doing so, DataGemma enhances the depth and accuracy of responses, offering more informed insights.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Preliminary &lt;a href=&quot;http://datacommons.org/link/DataGemmaPaper&quot;&gt;research&lt;/a&gt; indicates that these techniques significantly reduce hallucinations, especially when handling numerical facts. Early tests have shown promising results, suggesting that users across research, decision-making, and curiosity-driven explorations will experience more reliable interactions with AI models.&lt;/p&gt;

&lt;p&gt;The launch of DataGemma marks a significant advancement in addressing the issue of hallucination in large language models. By connecting AI to the rich, real-world data housed in Google’s Data Commons, DataGemma offers a pathway to more reliable and factually grounded AI outputs. The integration of retrieval techniques like RIG and RAG demonstrates how LLMs can be anchored in trustworthy data, making them more dependable for users across industries.&lt;/p&gt;

&lt;p&gt;As the technology continues to evolve, the improvements seen in DataGemma are a crucial step toward making AI not only more sophisticated but also more accurate and trustworthy. By ensuring that AI provides factual and context-rich information, we are closer to building a future where these models become indispensable tools for informed decision-making and deeper understanding of the world around us.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Large Language Models (LLMs) have revolutionized the AI landscape by providing powerful tools for generating human-like text, answering complex questions, and assisting with tasks like summarization and code generation. However, these models sometimes produce inaccurate information with confidence, a phenomenon known as &quot;hallucination.&quot; Addressing this issue is critical for enhancing AI reliability. Enter DataGemma, the first open model designed to reduce hallucinations by grounding LLMs in real-world statistical data from Google’s vast Data Commons. This article explores how DataGemma leverages the power of trusted data sources to improve the factual accuracy and reasoning of LLMs.</summary></entry><entry><title type="html">How Adobe Firefly is Shaping the Future of Creative Workflows</title><link href="http://localhost:4000/AdobeFirefly" rel="alternate" type="text/html" title="How Adobe Firefly is Shaping the Future of Creative Workflows" /><published>2024-09-11T00:00:00+03:00</published><updated>2024-09-11T00:00:00+03:00</updated><id>http://localhost:4000/AdobeFirefly</id><content type="html" xml:base="http://localhost:4000/AdobeFirefly">&lt;p&gt;Since its launch in March 2023, Adobe Firefly has transformed the creative landscape by offering innovative AI-powered features that enhance design, imaging, and vector workflows. These models have become integral to Creative Cloud and Adobe Express, enabling users to generate creative assets more efficiently. Now, Adobe is expanding Firefly's capabilities into the realm of video editing, preparing to revolutionize the process for editors and filmmakers. In this article, we’ll explore the journey of Firefly so far, its growing impact on the creative community, and its potential to reshape video production.&lt;/p&gt;

&lt;p&gt;Adobe Firefly’s early models, such as Generative Fill in Photoshop, Generative Remove in Lightroom, and Text-to-Template in Express, have quickly been embraced by the community. In less than a year, creators have generated over 12 billion images and vectors, making Firefly one of the most rapidly adopted tools within Adobe's ecosystem. This success is largely due to Adobe’s focus on user feedback, ensuring that the models meet the needs of both individual creators and enterprise customers.&lt;/p&gt;

&lt;p&gt;The creative world is increasingly driven by video, and Adobe is now preparing to unveil its Firefly Video Model, which will soon be available in beta (waitlist &lt;a href=&quot;https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon#form&quot;&gt;here&lt;/a&gt;). This new model is set to empower video editors with cutting-edge tools for ideation and content creation, helping them navigate the growing demands for short-form and engaging video content. Editors are often tasked with much more than simply cutting footage—they handle color correction, visual effects, audio, and more, all under tight deadlines. Firefly’s AI tools aim to streamline these processes, saving time while enhancing the quality of the final product.&lt;/p&gt;

&lt;p&gt;One of the standout features of Firefly’s Video Model is its ability to assist with common editorial tasks like removing unwanted objects, smoothing jump cuts, and filling gaps in footage using generative AI. These functions not only simplify technical workflows but also help editors stay focused on the creative aspects of storytelling. Additionally, Firefly’s tools will integrate with Adobe’s existing collaboration platform, Frame.io, making it easier to share creative intent and receive feedback from teams and stakeholders.&lt;/p&gt;

&lt;p&gt;Most importantly, Firefly's AI models are developed with ethical considerations in mind. Adobe has ensured that the content used to train these models is commercially safe, sourced only from material with proper permissions, and never from user-generated content. This commitment to responsible AI helps creators work confidently without concerns about copyright or intellectual property violations.&lt;/p&gt;

&lt;p&gt;Adobe Firefly has already proven itself as a game-changing tool for creative professionals, and its upcoming Video Model promises to further elevate video editing workflows. By integrating AI into various aspects of the post-production process, Firefly enables editors to focus on what they do best—telling compelling stories. As the demand for high-quality video content continues to grow, Adobe's generative AI tools will empower creators to meet deadlines while pushing the boundaries of their craft. With the beta release of Firefly’s Video Model on the horizon, the future of video editing looks brighter, faster, and more creative than ever before.&lt;/p&gt;

&lt;p&gt; It is decided to share some of Adobe's increadible progress with all of us so sit back and enjoy their work!&lt;/p&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://www.youtube.com/embed/puEgugluadk&quot; title=&quot;Adobe Firefly Video Model Coming Soon | Adobe Video&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;&lt;strong&gt;Original Footage:&lt;/strong&gt;&lt;/h3&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://blog.adobe.com/media_12ac94566a1ce98690d7929346442187b48cfdff6.mp4&quot; title=&quot;Adobe Firefly Video Model Coming Soon | Adobe Video&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;&lt;strong&gt;Generated Clip:&lt;/strong&gt;&lt;/h3&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://blog.adobe.com/media_1aa1fceac992b3db71ba6673c1706ef178c3ffd31.mp4&quot; title=&quot;Adobe Firefly Video Model Coming Soon | Adobe Video&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;&lt;strong&gt;Combined Sequence:&lt;/strong&gt;&lt;/h3&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://blog.adobe.com/media_1e8e41d1c1fef51f9346af4e817662d539191fdce.mp4&quot; title=&quot;Adobe Firefly Video Model Coming Soon | Adobe Video&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Since its launch in March 2023, Adobe Firefly has transformed the creative landscape by offering innovative AI-powered features that enhance design, imaging, and vector workflows. These models have become integral to Creative Cloud and Adobe Express, enabling users to generate creative assets more efficiently. Now, Adobe is expanding Firefly's capabilities into the realm of video editing, preparing to revolutionize the process for editors and filmmakers. In this article, we’ll explore the journey of Firefly so far, its growing impact on the creative community, and its potential to reshape video production.</summary></entry><entry><title type="html">LLaMA-Omni - Seamless Speech Interaction with Large Language Models</title><link href="http://localhost:4000/LlammaOmni" rel="alternate" type="text/html" title="LLaMA-Omni - Seamless Speech Interaction with Large Language Models" /><published>2024-09-10T00:00:00+03:00</published><updated>2024-09-10T00:00:00+03:00</updated><id>http://localhost:4000/LlammaOmni</id><content type="html" xml:base="http://localhost:4000/LlammaOmni">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fang,+Q&quot;&gt;Qingkai Fang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Guo,+S&quot;&gt;Shoutao Guo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhou,+Y&quot;&gt;Yan Zhou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ma,+Z&quot;&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+S&quot;&gt;Shaolei Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Feng,+Y&quot;&gt;Yang Feng&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.06666&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">GroUSE - A Benchmark to Evaluate Evaluators in Grounded Question Answering</title><link href="http://localhost:4000/GroUSE" rel="alternate" type="text/html" title="GroUSE - A Benchmark to Evaluate Evaluators in Grounded Question Answering" /><published>2024-09-10T00:00:00+03:00</published><updated>2024-09-10T00:00:00+03:00</updated><id>http://localhost:4000/GroUSE</id><content type="html" xml:base="http://localhost:4000/GroUSE">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge.&lt;/p&gt;
&lt;p&gt;To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4's judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection.&lt;/p&gt;
&lt;p&gt;We further show that finetuning Llama-3 on GPT-4's reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4's evaluations and calibration on reference situations.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muller,+S&quot;&gt;Sacha Muller&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Loison,+A&quot;&gt;António Loison&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Omrani,+B&quot;&gt;Bilel Omrani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Viaud,+G&quot;&gt;Gautier Viaud&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://www.arxiv.org/abs/2409.06595&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">SambaNova Launches The World’s Fastest AI Platform</title><link href="http://localhost:4000/SambaNova" rel="alternate" type="text/html" title="SambaNova Launches The World’s Fastest AI Platform" /><published>2024-09-10T00:00:00+03:00</published><updated>2024-09-10T00:00:00+03:00</updated><id>http://localhost:4000/SambaNova</id><content type="html" xml:base="http://localhost:4000/SambaNova">&lt;p&gt; In an exciting development for AI and machine learning, &lt;a href=&quot;https://sambanova.ai&quot;&gt;SambaNova Systems&lt;/a&gt; has announced the launch of SambaNova Cloud, the world’s fastest AI inference platform. Leveraging the power of its SN40L AI chip, SambaNova Cloud delivers unmatched speed and precision, running the groundbreaking Llama 3.1 405B model at an impressive 132 tokens per second (t/s). The platform is available to developers today, offering a powerful solution for building generative AI applications with both the largest and most capable open-source models.&lt;/p&gt;

&lt;h3&gt; The Power of Llama 3.1 in SambaNova Cloud&lt;/h3&gt;

&lt;p&gt; Meta’s Llama 3.1 models have gained significant attention this year, with versions ranging from 8B to 405B parameters. The 405B model, in particular, is a breakthrough for open-source AI, offering a serious alternative to the proprietary models from industry giants like OpenAI, Anthropic, and Google. However, deploying such a large model has always been a challenge due to its size, complexity, and the associated speed trade-offs.&lt;/p&gt;

&lt;p&gt; This is where SambaNova comes in. According to CEO Rodrigo Liang, SambaNova is the only platform running Llama 3.1 405B at full precision and at 132 t/s, a feat that competitors using Nvidia GPUs cannot match. By using the custom-built SN40L AI chip, SambaNova reduces the cost and complexity of deploying massive models like Llama 3.1 405B while delivering faster speeds than ever before.&lt;/p&gt;

&lt;p&gt; For enterprises, this means unprecedented flexibility. “Customers want versatility,” says Liang. “They need the 70B model at lightning-fast speeds for agentic AI workflows, and the 405B model for the highest fidelity and best results. Only SambaNova Cloud offers both today.”

&lt;h3&gt; Unmatched Speed for AI Developers&lt;/h3&gt;

&lt;p&gt; Developers can now access SambaNova Cloud through a free API, allowing them to build and deploy applications with world-record speeds. In addition to running the 405B model at 132 t/s, SambaNova Cloud also supports the smaller Llama 3.1 70B model at 461 tokens per second, making it ideal for agentic AI systems that require high-speed, real-time responses. These speeds are essential for applications needing fast token generation, such as conversational agents, real-time analytics, and autonomous systems.&lt;/p&gt;

&lt;p&gt; Andrew Ng, a renowned AI expert and founder of DeepLearning.AI, praised the technical achievements of SambaNova Cloud, stating that running the 405B model at 16-bit precision and over 100 t/s is a game-changer for developers working with large language models (LLMs).&lt;/p&gt;

&lt;h3&gt; Independent Validation of SambaNova's Speed&lt;/h3&gt;

&lt;p&gt; The platform’s impressive performance has been independently verified. According to George Cameron, Co-Founder of Artificial Analysis, SambaNova Cloud’s Llama 3.1 405B endpoint achieved a record speed of 132 tokens per second, outperforming other frontier models from OpenAI, Anthropic, and Google. This speed makes it the best option for AI use cases where rapid token processing is critical.&lt;/p&gt;

&lt;img src=&quot;https://sambanova.ai/hs-fs/hubfs/405b.jpg?width=1160&amp;amp;height=449&amp;amp;name=405b.jpg&quot; /&gt;

&lt;h3&gt; A Platform for Agentic AI and More&lt;/h3&gt;

&lt;p&gt; SambaNova Cloud isn’t just about speed—it’s designed to support a wide range of AI applications. The 70B model, in particular, is ideal for agentic AI workflows, where systems need to interact and collaborate to complete complex tasks. Companies like Bigtincan and Blackbox AI have already adopted SambaNova Cloud to power their AI-driven solutions, citing up to a 300% improvement in efficiency for their platforms.&lt;/p&gt;

&lt;h3&gt; How to Get Started with SambaNova Cloud&lt;/h3&gt;

&lt;p&gt; SambaNova Cloud offers multiple tiers for different needs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; Free Tier: Open to all developers today, providing free API access to Llama 3.1 models.&lt;/li&gt;
&lt;li&gt; Developer Tier: Launching by the end of 2024, this tier will offer higher rate limits and access to the 8B, 70B, and 405B models for advanced development.&lt;/li&gt;
&lt;li&gt; Enterprise Tier: Available now, this tier is designed for large-scale production workloads with the highest rate limits and scalability.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt; SambaNova’s SN40L Chip: The Heart of the Platform&lt;/h3&gt;

&lt;p&gt;At the core of SambaNova Cloud’s performance is the SN40L AI chip. Its patented dataflow architecture and three-tier memory design allow it to run AI models faster and more efficiently than traditional GPU-based solutions. This unique hardware accelerates inference speeds, making SambaNova Cloud the fastest platform for AI developers today.&lt;/p&gt;

&lt;h3&gt; SambaNova: Pioneering AI for the Enterprise&lt;/h3&gt;

&lt;p&gt; Founded in 2017 and headquartered in Palo Alto, SambaNova Systems was created by industry veterans from Sun/Oracle and Stanford University. The company is backed by top-tier investors, including SoftBank, BlackRock, and Intel Capital, and is committed to bringing cutting-edge AI technology to the enterprise. SambaNova’s cloud platform and custom AI chips enable organizations to quickly deploy state-of-the-art generative AI capabilities, transforming how businesses use AI.

&lt;p&gt; For developers and enterprises looking to leverage the power of open-source models like Llama 3.1 with unmatched speed and precision, SambaNova Cloud is available now. Visit &lt;a href=&quot;https://sambanova.ai&quot;&gt;SambaNova’s website&lt;/a&gt; or follow them on &lt;a href=&quot;https://www.linkedin.com/company/sambanova/&quot;&gt;LinkedIn&lt;/a&gt; and &lt;a href=&quot;http://twitter.com/SambaNovaAI&quot;&gt;X&lt;/a&gt; to learn more.&lt;/p&gt;

&lt;p&gt;To check out the full report, their blog post, click &lt;a href=&quot;https://sambanova.ai/press/worlds-fastest-ai-platform&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/p&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">In an exciting development for AI and machine learning, SambaNova Systems has announced the launch of SambaNova Cloud, the world’s fastest AI inference platform. Leveraging the power of its SN40L AI chip, SambaNova Cloud delivers unmatched speed and precision, running the groundbreaking Llama 3.1 405B model at an impressive 132 tokens per second (t/s). The platform is available to developers today, offering a powerful solution for building generative AI applications with both the largest and most capable open-source models.</summary></entry></feed>