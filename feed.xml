<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-07-22T18:13:16+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">Introduction to RAG models</title><link href="http://localhost:4000/rag1" rel="alternate" type="text/html" title="Introduction to RAG models" /><published>2024-07-22T00:00:00+03:00</published><updated>2024-07-22T00:00:00+03:00</updated><id>http://localhost:4000/rag1</id><content type="html" xml:base="http://localhost:4000/rag1">&lt;p&gt; Firstly, in case you don't know what is RAG here is an unofficial explanation. Imagine you’re on a treasure hunt, but instead of a dusty old map, you’ve got a genius guide who knows every hidden corner. That’s RAG, short for &lt;strong&gt;Retrieval-Augmented Generation&lt;/strong&gt;. It’s like having a super-smart friend who fetches the most relevant bits of knowledge from a massive library (the retrieval part) and then crafts a perfectly tailored response just for you (the generation part). So, if your brain is a bit like a rusty old filing cabinet, think of RAG as your personal, turbo-charged librarian who’s always got the answer before you can say &quot;Google it!&quot;&lt;/p&gt;

&lt;p&gt; In this series of posts, we are going to explore LangChain tools and create RAG models for several applications. We are going to go through the little details that may or may not work in our cases and how to fix those teeny tiny configurations that may be neccesary for our purposes. Without further ado..&lt;/p&gt;

&lt;h2&gt; Dotenv File &lt;/h2&gt;

&lt;p&gt;Before moving forward to modules and script, there is a high need for a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file. Initially, you need to create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file in the folder you are going to create this project, so that you save your passwords. This is not neccesary from the functionality point of view but it is from the security point of view. Think of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file as your project’s secret diary, where it whispers all its deepest, darkest secrets like passwords, API keys, and configuration settings. You need it because you don’t want these secrets plastered all over the code like graffiti. By keeping them in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file, you ensure they stay hidden and safe, only revealing themselves to those in the know—your code. So, a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file is like having a secret stash of information that keeps your project running smoothly without spilling the beans to the world. No need for functy file name or anything just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; file.&lt;/p&gt;

&lt;h2&gt; Modules Required &lt;/h2&gt;

&lt;p&gt; The python modules we are going to use for this introductory project are the following&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain import hub
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import AzureOpenAIEmbeddings
from langchain.chat_models import AzureChatOpenAI
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;os&lt;/strong&gt;: Used for accessing environment variables with os.getenv().&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;dotenv.load_dotenv&lt;/strong&gt;: Intended to load environment variables from the .env file.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_community.document_loaders.PyPDFLoader&lt;/strong&gt;: Used to load PDF documents.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_community.document_loaders.DirectoryLoader&lt;/strong&gt;: Used to load documents from a directory.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain.text_splitter.RecursiveCharacterTextSplitter&lt;/strong&gt;: Used to split documents into chunks.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_community.vectorstores.Chroma&lt;/strong&gt;: Used to create a vectorstore from document embeddings.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain.chat_models.AzureChatOpenAI&lt;/strong&gt;: Used to initialize the Azure OpenAI model for the QA chain.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain.hub&lt;/strong&gt;: Used to pull a prompt from the hub.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_core.output_parsers.StrOutputParser&lt;/strong&gt;: Used to parse the output of the QA chain.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;langchain_core.runnables.RunnablePassthrough&lt;/strong&gt;: Used in the QA chain to pass through the question.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt; In the file where we will run this python script, we need to create a folder where we are going to save our data, our files. Else we can load them from a different path. For the purposes of this series, I decided to use the first option. We use PyPDFLoader to laod them as in the following script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;loader = DirectoryLoader('data/', glob = '*.pdf', loader_cls=PyPDFLoader)
documents = loader.load()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Here we see that the first argument &lt;code&gt;data/&lt;/code&gt; is the relative path where the documents are placed. WIth the second argument &lt;code&gt;*.pdf&lt;/code&gt; we define that we want to take under consideration all the files that end with the afforementioned character sequence '.pdf' (the word any is represented by '*'). Lastly, we define the loader class to use for the purposes of the files loading process &lt;code&gt;PyPDFLoader&lt;/code&gt;. Mind that there are other classes that can be used for different types of documents like &lt;code&gt;UnstructuredFileLoader&lt;/code&gt;, &lt;code&gt;TextLoader&lt;/code&gt;, &lt;code&gt;BSHTMLLoader&lt;/code&gt;, &lt;code&gt;CSVLoader&lt;/code&gt;. To find more about this check &lt;a href=&quot;https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html&quot;&gt;here&lt;/a&gt;. Let your imagination go wild! There are also other types of loaders (which we are going to explore later on) where you can load information from webpages, youtube and many many more. After that is completed we use this class and &lt;code&gt;load()&lt;/code&gt; everything exists in the file path and save them to the onject documents.&lt;/p&gt;

&lt;h2&gt; Chunks and Overlap &lt;/h2&gt;

&lt;p&gt; Next in line of the things we need to accomplish is split the documents into chunks. Chunk?!?!? Imagine you’re trying to eat a massive pizza all by yourself. Chunk_size is like deciding how many slices you cut it into so you can manage each piece without choking. Chunk overlap, on the other hand, is making sure each slice has a bit of the previous one’s crust, so you don’t miss any of the delicious toppings in between. To optimize them, you balance the slice size (chunk_size) to be just right for easy munching, and the overlap so you get all the flavors without making it too repetitive. Get it right, and you’ll devour that pizza with maximum efficiency and satisfaction!&lt;/p&gt;

&lt;p&gt; Here are some tips to help you determine the optimal chunk size if common chunking methods, such as fixed chunking, are not suitable for your use case:

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Data Preprocessing&lt;/strong&gt;: Before deciding on the best chunk size, you need to preprocess your data to ensure its quality. For instance, if your data is sourced from the web, you might need to remove HTML tags or other noise elements.&lt;/li&gt;

&lt;li&gt; &lt;strong&gt;Chunk Sizes&lt;/strong&gt;: After preprocessing, choose a range of potential chunk sizes to test. The selection should consider the nature of the content (e.g., short messages vs. lengthy documents), the embedding model you’ll use, and its token limits. Aim to find a balance between preserving context and maintaining accuracy. Start by exploring various chunk sizes, such as smaller chunks (e.g., 128 or 256 tokens) for capturing granular semantic information and larger chunks (e.g., 512 or 1024 tokens) for retaining more context.&lt;/li&gt;

&lt;li&gt; &lt;strong&gt;Evaluation of Chunk Sizes by performance results.&lt;/strong&gt;: To test different chunk sizes, use either multiple indices or a single index with multiple namespaces. Create embeddings for the selected chunk sizes using a representative dataset and save them in your index or indices. Then, run a series of queries to evaluate the quality and compare the performance of the various chunk sizes. This process is likely iterative, requiring you to test different chunk sizes against different queries until you identify the best-performing chunk size for your content and expected queries.&lt;/li&gt;
&amp;lt;/ul&amp;lt;/p&amp;gt;

&lt;p&gt; For more information regarding chunk size and chunk overlap, you may refer on &lt;a href=&quot;https://www.kaggle.com/discussions/general/503436&quot;&gt;Guide to Chunk Size and Overlap&lt;/a&gt; by Kaggle, &lt;a href=&quot;https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies/&quot;&gt;Chunk Sizes&lt;/a&gt; by Llama Index  or &lt;a href=&quot;https://zilliz.com/learn/guide-to-chunking-sreategies-for-rag&quot;&gt;A Guide to Chunking Strategies for Retrieval Augmented Generation (RAG)&lt;/a&gt; by Zilliz. We are not goint to go through the process of deciding the optimal chunk size, as this is out of this project's scope.&lt;/p&gt; 

&lt;pre&gt;&lt;code&gt;text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32)
text_chunks = text_splitter.split_documents(documents)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now that we have collected our data, and divided into some well structured chunks, easily digestible pieces, we need to create an embeddings model. This model will assists us on creatign embeddings out of the collected and stored documents that we are interested on finding further information about. As I have already mentioned above, I have decided to use Azure OpenAI to assists us. It is up to you which model you are going to use. there are many alternatives bnoth free and paid ones for each part of our project to consider. Do not hesitate to ask me if you have any qyestions on how to change the code so that you use a different kind of model!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;embeddings = AzureOpenAIEmbeddings(
    deployment=os.getenv('OPENAI_DEPLOYMENT_NAME_EMB'),
    model=os.getenv('OPENAI_MODEL_NAME_EMB'),
    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
    openai_api_type=os.getenv('OPENAI_API_TYPE'),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Since we have our embeddings, we are now ready to create our vectorstore where we are going to save our embeddings. Imagine you’ve got a magical, super-organized pantry where every ingredient knows exactly where it belongs and can jump right into your hand when you need it. That’s a vectorstore! It’s a special kind of database where information is stored as vectors, or points in a high-dimensional space, making it super easy to find and retrieve. So, a vectorstore is like having a pantry where every spice, snack, and secret ingredient is neatly indexed and ready to leap out at your command, making your cooking—or in this case, data retrieval—fast and efficient! We have decided to use Chroma as our vectorstore service, but youare free to use any one you need. Some alternatives that you could consider are Pinecone, FAISS, Lance where you can find further information &lt;a href=&quot;https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.sklearn.SKLearnVectorStore.html&quot;&gt;SKLearnVectorStore&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vectorstore = Chroma.from_documents(documents = text_chunks,                                    
    embedding = embeddings,
    persist_directory=&quot;data/vectorstore&quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt; Retriever &lt;/h2&gt;

&lt;p&gt; Now that we have our data vectorestore set up, we are ready to initialize our retriever. Picture the retriever in a RAG process as your ultra-savvy shopping buddy who knows exactly where everything is in the store. When you need something specific, the retriever zips around the aisles, grabbing the most relevant items off the shelves and bringing them back to you in record time. In the RAG (Retrieval-Augmented Generation) process, the retriever’s job is to fetch the most pertinent pieces of information from a vast database, so the generator can then whip up a perfectly informed response. It’s like having a shopping wizard who makes sure you always have the right ingredients for the perfect recipe!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
prompt = hub.pull('rlm/rag-prompt')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Argument &lt;code&gt;'k':5&lt;/code&gt; makes sure that our retriever will bring back to the generator the 5 most similar items, not 6, not 4. The line &lt;code&gt;hub.pull('rlm/rag-prompt')&lt;/code&gt; is used to pull a specific prompt template named 'rlm/rag-prompt' from a hub. You could define and use your own prompt for this part (which we shall experiment in a later on post). To find out more on predefined qa-prompts, go to &amp;lt;a href'https://docs.smith.langchain.com/old/category/prompt-hub'&amp;gt;Langchain Hub&amp;lt;/a&amp;gt;.&lt;/p&gt;

&lt;h2&gt; Chain Creation &lt;/h2&gt;

&lt;p&gt; The next step in our RAG project is to define the LLM (Large Language Model) that will provide the answers for us. Imagine an LLM as a super-intelligent, chatty robot that’s read every book, article, and meme on the internet and somehow remembers them all. It stands for Large Language Model, and it’s like having a best friend who’s always ready to chat, offer advice, or spin a tale, because it’s been trained on a vast mountain of text data. This robot buddy can understand your questions and whip up responses that sound like they came straight from a well-read, eloquent author. So, if you ever need a conversation partner who’s a walking encyclopedia with a knack for witty comebacks, the LLM’s got your back!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;llm = AzureChatOpenAI(
    deployment_name=os.getenv('LLM_35_Deployment'),
    model_name=os.getenv('LLM_35_Model'),
    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),
    temperature=0,
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Moving forward, we need to define the RAG chain. Think of a RAG chain as a magical relay race where information is passed along to make the ultimate answer. Imagine a team of information specialists: the first runner grabs the relevant facts (that’s the retriever’s job), the second runner crafts those facts into a coherent, brilliant response (thanks to the generator), and the baton gets passed seamlessly from one to the other. This chain of handoffs ensures you get a well-rounded, perfectly polished answer every time. So, a RAG chain is like a finely-tuned relay team making sure no detail gets left behind and every answer is a winner! We need the RAG chain to clearly define the steps that we need to be included from prompt to final response. Later on we are going to take a closer look on LangGraph a new tool of Langchain which assist us on defining clearly a this process. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rag_chain = (
    {&quot;context&quot;: lambda x: retriever, &quot;question&quot;: RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; You can see clearly what are the steps of our first really simple RAG chain. Initially, the contect and question are passed through associated to the lambda &quot;x&quot; object (in our case the retriever). On the next step the information (context and question) are passed to the prompt so that the instructions are provided to our llm selected model. The model analyzes and constructes an answer to our question. At the end, with the assistance of &lt;code&gt;StrOutputParser()&lt;/code&gt; it is ensures that the answer is presented in the desired human readable format.&lt;/p&gt;

&lt;p&gt;Now you are ready to ask your own questions to the model you just created to find out more about your documents.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;question = &quot;My custom question&quot;
rag_chain.invoke(question)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Well, I hope you enjoyed this as much as I did and learned something from it! Hopw to see you again in the next post of this series where we are going to talk about Query transformation and how we can use LLM models so that an LLM can define our question on a different &quot;better&quot; way.&lt;p&gt; 

&lt;p&gt; &lt;i&gt;Be safe, code safer!&lt;/i&gt;&lt;/p&gt;
&lt;/p&gt;&lt;/p&gt;&lt;/ul&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="project" /><category term="Deep Learning" /><summary type="html">Firstly, in case you don't know what is RAG here is an unofficial explanation. Imagine you’re on a treasure hunt, but instead of a dusty old map, you’ve got a genius guide who knows every hidden corner. That’s RAG, short for Retrieval-Augmented Generation. It’s like having a super-smart friend who fetches the most relevant bits of knowledge from a massive library (the retrieval part) and then crafts a perfectly tailored response just for you (the generation part). So, if your brain is a bit like a rusty old filing cabinet, think of RAG as your personal, turbo-charged librarian who’s always got the answer before you can say &quot;Google it!&quot;</summary></entry><entry><title type="html">GPT 4o Mini - Advancing cost-efficient intelligence</title><link href="http://localhost:4000/GPT4oMini" rel="alternate" type="text/html" title="GPT 4o Mini - Advancing cost-efficient intelligence" /><published>2024-07-18T00:00:00+03:00</published><updated>2024-07-18T00:00:00+03:00</updated><id>http://localhost:4000/GPT4oMini</id><content type="html" xml:base="http://localhost:4000/GPT4oMini">&lt;p&gt; In the official &lt;a href ='https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/'&gt;announcement&lt;/a&gt;, OpenAI, has introduced GPT-4o Mini, a new iteration in the GPT-4 series designed to deliver high-quality AI performance while being significantly more cost-effective. This latest model aims to make advanced AI capabilities more accessible and affordable for a wider range of applications and users.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key Features of GPT-4o Mini:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt; &lt;strong&gt;Cost Efficiency&lt;/strong&gt;: GPT-4o Mini is engineered to reduce operational costs without compromising on the quality of output. This makes it an ideal solution for businesses and developers seeking budget-friendly AI solutions.  We can see that GPT-4o mini is more than 60% cheaper than GPT-3.5 Turbo, priced at 15¢ per 1M input tokens and 60¢ per 1M output tokens (roughly the equivalent of 2500 pages in a standard book).&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Compact Size, High Performance&lt;/strong&gt;: Despite its smaller footprint, GPT-4o Mini maintains robust performance metrics, ensuring that users can still leverage the powerfol capabilities of the GPT-4 architecture. It is stated in the official announcement that GPT-4o mini outperforms GPT-2.5 Turbo in textual intelligence-scoring 82% on MMLU compared to 69.8% and multimodal reasoning&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Versatility&lt;/strong&gt;: The model is designed to cater to various applications, from customer service bots and content generation to complex data analysis, proving its adaptability across different sectors.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Accessibility&lt;/strong&gt;: By lowering the cost barrier, OpenAI aims to democratize AI technology, making it more accessible to startups, small businesses, and educational institutions.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Environmental Impact&lt;/strong&gt;: The efficient design of GPT-4o Mini also translates to lower energy consumption, aligning with sustainable practices and reducing the carbon footprint of AI operations.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Modalities&lt;/strong&gt;: GPT-4o mini currently supports text and vision capabilities, and the team stated that it is planned to add support for audio and video inputs and outputs in the future.
&lt;li&gt; &lt;strong&gt;Language&lt;/strong&gt;:  GPT-4o mini has improved multilingual understanding over GPT-3.5 Turbo across a wide range of non-English languages. Take a second and test your own mother tongue it may surprise you positively!
&lt;/ol&gt;

&lt;p&gt; Other than that &lt;a href='https://community.openai.com/u/jeffsharris/summary'&gt;jeffsharris&lt;/a&gt; (OpenAI Staff) stated that:

&lt;em&gt;Like GPT-4o, GPT-4o mini has a 128k context window and a knowledge cut-off date of October 2023. It also supports up to 16,384 max_tokens 82.

We plan to launch fine-tuning for GPT-4o mini in the coming days. You can learn more about GPT-4o mini in our &lt;a href= 'https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence'&gt;announcement blog&lt;/a&gt; 659 and &lt;a href='http://platform.openai.com/docs/models/gpt-4o-mini'&gt;API documentation&lt;/a&gt; 532, or by testing the model in &lt;a href='https://platform.openai.com/playground/chat?models=gpt-4o-mini'&gt;Playground&lt;/a&gt; 90. Excited to hear what you think! &lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This release marks a significant step in OpenAI's mission to provide powerfol, efficient, and accessible AI tools, empowering more users to harness the potential of artificial intelligence in their respective fields.&lt;/p&gt;

&lt;p&gt; For those of you using OpenAI models through Azure, worry not, in &lt;a href='https://azure.microsoft.com/en-us/blog/openais-fastest-model-gpt-4o-mini-is-now-available-on-azure-ai/'&gt; this announcement&lt;/a&gt; that was published on 18th of July 2024, Microsoft has announced the availability of OpenAI's latest and fastest model, GPT-4o Mini, on Azure AI. This integration brings the advanced capabilities of GPT-4o Mini to Azure users, offering enhanced performance and cost efficiency. This collaboration between Microsoft and OpenAI underscores a commitment to making advanced AI more accessible and affordable, empowering businesses to innovate and optimize their operations with the latest AI tools.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">In the official announcement, OpenAI, has introduced GPT-4o Mini, a new iteration in the GPT-4 series designed to deliver high-quality AI performance while being significantly more cost-effective. This latest model aims to make advanced AI capabilities more accessible and affordable for a wider range of applications and users.</summary></entry><entry><title type="html">SpreadsheetLLM - Encoding Spreadsheets for Large Language Models</title><link href="http://localhost:4000/MicrosoftSpreadsheetLLM" rel="alternate" type="text/html" title="SpreadsheetLLM - Encoding Spreadsheets for Large Language Models" /><published>2024-07-12T00:00:00+03:00</published><updated>2024-07-12T00:00:00+03:00</updated><id>http://localhost:4000/MicrosoftSpreadsheetLLM</id><content type="html" xml:base="http://localhost:4000/MicrosoftSpreadsheetLLM">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Spreadsheets, with their extensive two-dimensional grids, various layouts, and diverse formatting options, present notable challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2407.09025'&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">MInference</title><link href="http://localhost:4000/MInference" rel="alternate" type="text/html" title="MInference" /><published>2024-07-07T00:00:00+03:00</published><updated>2024-07-07T00:00:00+03:00</updated><id>http://localhost:4000/MInference</id><content type="html" xml:base="http://localhost:4000/MInference">&lt;p&gt;&lt;q&gt; Now, you can process 1M context 10x faster in a single A100 using Long-context LLMs like LLaMA-3-8B-1M, GLM-4-1M, with even better accuracy, try MInference 1.0 right now!&lt;/q&gt; as stated in the announcement.&lt;/p&gt;

&lt;p&gt;Microsoft has recently unveiled MInference, an open-source library designed to streamline and enhance the process of machine learning inference. This repository, hosted on &lt;a href='https://github.com/microsoft/MInference?tab=readme-ov-file'&gt;GitHub&lt;/a&gt;, offers a comprehensive suite of tools and resources aimed at developers and data scientists looking to integrate efficient inference capabilities into their applications.&lt;/p&gt;

&lt;p&gt; Some of the key features are 

&lt;ol&gt; 
    &lt;li&gt; High Performance: MInference is optimized for speed and efficiency. This will make it certain for you to have rapid inference times even with large and complex models. This makes it ideal for real-time applications where quick response times are critical for you and/or your business.&lt;/li&gt;
    &lt;li&gt; Versatility: The library supports a wide range of machine learning models and frameworks. Whether you are working with PyTorch of TensorFlow MInference provides seamless integration, allowing for flexibility and ease of use across different platforms and environments. Take a look at the &lt;a href='https://github.com/microsoft/MInference/tree/main/examples'&gt;examples&lt;/a&gt; to figure out more about how to use it.&lt;/li&gt;
    &lt;li&gt; Scalability: Designed with scalability in mind, MInference can handle inference tasks from small-scale projects to large, enterprise-level applications. This scalability ensures that as your data and model complexity grow, MInference can accommodate and maintain performance. Keep in mind though that currently the supported models are:&lt;/li&gt;
    &lt;ul&gt;
        &lt;li&gt; LLaMA-3: &lt;a href='https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k'&gt;gradientai/Llama-3-8B-Instruct-262k&lt;/a&gt;, &lt;a href='https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k'&gt;gradientai/Llama-3-8B-Instruct-Gradient-1048k&lt;/a&gt;, &lt;a href='https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-4194k'&gt;gradientai/Llama-3-8B-Instruct-Gradient-4194k&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt; GLM-4: &lt;a href='https://huggingface.co/THUDM/glm-4-9b-chat-1m'&gt;THUDM/glm-4-9b-chat-1m&lt;/a&gt; &lt;/li&gt;
        &lt;li&gt; Yi: &lt;a href='https://huggingface.co/01-ai/Yi-9B-200K'&gt;01-ai/Yi-9B-200K&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt; Phi-3: &lt;a href='https://huggingface.co/microsoft/Phi-3-mini-128k-instruct'&gt;microsoft/Phi-3-mini-128k-instruct&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt; Qwen2: &lt;a href='https://huggingface.co/Qwen/Qwen2-7B-Instruct'&gt;Qwen/Qwen2-7B-Instruct&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
    &lt;li&gt; User-Friendly API: MInference boasts a user-friendly API, making it accessible for both beginners and experienced practitioners. The well-documented functions and examples facilitate a smooth learning curve and quick implementation.&lt;/li&gt;
    &lt;li&gt; Community and Support: As an open-source project, MInference benefits from the contributions and support of the developer community. You can take advantage of this open-source project to contibute and/or get any support resolving any issues that may come up.&lt;/li&gt;

&lt;p&gt;So to get statred with MInference, in case you feel like it, you check the detailed README file that has the instructions to install anything needed acompanied with examples and documentations. Whether you are a developer of a data scientist I think you will find it tempting to include and impletement efficient machine learning inference in your project so my advice, take a look at is and see if it fits your likes.&lt;/p&gt;

&lt;p&gt; If you are interested about reading more, there is a paper, currently in review that you can find &lt;a href='https://arxiv.org/abs/2407.02490'&gt;here&lt;/a&gt;&lt;/p&gt;.</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Now, you can process 1M context 10x faster in a single A100 using Long-context LLMs like LLaMA-3-8B-1M, GLM-4-1M, with even better accuracy, try MInference 1.0 right now! as stated in the announcement.</summary></entry><entry><title type="html">Gen-3 Alpha opened by Runway</title><link href="http://localhost:4000/RunwayTTV" rel="alternate" type="text/html" title="Gen-3 Alpha opened by Runway" /><published>2024-07-06T00:00:00+03:00</published><updated>2024-07-06T00:00:00+03:00</updated><id>http://localhost:4000/RunwayTTV</id><content type="html" xml:base="http://localhost:4000/RunwayTTV">&lt;p&gt;As it is stated &lt;a href='https://runwayml.com/blog/introducing-gen-3-alpha/'&gt;here&lt;/a&gt; &lt;q&gt;Gen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;Based on the results we see, we should be excited about upcoming Text to Video, Image to Video and Text to Image tools that will be available for us to use. You can take a deeper view on their products &lt;a href='https://runwayml.com/'&gt;here&lt;/a&gt; and try runway for free &lt;a href='https://app.runwayml.com/signup'&gt;here.&lt;/a&gt; I don't know about you but I can't wait to check the upcoming models they have to offer!&lt;/p&gt;

&lt;p&gt;For those of you that have not yet got the chance to know Runway, it is founded in 2018 by Cristóbal Valenzuela, Anastasis Germanidis and Alejandro Matamala-Ortiz. As Paul Drews and Emily Zhao writting at &lt;a href='https://salesforceventures.com/perspectives/welcome-runway/'&gt;Salesforce ventures.&lt;/a&gt; The initial idea came out of Cris’ thesis project at the Interactive Telecommunications Program at NYU, where he met his co-founders while researching applications of machine learning models for image and video use in the creative domains.&lt;/p&gt;

&lt;p&gt;Informed by their own experiences as artists, the Runway co-founders set out to answer the question of how a well-built digital tool could simplify the approachability of complex ML models and circumvent the need for deep technical background to give better access to state of the art machine intelligence to artists and designers. The mission of Runway was to democratize access to machine learning so more people can start thinking of new and creative ways to use those models.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">As it is stated here Gen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.</summary></entry><entry><title type="html">Gemini 1.5 Pro 2M context window</title><link href="http://localhost:4000/GoogleAIStudio" rel="alternate" type="text/html" title="Gemini 1.5 Pro 2M context window" /><published>2024-07-06T00:00:00+03:00</published><updated>2024-07-06T00:00:00+03:00</updated><id>http://localhost:4000/GoogleAIStudio</id><content type="html" xml:base="http://localhost:4000/GoogleAIStudio">&lt;p&gt;&lt;q&gt;Gemini 1.5 Pro 2M context window, code execution capabilities, and Gemma 2 are available today&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;As it is stated in the official blog post of &lt;a href='https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/'&gt;Google for developers&lt;/a&gt; it is decided to give developers access to the 2 million context window for Gemini 1.5 Pro, code execution capabilities in the Gemini API, and adding Gemma 2 in Google AI Studio.So Google Cloud is making two variations of its flagship AI model—Gemini 1.5 Flash and Pro—publicly accessible.&lt;/p&gt;

&lt;p&gt;A small multimodal model with a 1 million context window that tackles narrow high-frequency tasks and the most powerful version of Google’s LLM, upgraded to contain a 2 million context window, respectively are open to all developers to try and experiment with.&lt;/p&gt;

&lt;p&gt;Through this action Gemini variations aims to showcase how Google AI's work will assist businesses to develop, monitor annd/or expand challenging AI agents to find delicate solutions to their problems. During a press announcement, Google Cloud Chief Executive Thomas Kurian boasts the company sees “incredible momentum” with its generative AI recent developments, with organizations such as Accenture, Airbus, Anthropic, Box, Broadcom, Cognizant, Confluent, Databricks, Deloitte, Equifax, Estée Lauder Companies, Ford, GitLab, GM, the Golden State Warriors, Goldman Sachs, Hugging Face, IHG Hotels and Resorts, Lufthansa Group, Moody’s, Samsung, and others building on its platform. He attributes this adoption growth to the combination of what Google’s models are capable of and the company’s Vertex platform. It’ll &lt;q&gt;continue to introduce new capability in both those layers at a rapid pace.&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;You can find out more about recent advancements &lt;a href='https://developers.google.com/'&gt;here&lt;/a&gt; under the category, Trending news.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Gemini 1.5 Pro 2M context window, code execution capabilities, and Gemma 2 are available today</summary></entry><entry><title type="html">RouteLLM-Learning to Route LLMs with Preference Data</title><link href="http://localhost:4000/RouteLLM" rel="alternate" type="text/html" title="RouteLLM-Learning to Route LLMs with Preference Data" /><published>2024-07-01T00:00:00+03:00</published><updated>2024-07-01T00:00:00+03:00</updated><id>http://localhost:4000/RouteLLM</id><content type="html" xml:base="http://localhost:4000/RouteLLM">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2406.18665'&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">AI Agents That Matter</title><link href="http://localhost:4000/AIAgentsThatMatter" rel="alternate" type="text/html" title="AI Agents That Matter" /><published>2024-07-01T00:00:00+03:00</published><updated>2024-07-01T00:00:00+03:00</updated><id>http://localhost:4000/AIAgentsThatMatter</id><content type="html" xml:base="http://localhost:4000/AIAgentsThatMatter">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;AI agents are an exciting new research direction, and agent development is driven by benchmarks. Our analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. Our focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. We design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. We prescribe a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. We hope that the steps we introduce for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2407.01502'&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Google releases Gemma 2</title><link href="http://localhost:4000/Gemma2" rel="alternate" type="text/html" title="Google releases Gemma 2" /><published>2024-06-27T00:00:00+03:00</published><updated>2024-06-27T00:00:00+03:00</updated><id>http://localhost:4000/Gemma2</id><content type="html" xml:base="http://localhost:4000/Gemma2">&lt;p&gt;Google has introduced Gemma 2, its latest generation of open AI models, aimed at enhancing research and development in artificial intelligence. With 9B and 27B parameter versions, Gemma 2 boasts significant improvements in performance and efficiency, making it cost-effective to deploy on common hardware like NVIDIA GPUs. The model is designed for seamless integration with existing AI tools and frameworks, ensuring swift and efficient inference processes.&lt;/p&gt;

&lt;p&gt;Gemma 2 stands out for its focus on responsible AI development. It incorporates advanced safety features and provides open-sourced evaluation tools to facilitate ethical AI research. This ensures that developers can work with the model while adhering to best practices in AI safety and responsibility. The availability of these tools underscores Google's commitment to fostering a trustworthy AI ecosystem.&lt;/p&gt;

&lt;p&gt;Researchers and developers can access Gemma 2 through various platforms, including Google AI Studio, Kaggle, and Hugging Face. This accessibility is intended to encourage widespread experimentation and innovation within the AI community. By providing such a powerful and open resource, Google aims to accelerate advancements in AI and support the development of new, groundbreaking applications.&lt;/p&gt;

&lt;p&gt;For more details, visit the official &lt;a href=&quot;https://blog.google/technology/developers/google-gemma-2/&quot;&gt;Google blog post&lt;/a&gt;.</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Google has introduced Gemma 2, its latest generation of open AI models, aimed at enhancing research and development in artificial intelligence. With 9B and 27B parameter versions, Gemma 2 boasts significant improvements in performance and efficiency, making it cost-effective to deploy on common hardware like NVIDIA GPUs. The model is designed for seamless integration with existing AI tools and frameworks, ensuring swift and efficient inference processes.</summary></entry><entry><title type="html">Data curation via joint example selection further accelerates multimodal learning</title><link href="http://localhost:4000/DataCuration" rel="alternate" type="text/html" title="Data curation via joint example selection further accelerates multimodal learning" /><published>2024-06-25T00:00:00+03:00</published><updated>2024-06-25T00:00:00+03:00</updated><id>http://localhost:4000/DataCuration</id><content type="html" xml:base="http://localhost:4000/DataCuration">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly selecting batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from larger super-batches, we also leverage recent advances in model approximation to reduce the associated computational overhead. As a result, our approach--multimodal contrastive learning with joint example selection (JEST)--surpasses state-of-the-art models with up to 13× fewer iterations and 10× less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing the level of data curation as a new dimension for neural scaling laws.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href='https://arxiv.org/abs/2406.17711'&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry></feed>