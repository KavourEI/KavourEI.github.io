<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-07-06T15:06:48+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">Gen-3 Alpha opened by Runway</title><link href="http://localhost:4000/RunwayTTV" rel="alternate" type="text/html" title="Gen-3 Alpha opened by Runway" /><published>2024-07-06T00:00:00+03:00</published><updated>2024-07-06T00:00:00+03:00</updated><id>http://localhost:4000/RunwayTTV</id><content type="html" xml:base="http://localhost:4000/RunwayTTV">&lt;p&gt;As it is stated &lt;a href=&quot;https://runwayml.com/blog/introducing-gen-3-alpha/&quot;&gt;here&lt;/a&gt; &lt;q&gt;Gen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;Based on the results we see, we should be excited about upcoming Text to Video, Image to Video and Text to Image tools that will be available for us to use. You can take a deeper view on their products &lt;a href=&quot;https://runwayml.com/&quot;&gt;here&lt;/a&gt; and try runway for free &lt;a href=&quot;https://app.runwayml.com/signup&quot;&gt;here.&lt;/a&gt; I don't know about you but I can't wait to check the upcoming models they have to offer!&lt;/p&gt;

&lt;p&gt;For those of you that have not yet got the chance to know Runway, it is founded in 2018 by Cristóbal Valenzuela, Anastasis Germanidis and Alejandro Matamala-Ortiz. As Paul Drews and Emily Zhao writting at &lt;a href=&quot;https://salesforceventures.com/perspectives/welcome-runway/&quot;&gt;Salesforce ventures.&lt;/a&gt; The initial idea came out of Cris’ thesis project at the Interactive Telecommunications Program at NYU, where he met his co-founders while researching applications of machine learning models for image and video use in the creative domains.&lt;/p&gt;

&lt;p&gt;Informed by their own experiences as artists, the Runway co-founders set out to answer the question of how a well-built digital tool could simplify the approachability of complex ML models and circumvent the need for deep technical background to give better access to state of the art machine intelligence to artists and designers. The mission of Runway was to democratize access to machine learning so more people can start thinking of new and creative ways to use those models.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">As it is stated here Gen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.</summary></entry><entry><title type="html">Gemini 1.5 Pro 2M context window</title><link href="http://localhost:4000/GoogleAIStudio" rel="alternate" type="text/html" title="Gemini 1.5 Pro 2M context window" /><published>2024-07-06T00:00:00+03:00</published><updated>2024-07-06T00:00:00+03:00</updated><id>http://localhost:4000/GoogleAIStudio</id><content type="html" xml:base="http://localhost:4000/GoogleAIStudio">&lt;p&gt;&lt;q&gt;Gemini 1.5 Pro 2M context window, code execution capabilities, and Gemma 2 are available today&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;As it is stated in the official blog post of &lt;a href=&quot;https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/&quot;&gt;Google for developers&lt;/a&gt; it is decided to give developers access to the 2 million context window for Gemini 1.5 Pro, code execution capabilities in the Gemini API, and adding Gemma 2 in Google AI Studio.So Google Cloud is making two variations of its flagship AI model—Gemini 1.5 Flash and Pro—publicly accessible.&lt;/p&gt;

&lt;p&gt;A small multimodal model with a 1 million context window that tackles narrow high-frequency tasks and the most powerful version of Google’s LLM, upgraded to contain a 2 million context window, respectively are open to all developers to try and experiment with.&lt;/p&gt;

&lt;p&gt;Through this action Gemini variations aims to showcase how Google AI's work will assist businesses to develop, monitor annd/or expand challenging AI agents to find delicate solutions to their problems. During a press announcement, Google Cloud Chief Executive Thomas Kurian boasts the company sees “incredible momentum” with its generative AI recent developments, with organizations such as Accenture, Airbus, Anthropic, Box, Broadcom, Cognizant, Confluent, Databricks, Deloitte, Equifax, Estée Lauder Companies, Ford, GitLab, GM, the Golden State Warriors, Goldman Sachs, Hugging Face, IHG Hotels and Resorts, Lufthansa Group, Moody’s, Samsung, and others building on its platform. He attributes this adoption growth to the combination of what Google’s models are capable of and the company’s Vertex platform. It’ll &lt;q&gt;continue to introduce new capability in both those layers at a rapid pace.&lt;/q&gt;&lt;/p&gt;

&lt;p&gt;You can find out more about recent advancements &lt;a href=&quot;https://developers.google.com/&quot;&gt;here&lt;/a&gt; under the category, Trending news.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Gemini 1.5 Pro 2M context window, code execution capabilities, and Gemma 2 are available today</summary></entry><entry><title type="html">Google releases Gemma 2</title><link href="http://localhost:4000/Gemma2" rel="alternate" type="text/html" title="Google releases Gemma 2" /><published>2024-06-27T00:00:00+03:00</published><updated>2024-06-27T00:00:00+03:00</updated><id>http://localhost:4000/Gemma2</id><content type="html" xml:base="http://localhost:4000/Gemma2">&lt;p&gt;Google has introduced Gemma 2, its latest generation of open AI models, aimed at enhancing research and development in artificial intelligence. With 9B and 27B parameter versions, Gemma 2 boasts significant improvements in performance and efficiency, making it cost-effective to deploy on common hardware like NVIDIA GPUs. The model is designed for seamless integration with existing AI tools and frameworks, ensuring swift and efficient inference processes.&lt;/p&gt;

&lt;p&gt;Gemma 2 stands out for its focus on responsible AI development. It incorporates advanced safety features and provides open-sourced evaluation tools to facilitate ethical AI research. This ensures that developers can work with the model while adhering to best practices in AI safety and responsibility. The availability of these tools underscores Google's commitment to fostering a trustworthy AI ecosystem.&lt;/p&gt;

&lt;p&gt;Researchers and developers can access Gemma 2 through various platforms, including Google AI Studio, Kaggle, and Hugging Face. This accessibility is intended to encourage widespread experimentation and innovation within the AI community. By providing such a powerful and open resource, Google aims to accelerate advancements in AI and support the development of new, groundbreaking applications.&lt;/p&gt;

&lt;p&gt;For more details, visit the official &lt;a href=&quot;https://blog.google/technology/developers/google-gemma-2/&quot;&gt;Google blog post&lt;/a&gt;.
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Google has introduced Gemma 2, its latest generation of open AI models, aimed at enhancing research and development in artificial intelligence. With 9B and 27B parameter versions, Gemma 2 boasts significant improvements in performance and efficiency, making it cost-effective to deploy on common hardware like NVIDIA GPUs. The model is designed for seamless integration with existing AI tools and frameworks, ensuring swift and efficient inference processes.</summary></entry><entry><title type="html">Data curation via joint example selection further accelerates multimodal learning</title><link href="http://localhost:4000/DataCuration" rel="alternate" type="text/html" title="Data curation via joint example selection further accelerates multimodal learning" /><published>2024-06-25T00:00:00+03:00</published><updated>2024-06-25T00:00:00+03:00</updated><id>http://localhost:4000/DataCuration</id><content type="html" xml:base="http://localhost:4000/DataCuration">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly selecting batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from larger super-batches, we also leverage recent advances in model approximation to reduce the associated computational overhead. As a result, our approach--multimodal contrastive learning with joint example selection (JEST)--surpasses state-of-the-art models with up to 13× fewer iterations and 10× less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing the level of data curation as a new dimension for neural scaling laws.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.17711&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Connecting the Dots - LLMs can Infer and Verbalize Latent Structure from Disparate Training Data</title><link href="http://localhost:4000/ConnectDots" rel="alternate" type="text/html" title="Connecting the Dots - LLMs can Infer and Verbalize Latent Structure from Disparate Training Data" /><published>2024-06-20T00:00:00+03:00</published><updated>2024-06-20T00:00:00+03:00</updated><id>http://localhost:4000/ConnectDots</id><content type="html" xml:base="http://localhost:4000/ConnectDots">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. Further experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs (x,f(x)) can articulate a definition of f and compute inverses. While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures. Overall, the ability of LLMs to &quot;connect the dots&quot; without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.14546&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Meta releases New AI Research Models to Accelerate Innovation at Scale</title><link href="http://localhost:4000/MetaNewAI" rel="alternate" type="text/html" title="Meta releases New AI Research Models to Accelerate Innovation at Scale" /><published>2024-06-18T00:00:00+03:00</published><updated>2024-06-18T00:00:00+03:00</updated><id>http://localhost:4000/MetaNewAI</id><content type="html" xml:base="http://localhost:4000/MetaNewAI">&lt;p&gt; For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever. &lt;/p&gt;

&lt;p&gt; Today, we shared some of our latest FAIR research models with the world. These publicly released models include image-to-text and text-to-music generation models, a multi-token prediction model, and a technique for detecting AI-generated speech. By making this research publicly available, Meta aims to inspire further iterations and ultimately promote responsible advancements in AI. &lt;/p&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://www.youtube.com/embed/p9oM5dWmFZ0&quot; title=&quot;Meet Meta Chameleon&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For more information you can see the official meta &lt;a href=&quot;https://about.fb.com/news/2024/06/releasing-new-ai-research-models-to-accelerate-innovation-at-scale/&quot;&gt;announcement&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever.</summary></entry><entry><title type="html">Google open Project</title><link href="http://localhost:4000/GoogleProjectIDX" rel="alternate" type="text/html" title="Google open Project" /><published>2024-06-14T00:00:00+03:00</published><updated>2024-06-14T00:00:00+03:00</updated><id>http://localhost:4000/GoogleProjectIDX</id><content type="html" xml:base="http://localhost:4000/GoogleProjectIDX">&lt;p&gt;During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.&lt;/p&gt;

&lt;p&gt; &lt;i&gt;&quot;As AI becomes more prevalent, the complexities that come with deploying all of that really becomes harder, becomes greater, and we wanted to help solve that challenge,&quot;&lt;/i&gt; said Jeanine Banks, Google’s VP and general manager for Developer X and the company’s head of developer relations.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;&quot;That’s why we built project IDX, a multi-platform development experience that makes building applications fast and easy. Project IDX makes it really frictionless to get going with your preferred framework or language with easy-to-use templates like Next.js, Astro, Flutter, Dart, Angular, Go and more.&quot;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt; With this update, Google is integrating the Google Maps Platform into Project IDX, enabling the addition of geolocation features to apps. The update also includes integrations with Chrome Dev Tools and Lighthouse to assist in debugging applications. Soon, developers will be able to deploy apps to Cloud Run, Google Cloud's serverless platform for front- and back-end services.&lt;/p&gt;

&lt;p&gt; The development environment will also integrate with Checks, Google's AI-powered compliance platform, which is transitioning from beta to general availability on Tuesday.&lt;/p&gt;

&lt;p&gt; Project IDX isn't just about building AI-enabled applications; it's also about using AI to enhance the coding process. To facilitate this, IDX includes standard features like code completion and a chat assistant sidebar, as well as innovative tools like the ability to highlight a snippet of code and use Google's Gemini model to modify it, similar to generative fill in Photoshop.&lt;/p&gt;

&lt;p&gt; Whenever Gemini suggests code, it provides links back to the original source and its associated license.&lt;/p&gt;

&lt;p&gt; Built on the open-source Visual Studio Code, Project IDX also integrates seamlessly with GitHub, simplifying integration with existing workflows. In one of the latest releases, Google added built-in iOS and Android emulators for mobile developers directly into the IDE.&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/-wlZY4tfGMY&quot; title=&quot;Project IDX: Full-stack application development with generative AI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt; Feeling like you have to give it a go?. Check ProjectIDX &lt;a href=&quot;https://idx.dev/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.</summary></entry><entry><title type="html">NVIDIA announced Nemotron 340B</title><link href="http://localhost:4000/NVIDIARelease" rel="alternate" type="text/html" title="NVIDIA announced Nemotron 340B" /><published>2024-06-14T00:00:00+03:00</published><updated>2024-06-14T00:00:00+03:00</updated><id>http://localhost:4000/NVIDIARelease</id><content type="html" xml:base="http://localhost:4000/NVIDIARelease">&lt;p&gt;Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.&lt;/p&gt;

&lt;p&gt;NVIDIA on 14th of June, announced Nemotron-4 340B, a family of open models that developers can use to generate synthetic data for training large language models (LLMs) for commercial applications across healthcare, finance, manufacturing, retail and every other industry. &lt;/p&gt;

&lt;p&gt;High-quality training data plays a critical role in the performance, accuracy and quality of responses from a custom LLM — but robust datasets can be prohibitively expensive and difficult to access. &lt;/p&gt;

&lt;p&gt;Through a uniquely permissive open model license, Nemotron-4 340B gives developers a free, scalable way to generate synthetic data that can help build powerful LLMs.&lt;/p&gt;

&lt;p&gt;The Nemotron-4 340B family includes base, instruct and reward models that form a pipeline to generate synthetic data used for training and refining LLMs. The models are optimized to work with NVIDIA NeMo, an open-source framework for end-to-end model training, including data curation, customization and evaluation. They’re also optimized for inference with the open-source NVIDIA TensorRT-LLM library. &lt;/p&gt;

&lt;p&gt;Nemotron-4 340B can be downloaded now from the NVIDIA NGC catalog and Hugging Face. Developers will soon be able to access the models at ai.nvidia.com, where they’ll be packaged as an NVIDIA NIM microservice with a standard application programming interface that can be deployed anywhere.&lt;/p&gt;

&lt;p&gt;If you want to find out more go to the &lt;a href=&quot;https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/&quot;&gt;official blog of NVIDIA&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.</summary></entry><entry><title type="html">Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B</title><link href="http://localhost:4000/MonteCarloTrees" rel="alternate" type="text/html" title="Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B" /><published>2024-06-13T00:00:00+03:00</published><updated>2024-06-13T00:00:00+03:00</updated><id>http://localhost:4000/MonteCarloTrees</id><content type="html" xml:base="http://localhost:4000/MonteCarloTrees">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07394&quot;&gt;Monte Carlo Trees&lt;/a&gt; with Llama-3 8B solve mathematics limitations of LLMs&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Monte Carlo Trees with Llama-3 8B solve mathematics limitations of LLMs</summary></entry><entry><title type="html">Depth Anything V2</title><link href="http://localhost:4000/DepthAnythingV2" rel="alternate" type="text/html" title="Depth Anything V2" /><published>2024-06-13T00:00:00+03:00</published><updated>2024-06-13T00:00:00+03:00</updated><id>http://localhost:4000/DepthAnythingV2</id><content type="html" xml:base="http://localhost:4000/DepthAnythingV2">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07522&quot;&gt;Samba&lt;/a&gt; architecture achieves 3.73x faster throughput with enhanced context&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1&amp;rpar; replacing all labeled real images with synthetic images, 2&amp;rpar; scaling up the capacity of our teacher model, and 3&amp;rpar; teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Samba architecture achieves 3.73x faster throughput with enhanced context</summary></entry></feed>