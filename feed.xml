<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-07-01T11:36:58+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">Google releases Gemma 2</title><link href="http://localhost:4000/Gemma2" rel="alternate" type="text/html" title="Google releases Gemma 2" /><published>2024-06-27T00:00:00+03:00</published><updated>2024-06-27T00:00:00+03:00</updated><id>http://localhost:4000/Gemma2</id><content type="html" xml:base="http://localhost:4000/Gemma2">&lt;p&gt;Google has introduced Gemma 2, its latest generation of open AI models, aimed at enhancing research and development in artificial intelligence. With 9B and 27B parameter versions, Gemma 2 boasts significant improvements in performance and efficiency, making it cost-effective to deploy on common hardware like NVIDIA GPUs. The model is designed for seamless integration with existing AI tools and frameworks, ensuring swift and efficient inference processes.&lt;/p&gt;

&lt;p&gt;Gemma 2 stands out for its focus on responsible AI development. It incorporates advanced safety features and provides open-sourced evaluation tools to facilitate ethical AI research. This ensures that developers can work with the model while adhering to best practices in AI safety and responsibility. The availability of these tools underscores Google's commitment to fostering a trustworthy AI ecosystem.&lt;/p&gt;

&lt;p&gt;Researchers and developers can access Gemma 2 through various platforms, including Google AI Studio, Kaggle, and Hugging Face. This accessibility is intended to encourage widespread experimentation and innovation within the AI community. By providing such a powerful and open resource, Google aims to accelerate advancements in AI and support the development of new, groundbreaking applications.&lt;/p&gt;

&lt;p&gt;For more details, visit the official &lt;a href=&quot;https://blog.google/technology/developers/google-gemma-2/&quot;&gt;Google blog post&lt;/a&gt;.
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Google has introduced Gemma 2, its latest generation of open AI models, aimed at enhancing research and development in artificial intelligence. With 9B and 27B parameter versions, Gemma 2 boasts significant improvements in performance and efficiency, making it cost-effective to deploy on common hardware like NVIDIA GPUs. The model is designed for seamless integration with existing AI tools and frameworks, ensuring swift and efficient inference processes.</summary></entry><entry><title type="html">Data curation via joint example selection further accelerates multimodal learning</title><link href="http://localhost:4000/DataCuration" rel="alternate" type="text/html" title="Data curation via joint example selection further accelerates multimodal learning" /><published>2024-06-25T00:00:00+03:00</published><updated>2024-06-25T00:00:00+03:00</updated><id>http://localhost:4000/DataCuration</id><content type="html" xml:base="http://localhost:4000/DataCuration">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly selecting batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from larger super-batches, we also leverage recent advances in model approximation to reduce the associated computational overhead. As a result, our approach--multimodal contrastive learning with joint example selection (JEST)--surpasses state-of-the-art models with up to 13× fewer iterations and 10× less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing the level of data curation as a new dimension for neural scaling laws.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.17711&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Connecting the Dots - LLMs can Infer and Verbalize Latent Structure from Disparate Training Data</title><link href="http://localhost:4000/ConnectDots" rel="alternate" type="text/html" title="Connecting the Dots - LLMs can Infer and Verbalize Latent Structure from Disparate Training Data" /><published>2024-06-20T00:00:00+03:00</published><updated>2024-06-20T00:00:00+03:00</updated><id>http://localhost:4000/ConnectDots</id><content type="html" xml:base="http://localhost:4000/ConnectDots">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. Further experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs (x,f(x)) can articulate a definition of f and compute inverses. While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures. Overall, the ability of LLMs to &quot;connect the dots&quot; without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2406.14546&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Meta releases New AI Research Models to Accelerate Innovation at Scale</title><link href="http://localhost:4000/MetaNewAI" rel="alternate" type="text/html" title="Meta releases New AI Research Models to Accelerate Innovation at Scale" /><published>2024-06-18T00:00:00+03:00</published><updated>2024-06-18T00:00:00+03:00</updated><id>http://localhost:4000/MetaNewAI</id><content type="html" xml:base="http://localhost:4000/MetaNewAI">&lt;p&gt; For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever. &lt;/p&gt;

&lt;p&gt; Today, we shared some of our latest FAIR research models with the world. These publicly released models include image-to-text and text-to-music generation models, a multi-token prediction model, and a technique for detecting AI-generated speech. By making this research publicly available, Meta aims to inspire further iterations and ultimately promote responsible advancements in AI. &lt;/p&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://www.youtube.com/embed/p9oM5dWmFZ0&quot; title=&quot;Meet Meta Chameleon&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For more information you can see the official meta &lt;a href=&quot;https://about.fb.com/news/2024/06/releasing-new-ai-research-models-to-accelerate-innovation-at-scale/&quot;&gt;announcement&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever.</summary></entry><entry><title type="html">NVIDIA announced Nemotron 340B</title><link href="http://localhost:4000/NVIDIARelease" rel="alternate" type="text/html" title="NVIDIA announced Nemotron 340B" /><published>2024-06-14T00:00:00+03:00</published><updated>2024-06-14T00:00:00+03:00</updated><id>http://localhost:4000/NVIDIARelease</id><content type="html" xml:base="http://localhost:4000/NVIDIARelease">&lt;p&gt;Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.&lt;/p&gt;

&lt;p&gt;NVIDIA on 14th of June, announced Nemotron-4 340B, a family of open models that developers can use to generate synthetic data for training large language models (LLMs) for commercial applications across healthcare, finance, manufacturing, retail and every other industry. &lt;/p&gt;

&lt;p&gt;High-quality training data plays a critical role in the performance, accuracy and quality of responses from a custom LLM — but robust datasets can be prohibitively expensive and difficult to access. &lt;/p&gt;

&lt;p&gt;Through a uniquely permissive open model license, Nemotron-4 340B gives developers a free, scalable way to generate synthetic data that can help build powerful LLMs.&lt;/p&gt;

&lt;p&gt;The Nemotron-4 340B family includes base, instruct and reward models that form a pipeline to generate synthetic data used for training and refining LLMs. The models are optimized to work with NVIDIA NeMo, an open-source framework for end-to-end model training, including data curation, customization and evaluation. They’re also optimized for inference with the open-source NVIDIA TensorRT-LLM library. &lt;/p&gt;

&lt;p&gt;Nemotron-4 340B can be downloaded now from the NVIDIA NGC catalog and Hugging Face. Developers will soon be able to access the models at ai.nvidia.com, where they’ll be packaged as an NVIDIA NIM microservice with a standard application programming interface that can be deployed anywhere.&lt;/p&gt;

&lt;p&gt;If you want to find out more go to the &lt;a href=&quot;https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/&quot;&gt;official blog of NVIDIA&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.</summary></entry><entry><title type="html">Google open Project</title><link href="http://localhost:4000/GoogleProjectIDX" rel="alternate" type="text/html" title="Google open Project" /><published>2024-06-14T00:00:00+03:00</published><updated>2024-06-14T00:00:00+03:00</updated><id>http://localhost:4000/GoogleProjectIDX</id><content type="html" xml:base="http://localhost:4000/GoogleProjectIDX">&lt;p&gt;During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.&lt;/p&gt;

&lt;p&gt; &lt;i&gt;&quot;As AI becomes more prevalent, the complexities that come with deploying all of that really becomes harder, becomes greater, and we wanted to help solve that challenge,&quot;&lt;/i&gt; said Jeanine Banks, Google’s VP and general manager for Developer X and the company’s head of developer relations.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;&quot;That’s why we built project IDX, a multi-platform development experience that makes building applications fast and easy. Project IDX makes it really frictionless to get going with your preferred framework or language with easy-to-use templates like Next.js, Astro, Flutter, Dart, Angular, Go and more.&quot;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt; With this update, Google is integrating the Google Maps Platform into Project IDX, enabling the addition of geolocation features to apps. The update also includes integrations with Chrome Dev Tools and Lighthouse to assist in debugging applications. Soon, developers will be able to deploy apps to Cloud Run, Google Cloud's serverless platform for front- and back-end services.&lt;/p&gt;

&lt;p&gt; The development environment will also integrate with Checks, Google's AI-powered compliance platform, which is transitioning from beta to general availability on Tuesday.&lt;/p&gt;

&lt;p&gt; Project IDX isn't just about building AI-enabled applications; it's also about using AI to enhance the coding process. To facilitate this, IDX includes standard features like code completion and a chat assistant sidebar, as well as innovative tools like the ability to highlight a snippet of code and use Google's Gemini model to modify it, similar to generative fill in Photoshop.&lt;/p&gt;

&lt;p&gt; Whenever Gemini suggests code, it provides links back to the original source and its associated license.&lt;/p&gt;

&lt;p&gt; Built on the open-source Visual Studio Code, Project IDX also integrates seamlessly with GitHub, simplifying integration with existing workflows. In one of the latest releases, Google added built-in iOS and Android emulators for mobile developers directly into the IDE.&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/-wlZY4tfGMY&quot; title=&quot;Project IDX: Full-stack application development with generative AI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt; Feeling like you have to give it a go?. Check ProjectIDX &lt;a href=&quot;https://idx.dev/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.</summary></entry><entry><title type="html">Depth Anything V2</title><link href="http://localhost:4000/DepthAnythingV2" rel="alternate" type="text/html" title="Depth Anything V2" /><published>2024-06-13T00:00:00+03:00</published><updated>2024-06-13T00:00:00+03:00</updated><id>http://localhost:4000/DepthAnythingV2</id><content type="html" xml:base="http://localhost:4000/DepthAnythingV2">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07522&quot;&gt;Samba&lt;/a&gt; architecture achieves 3.73x faster throughput with enhanced context&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1&amp;rpar; replacing all labeled real images with synthetic images, 2&amp;rpar; scaling up the capacity of our teacher model, and 3&amp;rpar; teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Samba architecture achieves 3.73x faster throughput with enhanced context</summary></entry><entry><title type="html">Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B</title><link href="http://localhost:4000/MonteCarloTrees" rel="alternate" type="text/html" title="Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B" /><published>2024-06-13T00:00:00+03:00</published><updated>2024-06-13T00:00:00+03:00</updated><id>http://localhost:4000/MonteCarloTrees</id><content type="html" xml:base="http://localhost:4000/MonteCarloTrees">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07394&quot;&gt;Monte Carlo Trees&lt;/a&gt; with Llama-3 8B solve mathematics limitations of LLMs&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Monte Carlo Trees with Llama-3 8B solve mathematics limitations of LLMs</summary></entry><entry><title type="html">Samba - Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</title><link href="http://localhost:4000/Samba" rel="alternate" type="text/html" title="Samba - Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling" /><published>2024-06-11T00:00:00+03:00</published><updated>2024-06-11T00:00:00+03:00</updated><id>http://localhost:4000/Samba</id><content type="html" xml:base="http://localhost:4000/Samba">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07522&quot;&gt;Samba&lt;/a&gt; architecture achieves 3.73x faster throughput with enhanced context&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in &lt;a href=&quot;https://github.com/microsoft/Samba&quot;&gt;this https URL&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Samba architecture achieves 3.73x faster throughput with enhanced context</summary></entry><entry><title type="html">Apple announced partnership with ChatGPT</title><link href="http://localhost:4000/AppleGPT" rel="alternate" type="text/html" title="Apple announced partnership with ChatGPT" /><published>2024-06-10T00:00:00+03:00</published><updated>2024-06-10T00:00:00+03:00</updated><id>http://localhost:4000/AppleGPT</id><content type="html" xml:base="http://localhost:4000/AppleGPT">&lt;p&gt;Apple is partnering with OpenAI to put ChatGPT into Siri, the company announced at its WWDC 2024 keynote on 10th of June 2024.&lt;/p&gt;

&lt;iframe width=&quot;600&quot; height=&quot;338&quot; src=&quot;https://www.youtube.com/embed/p2dhZ3AoDDs&quot; title=&quot;Biggest AI announcements from Apple&amp;#39;s WWDC 2024&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;During the keynote of its annual Worldwide Developers Conference (WWDC) on Monday, Apple unveiled a new generative artificial intelligence (AI) offering called Apple Intelligence. The company also announced a highly anticipated partnership with OpenAI.&lt;/p&gt;

&lt;p&gt;Apple Intelligence is designed as a personal intelligence system for iPhone, iPad, and Mac, combining generative models with personal context to enhance relevance. This offering aims to simplify everyday tasks and actions across various apps. Alongside Apple Intelligence, Apple introduced a privacy-focused solution called Private Cloud Compute. As both, OpenAI and Apple, sides of this agreement stated &lt;i&gt;&quot;Apple users are asked before any questions are sent to ChatGPT, along with any documents or photos, and Siri then presents the answer directly.&quot;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Additionally, Apple is integrating OpenAI’s ChatGPT into iOS 18, iPadOS 18, and macOS Sequoia. This integration, which includes features like the new Writing Tools and Siri, will be powered by OpenAI’s GPT-4o model and will be available later this year.&lt;/p&gt;

&lt;p&gt;For more info on the topic:
&lt;ul&gt;
&lt;li&gt; &lt;a href=&quot;https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/&quot;&gt;Apple&lt;/a&gt;'s press release about latest advancements on the topic&lt;/li&gt;
&lt;li&gt; &lt;a href=&quot;https://openai.com/index/openai-and-apple-announce-partnership/&quot;&gt;OpenAI&lt;/a&gt;'s announcement about the partnership&lt;/li&gt;&amp;lt;/li&amp;gt;
&amp;lt;/p&amp;gt;
&lt;/ul&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Apple is partnering with OpenAI to put ChatGPT into Siri, the company announced at its WWDC 2024 keynote on 10th of June 2024.</summary></entry></feed>