<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-06-29T22:16:01+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">Meta releases New AI Research Models to Accelerate Innovation at Scale</title><link href="http://localhost:4000/MetaNewAI" rel="alternate" type="text/html" title="Meta releases New AI Research Models to Accelerate Innovation at Scale" /><published>2024-06-18T00:00:00+03:00</published><updated>2024-06-18T00:00:00+03:00</updated><id>http://localhost:4000/MetaNewAI</id><content type="html" xml:base="http://localhost:4000/MetaNewAI">&lt;p&gt; For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever. &lt;/p&gt;

&lt;p&gt; Today, we shared some of our latest FAIR research models with the world. These publicly released models include image-to-text and text-to-music generation models, a multi-token prediction model, and a technique for detecting AI-generated speech. By making this research publicly available, Meta aims to inspire further iterations and ultimately promote responsible advancements in AI. &lt;/p&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://www.youtube.com/embed/p9oM5dWmFZ0&quot; title=&quot;Meet Meta Chameleon&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For more information you can see the official meta &lt;a href=&quot;https://about.fb.com/news/2024/06/releasing-new-ai-research-models-to-accelerate-innovation-at-scale/&quot;&gt;announcement&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">For over a decade, Meta's Fundamental AI Research (FAIR) team has been dedicated to advancing AI through open research. In light of rapid innovations in the field, we recognize that collaboration with the global AI community is more crucial than ever.</summary></entry><entry><title type="html">NVIDIA announced Nemotron 340B</title><link href="http://localhost:4000/NVIDIARelease" rel="alternate" type="text/html" title="NVIDIA announced Nemotron 340B" /><published>2024-06-14T00:00:00+03:00</published><updated>2024-06-14T00:00:00+03:00</updated><id>http://localhost:4000/NVIDIARelease</id><content type="html" xml:base="http://localhost:4000/NVIDIARelease">&lt;p&gt;Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.&lt;/p&gt;

&lt;p&gt;NVIDIA on 14th of June, announced Nemotron-4 340B, a family of open models that developers can use to generate synthetic data for training large language models (LLMs) for commercial applications across healthcare, finance, manufacturing, retail and every other industry. &lt;/p&gt;

&lt;p&gt;High-quality training data plays a critical role in the performance, accuracy and quality of responses from a custom LLM — but robust datasets can be prohibitively expensive and difficult to access. &lt;/p&gt;

&lt;p&gt;Through a uniquely permissive open model license, Nemotron-4 340B gives developers a free, scalable way to generate synthetic data that can help build powerful LLMs.&lt;/p&gt;

&lt;p&gt;The Nemotron-4 340B family includes base, instruct and reward models that form a pipeline to generate synthetic data used for training and refining LLMs. The models are optimized to work with NVIDIA NeMo, an open-source framework for end-to-end model training, including data curation, customization and evaluation. They’re also optimized for inference with the open-source NVIDIA TensorRT-LLM library. &lt;/p&gt;

&lt;p&gt;Nemotron-4 340B can be downloaded now from the NVIDIA NGC catalog and Hugging Face. Developers will soon be able to access the models at ai.nvidia.com, where they’ll be packaged as an NVIDIA NIM microservice with a standard application programming interface that can be deployed anywhere.&lt;/p&gt;

&lt;p&gt;If you want to find out more go to the &lt;a href=&quot;https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/&quot;&gt;official blog of NVIDIA&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Nemotron-4 340B, a family of models optimized for NVIDIA NeMo and NVIDIA TensorRT-LLM, includes cutting-edge instruct and reward models, and a dataset for generative AI training.</summary></entry><entry><title type="html">Google open Project</title><link href="http://localhost:4000/GoogleProjectIDX" rel="alternate" type="text/html" title="Google open Project" /><published>2024-06-14T00:00:00+03:00</published><updated>2024-06-14T00:00:00+03:00</updated><id>http://localhost:4000/GoogleProjectIDX</id><content type="html" xml:base="http://localhost:4000/GoogleProjectIDX">&lt;p&gt;During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.&lt;/p&gt;

&lt;p&gt; &lt;i&gt;&quot;As AI becomes more prevalent, the complexities that come with deploying all of that really becomes harder, becomes greater, and we wanted to help solve that challenge,&quot;&lt;/i&gt; said Jeanine Banks, Google’s VP and general manager for Developer X and the company’s head of developer relations.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;&quot;That’s why we built project IDX, a multi-platform development experience that makes building applications fast and easy. Project IDX makes it really frictionless to get going with your preferred framework or language with easy-to-use templates like Next.js, Astro, Flutter, Dart, Angular, Go and more.&quot;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt; With this update, Google is integrating the Google Maps Platform into Project IDX, enabling the addition of geolocation features to apps. The update also includes integrations with Chrome Dev Tools and Lighthouse to assist in debugging applications. Soon, developers will be able to deploy apps to Cloud Run, Google Cloud's serverless platform for front- and back-end services.&lt;/p&gt;

&lt;p&gt; The development environment will also integrate with Checks, Google's AI-powered compliance platform, which is transitioning from beta to general availability on Tuesday.&lt;/p&gt;

&lt;p&gt; Project IDX isn't just about building AI-enabled applications; it's also about using AI to enhance the coding process. To facilitate this, IDX includes standard features like code completion and a chat assistant sidebar, as well as innovative tools like the ability to highlight a snippet of code and use Google's Gemini model to modify it, similar to generative fill in Photoshop.&lt;/p&gt;

&lt;p&gt; Whenever Gemini suggests code, it provides links back to the original source and its associated license.&lt;/p&gt;

&lt;p&gt; Built on the open-source Visual Studio Code, Project IDX also integrates seamlessly with GitHub, simplifying integration with existing workflows. In one of the latest releases, Google added built-in iOS and Android emulators for mobile developers directly into the IDE.&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/-wlZY4tfGMY&quot; title=&quot;Project IDX: Full-stack application development with generative AI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt; Feeling like you have to give it a go?. Check ProjectIDX &lt;a href=&quot;https://idx.dev/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">During the Google I/O 2024 developer conference, Google revealed that Project IDX, its next-generation, AI-powered browser-based development environment, is now in open beta. Initially introduced in August as an invite-only service, Project IDX has already been tested by over 100,000 developers.</summary></entry><entry><title type="html">Depth Anything V2</title><link href="http://localhost:4000/DepthAnythingV2" rel="alternate" type="text/html" title="Depth Anything V2" /><published>2024-06-13T00:00:00+03:00</published><updated>2024-06-13T00:00:00+03:00</updated><id>http://localhost:4000/DepthAnythingV2</id><content type="html" xml:base="http://localhost:4000/DepthAnythingV2">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07522&quot;&gt;Samba&lt;/a&gt; architecture achieves 3.73x faster throughput with enhanced context&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1&amp;rpar; replacing all labeled real images with synthetic images, 2&amp;rpar; scaling up the capacity of our teacher model, and 3&amp;rpar; teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Samba architecture achieves 3.73x faster throughput with enhanced context</summary></entry><entry><title type="html">Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B</title><link href="http://localhost:4000/MonteCarloTrees" rel="alternate" type="text/html" title="Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B" /><published>2024-06-13T00:00:00+03:00</published><updated>2024-06-13T00:00:00+03:00</updated><id>http://localhost:4000/MonteCarloTrees</id><content type="html" xml:base="http://localhost:4000/MonteCarloTrees">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07394&quot;&gt;Monte Carlo Trees&lt;/a&gt; with Llama-3 8B solve mathematics limitations of LLMs&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Monte Carlo Trees with Llama-3 8B solve mathematics limitations of LLMs</summary></entry><entry><title type="html">Samba - Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</title><link href="http://localhost:4000/Samba" rel="alternate" type="text/html" title="Samba - Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling" /><published>2024-06-11T00:00:00+03:00</published><updated>2024-06-11T00:00:00+03:00</updated><id>http://localhost:4000/Samba</id><content type="html" xml:base="http://localhost:4000/Samba">&lt;p&gt; &lt;a href=&quot;https://arxiv.org/abs/2406.07522&quot;&gt;Samba&lt;/a&gt; architecture achieves 3.73x faster throughput with enhanced context&lt;/p&gt;

&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt;Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in &lt;a href=&quot;https://github.com/microsoft/Samba&quot;&gt;this https URL&lt;/a&gt;.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Samba architecture achieves 3.73x faster throughput with enhanced context</summary></entry><entry><title type="html">Apple announced partnership with ChatGPT</title><link href="http://localhost:4000/AppleGPT" rel="alternate" type="text/html" title="Apple announced partnership with ChatGPT" /><published>2024-06-10T00:00:00+03:00</published><updated>2024-06-10T00:00:00+03:00</updated><id>http://localhost:4000/AppleGPT</id><content type="html" xml:base="http://localhost:4000/AppleGPT">&lt;p&gt;Apple is partnering with OpenAI to put ChatGPT into Siri, the company announced at its WWDC 2024 keynote on 10th of June 2024.&lt;/p&gt;

&lt;iframe width=&quot;600&quot; height=&quot;338&quot; src=&quot;https://www.youtube.com/embed/p2dhZ3AoDDs&quot; title=&quot;Biggest AI announcements from Apple&amp;#39;s WWDC 2024&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;During the keynote of its annual Worldwide Developers Conference (WWDC) on Monday, Apple unveiled a new generative artificial intelligence (AI) offering called Apple Intelligence. The company also announced a highly anticipated partnership with OpenAI.&lt;/p&gt;

&lt;p&gt;Apple Intelligence is designed as a personal intelligence system for iPhone, iPad, and Mac, combining generative models with personal context to enhance relevance. This offering aims to simplify everyday tasks and actions across various apps. Alongside Apple Intelligence, Apple introduced a privacy-focused solution called Private Cloud Compute. As both, OpenAI and Apple, sides of this agreement stated &lt;i&gt;&quot;Apple users are asked before any questions are sent to ChatGPT, along with any documents or photos, and Siri then presents the answer directly.&quot;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Additionally, Apple is integrating OpenAI’s ChatGPT into iOS 18, iPadOS 18, and macOS Sequoia. This integration, which includes features like the new Writing Tools and Siri, will be powered by OpenAI’s GPT-4o model and will be available later this year.&lt;/p&gt;

&lt;p&gt;For more info on the topic:
&lt;ul&gt;
&lt;li&gt; &lt;a href=&quot;https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/&quot;&gt;Apple&lt;/a&gt;'s press release about latest advancements on the topic&lt;/li&gt;
&lt;li&gt; &lt;a href=&quot;https://openai.com/index/openai-and-apple-announce-partnership/&quot;&gt;OpenAI&lt;/a&gt;'s announcement about the partnership&lt;/li&gt;&amp;lt;/li&amp;gt;
&amp;lt;/p&amp;gt;
&lt;/ul&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Apple is partnering with OpenAI to put ChatGPT into Siri, the company announced at its WWDC 2024 keynote on 10th of June 2024.</summary></entry><entry><title type="html">Anthropic’s now lets you create bots to work for you and interact with external APIs and tools</title><link href="http://localhost:4000/AnthropicBots" rel="alternate" type="text/html" title="Anthropic’s now lets you create bots to work for you and interact with external APIs and tools" /><published>2024-05-30T00:00:00+03:00</published><updated>2024-05-30T00:00:00+03:00</updated><id>http://localhost:4000/AnthropicBots</id><content type="html" xml:base="http://localhost:4000/AnthropicBots">&lt;p&gt; &lt;a href=&quot;https://www.anthropic.com/news/tool-use-ga&quot;&gt;Tool use&lt;/a&gt;, which enables Claude to interact with external tools and APIs, is now generally available across the entire Claude 3 model family on the Anthropic Messages API, Amazon Bedrock, and Google Cloud's Vertex AI. With tool use, Claude can perform tasks, manipulate data, and provide more dynamic—and accurate—responses.&lt;/p&gt;

&lt;h2&gt; Tool use &lt;/h2&gt;

&lt;p&gt; Define a toolset for Claude and specify your request in natural language. Claude will then select the appropriate tool to fulfill the task and, when appropriate, execute the corresponding action: 

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Extract structured data from unstructured text&lt;/strong&gt;: Pull names, dates, and amounts from invoices to reduce manual data entry.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Convert natural language requests into structured API calls&lt;/strong&gt;: Enable teams to self-serve common actions (e.g., &quot;cancel subscription&quot;) with simple commands.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Answer questions by searching databases or using web APIs&lt;/strong&gt;: Provide instant, accurate responses to customer inquiries in support chatbots. &lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Automate simple tasks through software APIs&lt;/strong&gt;: Save time and minimize errors in data entry or file management. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Orchestrate multiple fast Claude subagents for granular tasks&lt;/strong&gt;: Automatically find the optimal meeting time based on attendee availability. &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/b77htH1eX-s&quot; title=&quot;Claude 3 tool use: customer support&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Felling like you want to find out more?
&lt;ul&gt;
&lt;li&gt; Read the &lt;a href=&quot;https://docs.anthropic.com/en/docs/tool-use&quot;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Check the &lt;a href=&quot;https://github.com/anthropics/courses/tree/master/ToolUse&quot;&gt;tool use tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt; Explore the &lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/tool_use&quot;&gt;Anthropic Cookbooks on tool use&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Tool use, which enables Claude to interact with external tools and APIs, is now generally available across the entire Claude 3 model family on the Anthropic Messages API, Amazon Bedrock, and Google Cloud's Vertex AI. With tool use, Claude can perform tasks, manipulate data, and provide more dynamic—and accurate—responses.</summary></entry><entry><title type="html">Microsoft unveils Copilot + PCs, new Phi-3 models + Vision</title><link href="http://localhost:4000/MicrosoftAnnouncementsCopilot" rel="alternate" type="text/html" title="Microsoft unveils Copilot + PCs, new Phi-3 models + Vision" /><published>2024-05-23T00:00:00+03:00</published><updated>2024-05-23T00:00:00+03:00</updated><id>http://localhost:4000/MicrosoftAnnouncementsCopilot</id><content type="html" xml:base="http://localhost:4000/MicrosoftAnnouncementsCopilot">&lt;p&gt;As stated &lt;a href=&quot;https://ai.azure.com/explore/models/Phi-3-vision-128k-instruct/version/2/registry/azureml?tid=c2beb3f8-6ade-48ce-8e7d-6be943f1fbf7#overview&quot;&gt;here&lt;/a&gt;:

Phi-3 Vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.&lt;/p&gt;

&lt;p&gt;Resources and Technical Documentation: 
&lt;ul&gt;
&lt;li&gt; &lt;a href=&quot;https://aka.ms/phi3blog-april&quot;&gt;Phi-3 Microsoft Blog&lt;/a&gt; &lt;/li&gt;
&lt;li&gt; &lt;a href=&quot;https://aka.ms/phi3-tech-report&quot;&gt;Phi-3 Technocal Report&lt;/a&gt; &lt;li&gt;
&amp;lt;/ul&amp;gt;&amp;lt;/p&amp;gt;

&lt;p&gt;Moving forward, Microsoft has unveiled an ambitious new direction for its laptops, focusing on cutting-edge AI features that require a Neural Processing Unit (NPU). The new Copilot+ PCs badge designates approved AI-ready laptops, currently limited to models with Qualcomm Snapdragon X processors. While these laptops are not yet available, many are expected to hit the market soon.&lt;/p&gt;

&lt;p&gt;Microsoft emphasizes efficiency, value, and a new keyboard button as key elements of this new computing wave. This initiative aims to make AI PCs a significant and practical reality, bringing attention to Windows on Arm. As previously speculated, Microsoft is integrating its Copilot AI directly on local computers. Here’s a comprehensive overview of what we know from Microsoft’s Copilot+ PC press conference and prior information. 

Let's take a moment and watch the promotion video created by Microsoft.&lt;/p&gt;

&lt;iframe width=&quot;799&quot; height=&quot;449&quot; src=&quot;https://www.youtube.com/embed/5JmkWJNng2I&quot; title=&quot;Introducing Copilot+ PCs&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;So, what do you think about it? Amazed or feeling neutral according to the advancements and given the fact that Apple, Microsoft's rival, used NPU for first time at 2017 and at Microsoft the first usaged is recorded on 2021? Overall, my opinion is that we can get a lot out of this by using it properly. The future is exciting and I can't wait to see what may come!&lt;/p&gt;

&lt;/li&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">As stated here:</summary></entry><entry><title type="html">Paragon changes RAG model for your customers</title><link href="http://localhost:4000/paragon" rel="alternate" type="text/html" title="Paragon changes RAG model for your customers" /><published>2024-05-22T00:00:00+03:00</published><updated>2024-05-22T00:00:00+03:00</updated><id>http://localhost:4000/paragon</id><content type="html" xml:base="http://localhost:4000/paragon">&lt;p&gt; &lt;i&gt;Integrate your multi-tenant AI SaaS with 100+ 3rd party apps with 70% less engineering.&lt;/i&gt;

&lt;p&gt; Today, third-party integrations are one of the most important parts of building a SaaS application. The average enterprise uses over 1,000 different cloud apps, and customers expect to buy software that integrates seamlessly with their other tools. However, achieving a robust set of high-quality product integrations typically requires anywhere from months to years of engineering work to build and maintain across multiple providers.&lt;/p&gt;

&lt;p&gt; Paragon is an embedded solution for integrating your product with third-party SaaS apps, providing your customers with a seamless, unified integration experience. This allows teams to avoid the cost, time, and risk that come with building and maintaining their own integrations solution. A single installation of Paragon takes just a couple of hours and enables your application to support integrations with the most popular SaaS apps.&lt;/p&gt;

&lt;p&gt; You implement Paragon by adding the Connect SDK to your application, which allows you to display the Connect Portal - a component that your users interact with in order to connect their third-party app accounts. Paragon provides fully managed authentication for each third-party app provider we support, and allows you to access your users' app accounts using Workflows or via the Paragon API.&lt;/p&gt;

&lt;p&gt; If you are interested in finding more about paragon, you can click &lt;a href=&quot;https://www.useparagon.com/paragon-for-ai?utm_source=alphasignal&amp;amp;utm_medium=newsletter&amp;amp;utm_content=ai_pitch&quot;&gt;here&lt;/a&gt; and explore it on you own pace! &lt;/p&gt; 
&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Integrate your multi-tenant AI SaaS with 100+ 3rd party apps with 70% less engineering.</summary></entry></feed>