<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-09-24T13:23:49+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kavour</title><subtitle>Data Science and AI News</subtitle><entry><title type="html">DataGemma - Grounding AI in Real-World Data to Combat Hallucinations</title><link href="http://localhost:4000/GoogleDataGemma" rel="alternate" type="text/html" title="DataGemma - Grounding AI in Real-World Data to Combat Hallucinations" /><published>2024-09-12T00:00:00+03:00</published><updated>2024-09-12T00:00:00+03:00</updated><id>http://localhost:4000/GoogleDataGemma</id><content type="html" xml:base="http://localhost:4000/GoogleDataGemma">&lt;p&gt;Large Language Models (LLMs) have revolutionized the AI landscape by providing powerful tools for generating human-like text, answering complex questions, and assisting with tasks like summarization and code generation. However, these models sometimes produce inaccurate information with confidence, a phenomenon known as &quot;hallucination.&quot; Addressing this issue is critical for enhancing AI reliability. Enter &lt;a href=&quot;https://ai.google.dev/gemma&quot;&gt;DataGemma&lt;/a&gt;, the first open model designed to reduce hallucinations by grounding LLMs in real-world statistical data from Google’s vast &lt;a href=&quot;https://datacommons.org/&quot;&gt;Data Commons&lt;/a&gt;. This article explores how DataGemma leverages the power of trusted data sources to improve the factual accuracy and reasoning of LLMs.&lt;/p&gt;

&lt;h3&gt;The Challenges of Hallucination in AI&lt;/h3&gt;

&lt;p&gt;As AI models grow more advanced, they demonstrate remarkable capabilities in various domains. They can sift through extensive text databases, generate creative ideas, and even draft software code. Yet, despite their strengths, they are prone to hallucinations—generating outputs that are either partially or entirely incorrect. This challenge is particularly problematic when AI models are used in fields requiring high accuracy, such as research, policymaking, and data analysis. For AI to become a more dependable tool, it must consistently provide accurate information grounded in verifiable facts.&lt;/p&gt;

&lt;h3&gt;Introducing DataGemma and Data Commons&lt;/h3&gt;

&lt;p&gt;DataGemma is Google’s innovative solution to the hallucination problem. It works by connecting LLMs to the Data Commons, a public knowledge graph filled with over 240 billion data points across numerous statistical variables. This data is sourced from reputable organizations like the United Nations (UN), World Health Organization (WHO), and the Centers for Disease Control and Prevention (CDC). The wealth of reliable information within Data Commons spans topics like health, economics, demographics, and environmental trends.&lt;/p&gt;

&lt;p&gt;By integrating Data Commons, DataGemma ensures that LLMs can access real-world, trustworthy data during their response generation process. This connection allows AI systems to verify statistical claims, reducing the likelihood of hallucinations and improving the overall factual accuracy of the responses generated by models.&lt;/p&gt;

&lt;h3&gt;Grounding LLMs with DataGemma: RIG and RAG Approaches&lt;/h3&gt;

&lt;p&gt;DataGemma employs two primary techniques to mitigate hallucinations: RIG (Retrieval-Interleaved Generation) and RAG (Retrieval-Augmented Generation).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;lt;a href=https://colab.research.google.com/github/datacommonsorg/llm-tools/blob/master/notebooks/datagemma_rig.ipynb'&amp;gt;RIG&amp;lt;/a&amp;gt; (Retrieval-Interleaved Generation) – This method allows models to proactively query trusted sources, such as Data Commons, during the response generation process. If the model encounters a statistical query or data-related prompt, it retrieves accurate information from Data Commons before finalizing its response. This proactive retrieval helps the model fact-check its output, greatly minimizing the chances of hallucinating.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/datacommonsorg/llm-tools/blob/master/notebooks/datagemma_rag.ipynb&quot;&gt;RAG&lt;/a&gt; (Retrieval-Augmented Generation) – RAG enables LLMs to go beyond their initial training data, pulling in additional contextual information from external sources. In the case of DataGemma, the model utilizes a long context window to retrieve relevant information from Data Commons before generating a response. By doing so, DataGemma enhances the depth and accuracy of responses, offering more informed insights.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Preliminary &lt;a href=&quot;http://datacommons.org/link/DataGemmaPaper&quot;&gt;research&lt;/a&gt; indicates that these techniques significantly reduce hallucinations, especially when handling numerical facts. Early tests have shown promising results, suggesting that users across research, decision-making, and curiosity-driven explorations will experience more reliable interactions with AI models.&lt;/p&gt;

&lt;p&gt;The launch of DataGemma marks a significant advancement in addressing the issue of hallucination in large language models. By connecting AI to the rich, real-world data housed in Google’s Data Commons, DataGemma offers a pathway to more reliable and factually grounded AI outputs. The integration of retrieval techniques like RIG and RAG demonstrates how LLMs can be anchored in trustworthy data, making them more dependable for users across industries.&lt;/p&gt;

&lt;p&gt;As the technology continues to evolve, the improvements seen in DataGemma are a crucial step toward making AI not only more sophisticated but also more accurate and trustworthy. By ensuring that AI provides factual and context-rich information, we are closer to building a future where these models become indispensable tools for informed decision-making and deeper understanding of the world around us.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Large Language Models (LLMs) have revolutionized the AI landscape by providing powerful tools for generating human-like text, answering complex questions, and assisting with tasks like summarization and code generation. However, these models sometimes produce inaccurate information with confidence, a phenomenon known as &quot;hallucination.&quot; Addressing this issue is critical for enhancing AI reliability. Enter DataGemma, the first open model designed to reduce hallucinations by grounding LLMs in real-world statistical data from Google’s vast Data Commons. This article explores how DataGemma leverages the power of trusted data sources to improve the factual accuracy and reasoning of LLMs.</summary></entry><entry><title type="html">How Adobe Firefly is Shaping the Future of Creative Workflows</title><link href="http://localhost:4000/AdobeFirefly" rel="alternate" type="text/html" title="How Adobe Firefly is Shaping the Future of Creative Workflows" /><published>2024-09-11T00:00:00+03:00</published><updated>2024-09-11T00:00:00+03:00</updated><id>http://localhost:4000/AdobeFirefly</id><content type="html" xml:base="http://localhost:4000/AdobeFirefly">&lt;p&gt;Since its launch in March 2023, Adobe Firefly has transformed the creative landscape by offering innovative AI-powered features that enhance design, imaging, and vector workflows. These models have become integral to Creative Cloud and Adobe Express, enabling users to generate creative assets more efficiently. Now, Adobe is expanding Firefly's capabilities into the realm of video editing, preparing to revolutionize the process for editors and filmmakers. In this article, we’ll explore the journey of Firefly so far, its growing impact on the creative community, and its potential to reshape video production.&lt;/p&gt;

&lt;p&gt;Adobe Firefly’s early models, such as Generative Fill in Photoshop, Generative Remove in Lightroom, and Text-to-Template in Express, have quickly been embraced by the community. In less than a year, creators have generated over 12 billion images and vectors, making Firefly one of the most rapidly adopted tools within Adobe's ecosystem. This success is largely due to Adobe’s focus on user feedback, ensuring that the models meet the needs of both individual creators and enterprise customers.&lt;/p&gt;

&lt;p&gt;The creative world is increasingly driven by video, and Adobe is now preparing to unveil its Firefly Video Model, which will soon be available in beta (waitlist &lt;a href=&quot;https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon#form&quot;&gt;here&lt;/a&gt;). This new model is set to empower video editors with cutting-edge tools for ideation and content creation, helping them navigate the growing demands for short-form and engaging video content. Editors are often tasked with much more than simply cutting footage—they handle color correction, visual effects, audio, and more, all under tight deadlines. Firefly’s AI tools aim to streamline these processes, saving time while enhancing the quality of the final product.&lt;/p&gt;

&lt;p&gt;One of the standout features of Firefly’s Video Model is its ability to assist with common editorial tasks like removing unwanted objects, smoothing jump cuts, and filling gaps in footage using generative AI. These functions not only simplify technical workflows but also help editors stay focused on the creative aspects of storytelling. Additionally, Firefly’s tools will integrate with Adobe’s existing collaboration platform, Frame.io, making it easier to share creative intent and receive feedback from teams and stakeholders.&lt;/p&gt;

&lt;p&gt;Most importantly, Firefly's AI models are developed with ethical considerations in mind. Adobe has ensured that the content used to train these models is commercially safe, sourced only from material with proper permissions, and never from user-generated content. This commitment to responsible AI helps creators work confidently without concerns about copyright or intellectual property violations.&lt;/p&gt;

&lt;p&gt;Adobe Firefly has already proven itself as a game-changing tool for creative professionals, and its upcoming Video Model promises to further elevate video editing workflows. By integrating AI into various aspects of the post-production process, Firefly enables editors to focus on what they do best—telling compelling stories. As the demand for high-quality video content continues to grow, Adobe's generative AI tools will empower creators to meet deadlines while pushing the boundaries of their craft. With the beta release of Firefly’s Video Model on the horizon, the future of video editing looks brighter, faster, and more creative than ever before.&lt;/p&gt;

&lt;p&gt; It is decided to share some of Adobe's increadible progress with all of us so sit back and enjoy their work!&lt;/p&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://www.youtube.com/embed/puEgugluadk&quot; title=&quot;Adobe Firefly Video Model Coming Soon | Adobe Video&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;&lt;strong&gt;Original Footage:&lt;/strong&gt;&lt;/h3&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://blog.adobe.com/media_12ac94566a1ce98690d7929346442187b48cfdff6.mp4&quot; title=&quot;Adobe Firefly Video Model Coming Soon | Adobe Video&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;&lt;strong&gt;Generated Clip:&lt;/strong&gt;&lt;/h3&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://blog.adobe.com/media_1aa1fceac992b3db71ba6673c1706ef178c3ffd31.mp4&quot; title=&quot;Adobe Firefly Video Model Coming Soon | Adobe Video&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3&gt;&lt;strong&gt;Combined Sequence:&lt;/strong&gt;&lt;/h3&gt;

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://blog.adobe.com/media_1e8e41d1c1fef51f9346af4e817662d539191fdce.mp4&quot; title=&quot;Adobe Firefly Video Model Coming Soon | Adobe Video&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Since its launch in March 2023, Adobe Firefly has transformed the creative landscape by offering innovative AI-powered features that enhance design, imaging, and vector workflows. These models have become integral to Creative Cloud and Adobe Express, enabling users to generate creative assets more efficiently. Now, Adobe is expanding Firefly's capabilities into the realm of video editing, preparing to revolutionize the process for editors and filmmakers. In this article, we’ll explore the journey of Firefly so far, its growing impact on the creative community, and its potential to reshape video production.</summary></entry><entry><title type="html">LLaMA-Omni - Seamless Speech Interaction with Large Language Models</title><link href="http://localhost:4000/LlammaOmni" rel="alternate" type="text/html" title="LLaMA-Omni - Seamless Speech Interaction with Large Language Models" /><published>2024-09-10T00:00:00+03:00</published><updated>2024-09-10T00:00:00+03:00</updated><id>http://localhost:4000/LlammaOmni</id><content type="html" xml:base="http://localhost:4000/LlammaOmni">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Fang,+Q&quot;&gt;Qingkai Fang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Guo,+S&quot;&gt;Shoutao Guo&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhou,+Y&quot;&gt;Yan Zhou&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ma,+Z&quot;&gt;Zhengrui Ma&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Zhang,+S&quot;&gt;Shaolei Zhang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Feng,+Y&quot;&gt;Yang Feng&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.06666&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">SambaNova Launches The World’s Fastest AI Platform</title><link href="http://localhost:4000/SambaNova" rel="alternate" type="text/html" title="SambaNova Launches The World’s Fastest AI Platform" /><published>2024-09-10T00:00:00+03:00</published><updated>2024-09-10T00:00:00+03:00</updated><id>http://localhost:4000/SambaNova</id><content type="html" xml:base="http://localhost:4000/SambaNova">&lt;p&gt; In an exciting development for AI and machine learning, &lt;a href=&quot;https://sambanova.ai&quot;&gt;SambaNova Systems&lt;/a&gt; has announced the launch of SambaNova Cloud, the world’s fastest AI inference platform. Leveraging the power of its SN40L AI chip, SambaNova Cloud delivers unmatched speed and precision, running the groundbreaking Llama 3.1 405B model at an impressive 132 tokens per second (t/s). The platform is available to developers today, offering a powerful solution for building generative AI applications with both the largest and most capable open-source models.&lt;/p&gt;

&lt;h3&gt; The Power of Llama 3.1 in SambaNova Cloud&lt;/h3&gt;

&lt;p&gt; Meta’s Llama 3.1 models have gained significant attention this year, with versions ranging from 8B to 405B parameters. The 405B model, in particular, is a breakthrough for open-source AI, offering a serious alternative to the proprietary models from industry giants like OpenAI, Anthropic, and Google. However, deploying such a large model has always been a challenge due to its size, complexity, and the associated speed trade-offs.&lt;/p&gt;

&lt;p&gt; This is where SambaNova comes in. According to CEO Rodrigo Liang, SambaNova is the only platform running Llama 3.1 405B at full precision and at 132 t/s, a feat that competitors using Nvidia GPUs cannot match. By using the custom-built SN40L AI chip, SambaNova reduces the cost and complexity of deploying massive models like Llama 3.1 405B while delivering faster speeds than ever before.&lt;/p&gt;

&lt;p&gt; For enterprises, this means unprecedented flexibility. “Customers want versatility,” says Liang. “They need the 70B model at lightning-fast speeds for agentic AI workflows, and the 405B model for the highest fidelity and best results. Only SambaNova Cloud offers both today.”

&lt;h3&gt; Unmatched Speed for AI Developers&lt;/h3&gt;

&lt;p&gt; Developers can now access SambaNova Cloud through a free API, allowing them to build and deploy applications with world-record speeds. In addition to running the 405B model at 132 t/s, SambaNova Cloud also supports the smaller Llama 3.1 70B model at 461 tokens per second, making it ideal for agentic AI systems that require high-speed, real-time responses. These speeds are essential for applications needing fast token generation, such as conversational agents, real-time analytics, and autonomous systems.&lt;/p&gt;

&lt;p&gt; Andrew Ng, a renowned AI expert and founder of DeepLearning.AI, praised the technical achievements of SambaNova Cloud, stating that running the 405B model at 16-bit precision and over 100 t/s is a game-changer for developers working with large language models (LLMs).&lt;/p&gt;

&lt;h3&gt; Independent Validation of SambaNova's Speed&lt;/h3&gt;

&lt;p&gt; The platform’s impressive performance has been independently verified. According to George Cameron, Co-Founder of Artificial Analysis, SambaNova Cloud’s Llama 3.1 405B endpoint achieved a record speed of 132 tokens per second, outperforming other frontier models from OpenAI, Anthropic, and Google. This speed makes it the best option for AI use cases where rapid token processing is critical.&lt;/p&gt;

&lt;img src=&quot;https://sambanova.ai/hs-fs/hubfs/405b.jpg?width=1160&amp;amp;height=449&amp;amp;name=405b.jpg&quot; /&gt;

&lt;h3&gt; A Platform for Agentic AI and More&lt;/h3&gt;

&lt;p&gt; SambaNova Cloud isn’t just about speed—it’s designed to support a wide range of AI applications. The 70B model, in particular, is ideal for agentic AI workflows, where systems need to interact and collaborate to complete complex tasks. Companies like Bigtincan and Blackbox AI have already adopted SambaNova Cloud to power their AI-driven solutions, citing up to a 300% improvement in efficiency for their platforms.&lt;/p&gt;

&lt;h3&gt; How to Get Started with SambaNova Cloud&lt;/h3&gt;

&lt;p&gt; SambaNova Cloud offers multiple tiers for different needs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt; Free Tier: Open to all developers today, providing free API access to Llama 3.1 models.&lt;/li&gt;
&lt;li&gt; Developer Tier: Launching by the end of 2024, this tier will offer higher rate limits and access to the 8B, 70B, and 405B models for advanced development.&lt;/li&gt;
&lt;li&gt; Enterprise Tier: Available now, this tier is designed for large-scale production workloads with the highest rate limits and scalability.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt; SambaNova’s SN40L Chip: The Heart of the Platform&lt;/h3&gt;

&lt;p&gt;At the core of SambaNova Cloud’s performance is the SN40L AI chip. Its patented dataflow architecture and three-tier memory design allow it to run AI models faster and more efficiently than traditional GPU-based solutions. This unique hardware accelerates inference speeds, making SambaNova Cloud the fastest platform for AI developers today.&lt;/p&gt;

&lt;h3&gt; SambaNova: Pioneering AI for the Enterprise&lt;/h3&gt;

&lt;p&gt; Founded in 2017 and headquartered in Palo Alto, SambaNova Systems was created by industry veterans from Sun/Oracle and Stanford University. The company is backed by top-tier investors, including SoftBank, BlackRock, and Intel Capital, and is committed to bringing cutting-edge AI technology to the enterprise. SambaNova’s cloud platform and custom AI chips enable organizations to quickly deploy state-of-the-art generative AI capabilities, transforming how businesses use AI.

&lt;p&gt; For developers and enterprises looking to leverage the power of open-source models like Llama 3.1 with unmatched speed and precision, SambaNova Cloud is available now. Visit &lt;a href=&quot;https://sambanova.ai&quot;&gt;SambaNova’s website&lt;/a&gt; or follow them on &lt;a href=&quot;https://www.linkedin.com/company/sambanova/&quot;&gt;LinkedIn&lt;/a&gt; and &lt;a href=&quot;http://twitter.com/SambaNovaAI&quot;&gt;X&lt;/a&gt; to learn more.&lt;/p&gt;

&lt;p&gt;To check out the full report, their blog post, click &lt;a href=&quot;https://sambanova.ai/press/worlds-fastest-ai-platform&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/p&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">In an exciting development for AI and machine learning, SambaNova Systems has announced the launch of SambaNova Cloud, the world’s fastest AI inference platform. Leveraging the power of its SN40L AI chip, SambaNova Cloud delivers unmatched speed and precision, running the groundbreaking Llama 3.1 405B model at an impressive 132 tokens per second (t/s). The platform is available to developers today, offering a powerful solution for building generative AI applications with both the largest and most capable open-source models.</summary></entry><entry><title type="html">GroUSE - A Benchmark to Evaluate Evaluators in Grounded Question Answering</title><link href="http://localhost:4000/GroUSE" rel="alternate" type="text/html" title="GroUSE - A Benchmark to Evaluate Evaluators in Grounded Question Answering" /><published>2024-09-10T00:00:00+03:00</published><updated>2024-09-10T00:00:00+03:00</updated><id>http://localhost:4000/GroUSE</id><content type="html" xml:base="http://localhost:4000/GroUSE">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge.&lt;/p&gt;
&lt;p&gt;To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4's judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection.&lt;/p&gt;
&lt;p&gt;We further show that finetuning Llama-3 on GPT-4's reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4's evaluations and calibration on reference situations.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Muller,+S&quot;&gt;Sacha Muller&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Loison,+A&quot;&gt;António Loison&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Omrani,+B&quot;&gt;Bilel Omrani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Viaud,+G&quot;&gt;Gautier Viaud&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://www.arxiv.org/abs/2409.06595&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Multiple datasources - Route selections</title><link href="http://localhost:4000/rag3" rel="alternate" type="text/html" title="Multiple datasources - Route selections" /><published>2024-09-09T00:00:00+03:00</published><updated>2024-09-09T00:00:00+03:00</updated><id>http://localhost:4000/rag3</id><content type="html" xml:base="http://localhost:4000/rag3">&lt;p&gt; I hope you enjoy every step so far. Until this point of our Langchain/RAG journey, we have managed to build a simple local application and a querry transformation assistant. But what happens when we have multiple data sources? How to define where our application will retrieve the required information from? The definition of this process, finding the correct road, or better let's say finding the correct route, is called Routing.&lt;/p&gt;

&lt;p&gt; There are many ways to give directions in real life, this applies here as well! As we can see in the following image, there are two main categories of routing. 

&lt;ul&gt;
&lt;li&gt; &lt;strong&gt;Logical Routing&lt;/strong&gt;: Let LLM choose a DB based on the question.&lt;/li&gt;
&lt;li&gt; &lt;strong&gt;Semantic Routing&lt;/strong&gt;: Embed question and choose prompt based on similarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:1400/1*iBm4xuEwvnp9KFyH9x05cw.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt; Before going though the architecture of such a RAG model for either Logical or Semantic routing, we are going to take a look at the differences accompanied with some simplier examples to understand which is best for your case case to go with.&lt;/p&gt;

&lt;h3&gt; Logical Routing &lt;/h3&gt;

&lt;p&gt; Routes are defined based on pre-determined rules, conditions, or algorithms. These rules are purely technical, and they don't take into account the meaning or context of the data. Logical Routing decisions are made based on factors like IP addresses, routing tables, or specific conditions that match certain criteria. It focuses on efficiently directing traffic or requests based on a logical or structural framework.&lt;/p&gt;

&lt;p&gt;To put logical routing straight, imagine you’re in a city, and there are different routes based on traffic signals or road rules. These routes don't care about where you're coming from or your personal preferences, but simply about making sure traffic moves according to the road signs.

&lt;ul&gt;
&lt;li&gt; Situation: You’re driving from Point A to Point B.&lt;/li&gt;
&lt;li&gt; Logical Rule: If a road is closed, take the next available road.&lt;/li&gt;
&lt;li&gt; Routing Decision: You don't think about why the road is closed; you just follow the detour and proceed based on traffic rules. It's like having a GPS system giving you orders, while driving, in a city that you have never been.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;h3&gt; Semantic Routing &lt;/h3&gt;

&lt;p&gt;  Routes are chosen based on the meaning or context of the data being transmitted. The logical routing decisions consider the content, purpose, or relationships within the data. It uses metadata or content analysis to decide the best route for a request, often ensuring that the most relevant resources handle the data. It prioritizes meaning and relevance to ensure that the request or data is handled in the best possible way based on its content.&lt;/p&gt;

&lt;p&gt; Imagine you're in a library, and someone asks where to find a book. Instead of directing them based on just shelf numbers (logical), you direct them based on their interest in the content of the book. You ask, &quot;What topic are you looking for?&quot; and route them accordingly to the right section.

&lt;ul&gt;
&lt;li&gt; Situation: You want to ask a question, but there are different people who can answer it.&lt;/li&gt;
&lt;li&gt; Semantic Rule: If your question is about science, you’re directed to a scientist; if it's about cooking, you’re directed to a chef.&lt;/li&gt;
&lt;li&gt; Routing Decision: The decision isn’t based on random criteria but on the meaning of your question and who’s best equipped to answer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;
Based on Data/Context, Logical Routing ignores the meaning of the data and routes based on fixed rules while semantic routing analyzes the meaning and routes accordingly. Overall and while looking at the purpose, logical routing focuses on efficiency and technical structure and on the other hand, semantic routing focuses on relevance and meaningful processing.
&lt;/p&gt;

&lt;p&gt; Now that we got the main idea about what is the meaning of both, let us take a closer look to the code and how to build a system like that.&lt;/p&gt;

&lt;p&gt; Initially, we need to call the modules that are going to help us accomplish our goal. I am not going to go through &lt;code&gt;.env&lt;/code&gt; file again and won't be mentioning this file again in the future. Hopefully, you got the idea behind it, why it is important and the reasons I insist on using one. If you are not so sure on what file I am referring to, please take a look at the first two notebooks of this series and follow the steps to create one. Remember &lt;code&gt;.env&lt;/code&gt; file will keep your Usernames, Passwords and Endpoint IDs safe without the need to erase them or hide them when sharing any file. So the modules we are going to use within this notebook are the following.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import AzureOpenAIEmbeddings
# from langchain.chat_models import AzureChatOpenAI
from langchain_openai import AzureChatOpenAI
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from typing import Literal
from langchain.utils.math import cosine_similarity
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda

from dotenv import load_dotenv, dotenv_values
load_dotenv()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we are using LLM models we need to define embeddings so that we can communicate with the computers and we &lt;i&gt;&quot;talk&quot;&lt;/i&gt; the same language.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;embeddings = AzureOpenAIEmbeddings(
    deployment = os.getenv('OPENAI_DEPLOYMENT_NAME'),
    model = os.getenv('OPENAI_MODEL_NAME_EMB'),
    azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),
    openai_api_type = os.getenv('OPENAI_API_TYPE'),
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Apart from that, &lt;strong&gt;vectorstore&lt;/strong&gt; is essential as we need a place to store the vectors that will result from our embeddings call. Hecne, we are going to define a function call &lt;i&gt;vectorstor_gen&lt;/i&gt; in order to store the information acquired (vectors generatated) from the files one owns and what to search from.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def vectorstore_gen(file, dir):
    loader = PyPDFLoader(file)
    documents = loader.load()

    # Split text into chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)
    text_chunks = text_splitter.split_documents(documents)

    vectorstore = Chroma.from_documents(documents=text_chunks,
                                        embedding=embeddings,
                                        persist_directory=dir)
    vectorstore.persist()
    return vectorstore
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now image you have two documents or batter let's say two different files containing documents, and you want to create two vectorstore for both of them so that our llm will be able to choose betwen then and answer to your questions. Remember that we have created a folder data in our project and there we store all the data that we want to use for our RAG projects. In this example I am going to use dummy pdf names (&lt;i&gt;A.pdf&lt;/i&gt; and &lt;i&gt;B.pdf&lt;/i&gt;) and imaging that those two files contain information/documentation about different LLM models. Feel free to replace those with two irrelevant pdf files (when it comes to topic)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
# Create a vectorestore to answer questions about topic A
vectorstore_A = vectorstore_gen('data/A.pdf', 'data/vectorstore_A')
# Create a vectorstore to answer questions about topic B
vectorstore_B = vectorstore_gen('data/B.pdf', 'data/vectorstore_B')

retriever_A = vectorstore_A.as_retriever(search_kwargs={'k':5})
retriever_B = vectorstore_B.as_retriever(search_kwargs={'k':5})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Now it is time to create the classes we are going to use to our routing models. Initially we are going to start with Logical Routing. Initially, we are going to build a class to route to the most relevant datasource accourding to the User's question. We are going to use an extention of &lt;code&gt;BaseModel&lt;/code&gt;. The &lt;code&gt;BaseModel&lt;/code&gt; class provides automatic validation parsing and serialization of data. &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
class QueryRouter(BaseModel):
    &quot;&quot;&quot;Route a user query to the appropriate datasource that will help answer the query accurately&quot;&quot;&quot;
    datasource: Literal['A', 'B', 'general'] = Field(..., description=&quot;Given a user question choose which datasource would be most relevant for answering their question&quot;)
    question: str = Field(..., description=&quot;User question to be routed to the appropriate datasource&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;  Next we need to build and and call an LLM model that will help us build our RAG model, within which we are going to include routing stratey.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
llm = AzureChatOpenAI(
    deployment_name = os.getenv('LLM_35_Deployment'),
    model_name = os.getenv('LLM_35_Model'),
    azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),
    temperature = 0
)

structured_llm = llm.with_structured_output(QueryRouter, method=&quot;function_calling&quot;, include_raw=True)
structured_llm.invoke(&quot;Which datasource should be used for a question about general knowledge?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now it is time to build the routing prompt that will be included in the RAG pipeline.  &lt;strong&gt;Not that&lt;/strong&gt; if the pdf files that you include are not about a topic of NLP and/or LLMs pleas read the system prompt and change it accordingly&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
router_prompt = ChatPromptTemplate.from_messages(
    [
        (&quot;system&quot;,
         &quot;You are an expert router that can direct user queries to the appropriate datasource. Route the following user question about a topic in NLP and LLMs to the appropriate datasource.\nIf it is a general question not related to the provided datasources, route it to the general datasource.\n&quot;),
        (&quot;user&quot;, &quot;{question}&quot;)
    ]
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; Finally we are ready to create the first router pipeline for our RAG model and include everythin we have been creating so far. &lt;strong&gt;Not that&lt;/strong&gt; change the upcoming &lt;code&gt;question&lt;/code&gt; accordingly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
router = (
        {'question': RunnablePassthrough()}
        | router_prompt
        | structured_llm
)
question = &quot;How does the A work?&quot;
result = router.invoke(question)
result
result['parsed'].datasource.lower()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; The final step is to define the a function that will choose which database should be used to answer our question, either A or B. In case you are running alongsite this project, the code chunks, and you have replaced  A and B for example, (A could be vectore similiarity and B could be KAN Model) then you could repalce A and B in the following code chunk as well in order to obtain relevant answer/result.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
qa_prompt = hub.pull('rlm/rag-prompt')

def choose_route(result):

    llm_route = AzureChatOpenAI(
        deployment_name = os.getenv('LLM_35_Deployment'),
        model_name = os.getenv('LLM_35_Model'),
        azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT'),
        temperature = 0

    )
    if &quot;B&quot; in result['parsed'].datasource.lower():
        print(f&quot;&amp;gt; Asking about B ...\nQuestion: {result['parsed'].question}\nAnswer:&quot;)
        B_chain = (
            {'context': retriever_B, 'question': RunnablePassthrough()}
            | qa_prompt
            | llm_route
            | StrOutputParser()
        )
        return B_chain.invoke(result['parsed'].question)
    elif &quot;A&quot; in result['parsed'].datasource.lower():
        print(f&quot;&amp;gt; Asking about A ...\nQuestion: {result['parsed'].question}\nAnswer:&quot;)
        A_chain = (
            {'context': retriever_lora, 'question': RunnablePassthrough()}
            | qa_prompt
            | llm_route
            | StrOutputParser()
        )
        return A_chain.invoke(result['parsed'].question)
    else:
        print(f&quot;&amp;gt; Asking about a general question ...\nQuestion: {result.question}\nAnswer:&quot;)
        general_chain = llm_route | StrOutputParser()
        return general_chain.invoke(result.question)

full_chain = router | RunnableLambda(choose_route)
full_chain.invoke(&quot;What is A?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; That concludes our example for logical routing. Next we are going to change a bit our previous coding chunks and produce an example about semantic routing. In this example we are going to create two templates about different topics. We are going to assume that the users ask questions about physics or mathematics. Initially we need to create two templates that are going to give a character to our LLM according to the question provided.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
physical_template = &quot;&quot;&quot;
You are a very smart physics professor. \
You are great at answering questions about physics in a concise and easy to understand manner. \
When you don't know the answer to a question you admit that you don't know.

Here is a question:
{question}
&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
math_template = &quot;&quot;&quot;You are a very good mathematician. You are great at answering math questions. \
You are so good because you are able to break down hard problems into their component parts, \
answer the component parts, and then put them together to answer the broader question.

Here is a question:
{question}&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to create a list containing the character templates. We are going to create embeddings about those routes in order to calculate the similarity with the question provided by the user. Remember that semantic routing checks the similarity of the meaning after all. The choice of the database that the RAG will provide the answer from is not defined by rules as the previous case.&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
routes = [physical_template, math_template]
route_embeddings = embeddings.embed_documents(routes)
len(route_embeddings)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can proceed to the definition of a function that according to the topic of the question that was provided, LLM will answer the question with a routing strategy to either physics or mathematics experties.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
def router(input):
    # Generate embeddings for the user query
    query_embedding = embeddings.embed_query(input['question'])
    # Getting similarity scores between the user query and the routes. This contains the similarity scores between the user query and each of the two routes.
    similarity = cosine_similarity([query_embedding], route_embeddings)[0]
    # Find the route that gives the maximum similarity score
    route_id = similarity.argmax()
    if route_id == 0:
        print(f&quot;&amp;gt; Asking a physics question ...\nQuestion: {input['question']}\nAnswer:&quot;)
    else:
        print(f&quot;&amp;gt; Asking a math question ...\nQuestion: {input['question']}\nAnswer:&quot;)

    return PromptTemplate.from_template(routes[route_id])

semantic_router_chain = (
    {'question': RunnablePassthrough()}
    | RunnableLambda(router)
    | llm
    | StrOutputParser
)

semantic_router_chain.invoke(&quot;What is the formula for the area of a circle?&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt; I hope you enjoyed this session as much as I did and a learned a thing or two. I wish that you kept a code chunk and will use it later on, on your own projects. Stay tuned for the next topic that we are going to take a look at this LangChain series&lt;/p&gt;

&lt;p&gt;&lt;i&gt; Be safe, code safer!&lt;/i&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="project" /><summary type="html">I hope you enjoy every step so far. Until this point of our Langchain/RAG journey, we have managed to build a simple local application and a querry transformation assistant. But what happens when we have multiple data sources? How to define where our application will retrieve the required information from? The definition of this process, finding the correct road, or better let's say finding the correct route, is called Routing.</summary></entry><entry><title type="html">Google’s Illuminate - Transforming Academic Papers into AI-Generated Podcasts</title><link href="http://localhost:4000/GoogleIllumnate" rel="alternate" type="text/html" title="Google’s Illuminate - Transforming Academic Papers into AI-Generated Podcasts" /><published>2024-09-09T00:00:00+03:00</published><updated>2024-09-09T00:00:00+03:00</updated><id>http://localhost:4000/GoogleIllumnate</id><content type="html" xml:base="http://localhost:4000/GoogleIllumnate">&lt;p&gt;Google Labs has a long tradition of inviting users to explore innovative technologies, with notable successes like Gmail, which began as an exclusive beta. Now, Google is unveiling Illuminate, a groundbreaking project that transforms academic papers into AI-generated audio discussions, styled like an NPR podcast. The concept is straightforward but powerful: Google's large language model, Gemini, creates a concise summary of a research paper and follows it up with a Q&amp;amp;A session. These are voiced by two AI-generated personas—a male interviewer and a female expert—who guide listeners through a brief, engaging conversation about the paper’s key points.

What makes Illuminate especially valuable is its potential to make academic content more accessible and convenient. With this service, you can listen to insightful summaries of research papers while you're on the go—whether you're working out, commuting, or simply multitasking. The platform could also easily be adapted for other types of audio narration, extending its utility to a wide range of subjects and industries. Currently in private beta, Illuminate allows you to join the waitlist if you’re interested in early access and testing out this exciting new tool. You can check out samples on the Google Illuminate website to see its potential firsthand.&lt;/p&gt;

&lt;p&gt;Google Labs has always been at the forefront of innovation, frequently inviting users to explore groundbreaking tech. Remember Gmail? It started as a private beta before revolutionizing how we manage email. Today, Google is back with another fascinating project, &lt;strong&gt;Illuminate&lt;/strong&gt;. This new initiative takes a fresh approach to academic research, turning complex papers into digestible, AI-generated audio discussions styled after popular NPR podcasts.&lt;/p&gt;

&lt;p&gt;So, what exactly is Illuminate, and why is it worth your attention?&lt;/p&gt;

&lt;h3&gt;The Power Behind Illuminate&lt;/h3&gt;

&lt;p&gt; At the heart of Illuminate lies Google’s large language model, Gemini, which does the heavy lifting. Here’s how it works: Gemini generates a concise summary of an academic paper and pairs it with a Q&amp;amp;A session. Two AI-generated voices—a male interviewer and a female expert—then present the content in the form of an engaging, short interview. Imagine being guided through the highlights of a research paper as if you were listening to a lively radio discussion.&lt;/p&gt;

&lt;h3&gt; Why Is This So Useful?&lt;/h3&gt;

&lt;p&gt; For anyone who regularly dives into research papers, you know the grind. The sheer volume of academic literature can be overwhelming, and finding the time to sit down and read every paper in full is often a challenge. This is where Illuminate shines. It offers a new way to stay informed by converting these dense academic texts into easily consumable audio discussions. Picture yourself catching up on the latest research while driving to work, hitting the gym, or even while doing chores around the house. It’s multitasking made smarter.&lt;/p&gt;

&lt;p&gt; Illuminate isn’t just limited to academic papers either. The platform’s potential extends far beyond that, with the ability to adapt to other forms of narration. Whether for educational content, business reports, or any document-heavy field, this kind of AI-generated podcasting could become a versatile tool for delivering information in a more accessible and engaging format.&lt;/p&gt;

&lt;h3&gt; A Glimpse Into the Future&lt;/h3&gt;

&lt;p&gt;Illuminate is still in its early stages, currently available as a private beta. Google is letting select users test the waters while they continue refining the service. If you’re eager to see what the future of academic content consumption could look like, you can join the waitlist for access.&lt;/p&gt;

&lt;p&gt;Curious to experience it for yourself? You can check out some sample audio discussions on the &lt;a href=&quot;https://illuminate.google.com/home?pli=1&quot;&gt;Google Illuminate&lt;/a&gt; website. Whether you’re an academic researcher, a student, or just someone who loves learning on the go, Illuminate has the potential to change how we interact with complex information.&lt;/p&gt;

&lt;p&gt;In the end, this is more than just a novelty. It’s a glimpse into how AI can reshape content delivery, making it more convenient, engaging, and adaptable to our daily routines. The future of learning might just sound a lot like your favorite podcast.&lt;/p&gt;

&lt;iframe width=&quot;1175&quot; height=&quot;661&quot; src=&quot;https://www.youtube.com/embed/mxlPGgfMJJs&quot; title=&quot;Using long-context to make knowledge accessible&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Stay tuned, because Illuminate could be the next big step in AI-driven information consumption.&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Google Labs has a long tradition of inviting users to explore innovative technologies, with notable successes like Gmail, which began as an exclusive beta. Now, Google is unveiling Illuminate, a groundbreaking project that transforms academic papers into AI-generated audio discussions, styled like an NPR podcast. The concept is straightforward but powerful: Google's large language model, Gemini, creates a concise summary of a research paper and follows it up with a Q&amp;amp;A session. These are voiced by two AI-generated personas—a male interviewer and a female expert—who guide listeners through a brief, engaging conversation about the paper’s key points.</summary></entry><entry><title type="html">Exploring the Replit Agent - AI Power Coding for Developers</title><link href="http://localhost:4000/ReplitAgent" rel="alternate" type="text/html" title="Exploring the Replit Agent - AI Power Coding for Developers" /><published>2024-09-09T00:00:00+03:00</published><updated>2024-09-09T00:00:00+03:00</updated><id>http://localhost:4000/ReplitAgent</id><content type="html" xml:base="http://localhost:4000/ReplitAgent">&lt;p&gt;Replit has long been at the forefront of integrating AI into software development, and their latest offering, the &lt;strong&gt;Replit Agent&lt;/strong&gt;, is no exception. Currently available through a limited early access program, this AI-powered assistant is designed to help users create software projects from scratch using simple, natural language prompts. Whether you’re a seasoned developer or new to coding, Replit Agent aims to make building applications more intuitive and accessible.&lt;/p&gt;

&lt;h3&gt; What is the Replit Agent? &lt;/h3&gt;

&lt;p&gt;The Replit Agent is an experimental AI tool designed to assist developers in building software projects. It understands natural language commands, allowing users to describe what they want to build, and then the agent takes over—planning, writing, and deploying code. Whether you’re working on web-based applications or prototyping new ideas, the agent is meant to accelerate the development process by generating code, suggesting solutions, and helping manage tasks.&lt;/p&gt;

&lt;h3&gt; How It Works &lt;/h3&gt;

&lt;p&gt;To get started with the Replit Agent, subscribers to Replit Core or Replit Teams can dive right in. The agent is included in these plans at no extra cost during the early access phase, though pricing details for general release will be announced later in 2024. Here’s Replit CEO Amjad Masad with an overview of how the agent works:

&lt;iframe width=&quot;950&quot; height=&quot;534&quot; src=&quot;https://www.youtube.com/embed/IYiVPrxY8-Y&quot; title=&quot;Meet the Replit Agent&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

Here’s how you can start using it:

On the Web:
&lt;ol&gt;
&lt;li&gt; Log into your Replit account with a Core or Teams subscription. &lt;/li&gt;
&lt;li&gt; Visit the home page or select Create Repl in the left navigation. &lt;/li&gt;
&lt;li&gt; Input a prompt of what you would like the agent to build. 
    &lt;ul&gt;
    &lt;li&gt; A good prompt is descriptive and detailed. Imagine you are describing a task for a teammate at work to complete. What information would they have to know to get the job done?&lt;/li&gt;
    &lt;li&gt; We recommend letting the agent select which technologies to use rather than specifying specific languages or frameworks. &lt;/li&gt;
    &lt;li&gt; The agent is currently best at 0 -&amp;gt; 1 prototyping for web-based applications.&lt;/li&gt;
    &lt;/ul&gt;
    &amp;lt;img src=”https://docimg.replit.com/images/replitai/agent_01.png”&amp;gt;
&lt;li&gt; Review and iterate on the plan the agent generated. Feel free to edit or delete steps that the agent recommends. &lt;/li&gt;
    &lt;img src=&quot;https://docimg.replit.com/images/replitai/agent_02.png&quot; /&gt;
&lt;li&gt; Follow along with the agent’s progress. &lt;/li&gt;
    &lt;img src=&quot;https://docimg.replit.com/images/replitai/agent_03.png&quot; /&gt;
&lt;li&gt; Work with the agent to provide API keys, feedback, or direction as it progresses in building your application. &lt;/li&gt;
    &lt;img src=&quot;https://docimg.replit.com/images/replitai/agent_04.png&quot; /&gt;
&lt;li&gt; Test your application and ask follow up questions as needed. &lt;/li&gt;
&lt;li&gt; Deploy your application to production! Learn more about Replit Deployments. &lt;/li&gt;
&amp;lt;/ol&amp;gt;

On Mobile:
&lt;ol&gt;
&lt;li&gt;Download the Replit app (version 2.90.2 or higher) via this link or QR code.&lt;/li&gt;
    &lt;img src=&quot;https://docimg.replit.com/images/replitai/agent_05.png&quot; /&gt;
&lt;li&gt;Ensure you're logged in with the email associated with your Core or Teams account.&lt;/li&gt;
&lt;li&gt;Head to the &quot;Create&quot; tab and select &quot;Start with AI,&quot; then describe the project you want to build.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Currently, the Replit Agent excels at prototyping web-based applications, but there’s potential for it to grow into more diverse use cases. Keep in mind that while it’s still in the experimental phase, you might encounter the occasional error or unexpected behavior. Your feedback will be crucial in improving the product as it evolves.&lt;/p&gt;

&lt;h3&gt; The Future of AI in Software Development &lt;/h3&gt;

&lt;p&gt; The vision behind Replit Agent is bigger than just an AI assistant for coding—it’s a step toward closer collaboration between humans and machines in the software development process. Replit envisions a future where AI agents not only fill in gaps but also enhance developers’ capabilities, offering creative solutions, and speeding up workflows. As you experiment with the Replit Agent, you’ll be helping to shape what this future looks like.&lt;/p&gt;

&lt;h3&gt; Getting Early Access&lt;/h3&gt;

&lt;p&gt;Replit Agent is currently in early access, available to Core and Teams subscribers. If you're subscribed to one of these plans, you can start using the agent today without any additional cost. While usage limits are in place during this phase, these quotas are refreshed regularly, giving you ongoing opportunities to test and experiment with the agent’s capabilities.&lt;/p&gt;

&lt;h3&gt; Final Thoughts &lt;/h3&gt;

&lt;p&gt;The Replit Agent represents an exciting new chapter in AI-assisted development, bringing us closer to a world where machines and humans collaborate seamlessly on software projects. Whether you're building a prototype or simply looking to experiment with AI in your workflow, Replit Agent offers a promising glimpse into the future of development. Don’t miss out—if you’re eligible, dive into early access today and start shaping the future of AI-driven coding. &lt;/p&gt;

&lt;p&gt; To read more about this brand new Replit product head &lt;a href=&quot;https://docs.replit.com/replitai/agent?ref=maginative.com&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;&lt;/ol&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="news" /><summary type="html">Replit has long been at the forefront of integrating AI into software development, and their latest offering, the Replit Agent, is no exception. Currently available through a limited early access program, this AI-powered assistant is designed to help users create software projects from scratch using simple, natural language prompts. Whether you’re a seasoned developer or new to coding, Replit Agent aims to make building applications more intuitive and accessible.</summary></entry><entry><title type="html">Theory, Analysis, and Best Practices for Sigmoid Self-Attention</title><link href="http://localhost:4000/SigmoidSelfAttntion" rel="alternate" type="text/html" title="Theory, Analysis, and Best Practices for Sigmoid Self-Attention" /><published>2024-09-06T00:00:00+03:00</published><updated>2024-09-06T00:00:00+03:00</updated><id>http://localhost:4000/SigmoidSelfAttntion</id><content type="html" xml:base="http://localhost:4000/SigmoidSelfAttntion">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ramapuram,+J&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Danieli,+F&quot;&gt;Federico Danieli&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Dhekane,+E&quot;&gt;Eeshan Dhekane&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Weers,+F&quot;&gt;Floris Weers&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Busbridge,+D&quot;&gt;Dan Busbridge&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Ablin,+P&quot;&gt;Pierre Ablin&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Likhomanenko,+T&quot;&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Digani,+J&quot;&gt;Jagrit Digani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Gu,+Z&quot;&gt;Zijin Gu&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Shidani,+A&quot;&gt;Amitis Shidani&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Webb,+R&quot;&gt;Russ Webb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://arxiv.org/abs/2409.04431&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry><entry><title type="html">Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers</title><link href="http://localhost:4000/LLMNovelResearchIdeas" rel="alternate" type="text/html" title="Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers" /><published>2024-09-06T00:00:00+03:00</published><updated>2024-09-06T00:00:00+03:00</updated><id>http://localhost:4000/LLMNovelResearchIdeas</id><content type="html" xml:base="http://localhost:4000/LLMNovelResearchIdeas">&lt;h2&gt; Abstract &lt;/h2&gt;

&lt;p&gt; Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p &amp;lt; 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.&lt;/p&gt;

&lt;h2&gt; Authors &lt;/h2&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Si,+C&quot;&gt;Chenglei Si&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Yang,+D&quot;&gt;Diyi Yang&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/search/cs?searchtype=author&amp;amp;query=Hashimoto,+T&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;&amp;lt;/a&amp;gt;&lt;/p&gt;

&lt;p&gt;For more information go &lt;a href=&quot;https://www.arxiv.org/abs/2409.04109&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Kavour</name></author><category term="research" /><summary type="html">Abstract</summary></entry></feed>